{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5114a854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done! Output written to t2dv2_table_columns_dbpedia.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "CSV_PATH = 'classes_GS.csv'\n",
    "TABLES_FOLDER = 'tables'\n",
    "\n",
    "df = pd.read_csv(CSV_PATH, header=None)\n",
    "df.columns = ['table_file', 'dbpedia_class', 'dbpedia_url']\n",
    "df['table_id'] = df['table_file'].str.replace('.tar.gz', '', regex=False)\n",
    "mapping = df.set_index('table_id')[['dbpedia_class', 'dbpedia_url']].to_dict('index')\n",
    "\n",
    "output = []\n",
    "\n",
    "for table_id, meta in mapping.items():\n",
    "    json_path = os.path.join(TABLES_FOLDER, f\"{table_id}.json\")\n",
    "    if not os.path.isfile(json_path):\n",
    "        print(f\"Warning: {json_path} not found, skipping.\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        with open(json_path, 'r', encoding='utf-8') as f:\n",
    "            table_data = json.load(f)\n",
    "    except UnicodeDecodeError:\n",
    "        with open(json_path, 'r', encoding='latin-1') as f:\n",
    "            table_data = json.load(f)\n",
    "\n",
    "    relation = table_data.get('relation', [])\n",
    "    # The correct: first element of each row in 'relation' is the column name\n",
    "    for row in relation:\n",
    "        if row:  # not empty\n",
    "            colname = row[0]\n",
    "            output.append({\n",
    "                'table_id': table_id,\n",
    "                'column_name': colname,\n",
    "                'dbpedia_class': meta['dbpedia_class'],\n",
    "                'dbpedia_url': meta['dbpedia_url']\n",
    "            })\n",
    "\n",
    "out_df = pd.DataFrame(output)\n",
    "out_df.to_csv('t2dv2_table_columns_dbpedia.csv', index=False)\n",
    "print(\"Done! Output written to t2dv2_table_columns_dbpedia.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94844153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grouped result:\n",
      "                                 index                            name  \\\n",
      "0  10151359_0_8168779773862259178.json  10151359_0_8168779773862259178   \n",
      "1  10579449_0_1681126353774891032.json  10579449_0_1681126353774891032   \n",
      "2  10630177_0_4831842476649004753.json  10630177_0_4831842476649004753   \n",
      "3  11278409_0_3742771475298785475.json  11278409_0_3742771475298785475   \n",
      "4   1146722_1_7558140036342906956.json   1146722_1_7558140036342906956   \n",
      "\n",
      "        area  ColumnCount  \n",
      "0       Book            3  \n",
      "1  Newspaper            3  \n",
      "2   Building            7  \n",
      "3    Company            5  \n",
      "4   Mountain            7  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def group_t2dv2_columns(input_path: str, output_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Reads the Excel file at input_path, groups by ['index','name','area'],\n",
    "    counts the number of rows per group, and writes the result to output_path.\n",
    "    Returns the grouped DataFrame.\n",
    "    \"\"\"\n",
    "    # 1. Load the data\n",
    "    df = pd.read_excel(input_path)\n",
    "\n",
    "    # 2. Group by index, name, area and count rows\n",
    "    grouped = (\n",
    "        df\n",
    "        .groupby(['index', 'name', 'area'])\n",
    "        .size()\n",
    "        .reset_index(name='ColumnCount')\n",
    "    )\n",
    "\n",
    "    # 3. (Optional) Save to a new Excel file\n",
    "    grouped.to_excel(output_path, index=False)\n",
    "    return grouped\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_file  = \"t2dv2Columns.xlsx\"\n",
    "    output_file = \"Grouped_t2dv2Columns.xlsx\"\n",
    "    result = group_t2dv2_columns(input_file, output_file)\n",
    "    print(\"Grouped result:\")\n",
    "    print(result.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78ddefa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import difflib\n",
    "from gensim.models.fasttext import load_facebook_vectors\n",
    "\n",
    "# Load DBpedia ontology terms\n",
    "dbpedia_terms_df = pd.read_excel('dbpedia.xlsx')\n",
    "dbpedia_labels = dbpedia_terms_df['cleaned_label'].astype(str).str.lower().tolist()\n",
    "dbpedia_ids = dbpedia_terms_df['id'].astype(str).tolist()\n",
    "dbpedia_label_to_id = dict(zip(dbpedia_labels, dbpedia_ids))\n",
    "\n",
    "# Load FastText model (English)\n",
    "fasttext_model = load_facebook_vectors('cc.en.300.bin')  # update to your FastText path\n",
    "\n",
    "def get_embedding(text):\n",
    "    words = text.split()\n",
    "    vectors = [fasttext_model[w] for w in words if w in fasttext_model]\n",
    "    if not vectors:\n",
    "        return np.zeros(fasttext_model.vector_size)\n",
    "    return np.mean(vectors, axis=0)\n",
    "\n",
    "# Precompute embeddings for ontology labels\n",
    "dbpedia_embeddings = {label: get_embedding(label) for label in dbpedia_labels}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb69e0a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done! Output written to t2dv2_table_main_column_semantic.csv\n"
     ]
    }
   ],
   "source": [
    "def syntactic_match(header, labels_list, label_to_id, threshold=0.9):\n",
    "    header = header.strip().lower()\n",
    "    if header in label_to_id:\n",
    "        return label_to_id[header]\n",
    "    matches = difflib.get_close_matches(header, labels_list, n=1, cutoff=threshold)\n",
    "    if matches:\n",
    "        return label_to_id[matches[0]]\n",
    "    return None\n",
    "\n",
    "def semantic_match(header, labels_list, label_to_id, embeddings_dict, similarity_threshold=0.85):\n",
    "    header_emb = get_embedding(header)\n",
    "    best_sim = -1\n",
    "    best_label = None\n",
    "    for label, emb in embeddings_dict.items():\n",
    "        sim = np.dot(header_emb, emb) / (np.linalg.norm(header_emb) * np.linalg.norm(emb) + 1e-8)\n",
    "        if sim > best_sim:\n",
    "            best_sim = sim\n",
    "            best_label = label\n",
    "    if best_sim >= similarity_threshold:\n",
    "        return label_to_id[best_label], best_sim\n",
    "    return None, best_sim\n",
    "\n",
    "df = pd.read_csv('t2dv2_table_columns_dbpedia.csv')\n",
    "\n",
    "main_column = []\n",
    "\n",
    "for table_id, group in df.groupby('table_id'):\n",
    "    dbpedia_class = group['dbpedia_class'].iloc[0]\n",
    "    # Convert CamelCase to space-separated for a human-readable label\n",
    "    human_label = ''.join([' ' + c if c.isupper() else c for c in dbpedia_class]).strip().lower()\n",
    "    best_match = None\n",
    "    best_score = -1\n",
    "    match_type = None\n",
    "\n",
    "    # First try syntactic match for each column\n",
    "    for col in group['column_name'].dropna().astype(str):\n",
    "        syn_match = syntactic_match(col, dbpedia_labels, dbpedia_label_to_id)\n",
    "        if syn_match:\n",
    "            best_match = col\n",
    "            best_score = 1.0\n",
    "            match_type = 'syntactic'\n",
    "            break\n",
    "\n",
    "    # Then semantic if no syntactic found\n",
    "    if best_match is None:\n",
    "        for col in group['column_name'].dropna().astype(str):\n",
    "            sem_match, sim = semantic_match(col, dbpedia_labels, dbpedia_label_to_id, dbpedia_embeddings, similarity_threshold=0.8)\n",
    "            if sim > best_score:\n",
    "                best_match = col\n",
    "                best_score = sim\n",
    "                match_type = 'semantic'\n",
    "\n",
    "    # Fallback: substring match to the human label\n",
    "    if best_match is None:\n",
    "        for col in group['column_name'].dropna().astype(str):\n",
    "            if any(word in col.lower() for word in human_label.split()):\n",
    "                best_match = col\n",
    "                best_score = 0.5\n",
    "                match_type = 'fallback_label'\n",
    "                break\n",
    "\n",
    "    main_column.append({\n",
    "        'table_id': table_id,\n",
    "        'dbpedia_class': dbpedia_class,\n",
    "        'dbpedia_url': group['dbpedia_url'].iloc[0],\n",
    "        'main_column': best_match if best_match is not None else '',\n",
    "        'match_score': best_score,\n",
    "        'match_type': match_type if match_type else ''\n",
    "    })\n",
    "\n",
    "df_main = pd.DataFrame(main_column)\n",
    "df_main.to_csv('t2dv2_table_main_column_semantic.csv', index=False)\n",
    "print(\"Done! Output written to t2dv2_table_main_column_semantic.csv\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
