{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "01 Define dataset index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1971,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "222\n",
      "Dataset#: 222 \n",
      "Last run on: 2025-04-04 19:28:20\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from typing import Union, Tuple, List, Dict\n",
    "import logging\n",
    "import re\n",
    "import numpy as np\n",
    "import string\n",
    "from IPython.display import display, HTML\n",
    "from datetime import datetime, date\n",
    "\n",
    "desired_dataset_index = input(\"Insert number for dataset or Database.table for table\")\n",
    "\n",
    "print(desired_dataset_index)\n",
    "  \n",
    "try:\n",
    "    desired_dataset_index.isdigit()\n",
    "    desired_dataset_index = int(desired_dataset_index)\n",
    "    print(f\"Dataset#: {desired_dataset_index} \")\n",
    "    DB_or_dataset = \"Dataset\"\n",
    "    datasets_file_path = 'Fiftydatasets.xlsx'  \n",
    "    analysed_columns_file_path = 'AnalysedColumns.xlsx'     \n",
    "except ValueError:\n",
    "    print(f\"Database.table: {desired_dataset_index} \")\n",
    "    DB_or_dataset = \"DB\"\n",
    "    datasets_file_path = 'all_selected_databases_info.xlsx'\n",
    "    analysed_columns_file_path = 'AnalysedColumnsDB.xlsx'\n",
    "\n",
    "from datetime import datetime\n",
    "print(f\"Last run on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "# Load the sheet to obtain the details of all datasets\n",
    "datasets_df = pd.read_excel(datasets_file_path)\n",
    "datasets_df.head()\n",
    "\n",
    "analysed_columns_df = pd.read_excel(analysed_columns_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1972,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "desired_dataset_index: 222 Last run on: 2025-04-04 19:28:21\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>name</th>\n",
       "      <th>area</th>\n",
       "      <th>Original Column</th>\n",
       "      <th>ID</th>\n",
       "      <th>Column</th>\n",
       "      <th>Description</th>\n",
       "      <th>CleanedColumn</th>\n",
       "      <th>ColumnKeyword</th>\n",
       "      <th>ColumnFormat</th>\n",
       "      <th>DescriptionKeyword</th>\n",
       "      <th>DescriptionFormat</th>\n",
       "      <th>FinalFormat</th>\n",
       "      <th>SourceKeyword</th>\n",
       "      <th>FinalFormat.1</th>\n",
       "      <th>SourceKeyword.1</th>\n",
       "      <th>Unnamed: 16</th>\n",
       "      <th>Unnamed: 17</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>52</td>\n",
       "      <td>Iris</td>\n",
       "      <td>Life</td>\n",
       "      <td>1.       sepal length in cm</td>\n",
       "      <td>1</td>\n",
       "      <td>sepal length in cm</td>\n",
       "      <td>NaN</td>\n",
       "      <td>sepal length in cm</td>\n",
       "      <td>length</td>\n",
       "      <td>numerical&gt;=0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>numerical&gt;=0</td>\n",
       "      <td>length</td>\n",
       "      <td>numerical&gt;=0</td>\n",
       "      <td>length</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>52</td>\n",
       "      <td>Iris</td>\n",
       "      <td>Life</td>\n",
       "      <td>2.       sepal width in cm</td>\n",
       "      <td>2</td>\n",
       "      <td>sepal width in cm</td>\n",
       "      <td>NaN</td>\n",
       "      <td>sepal width in cm</td>\n",
       "      <td>width</td>\n",
       "      <td>numerical&gt;=0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>numerical&gt;=0</td>\n",
       "      <td>width</td>\n",
       "      <td>numerical&gt;=0</td>\n",
       "      <td>width</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>52</td>\n",
       "      <td>Iris</td>\n",
       "      <td>Life</td>\n",
       "      <td>3.       petal length in cm</td>\n",
       "      <td>3</td>\n",
       "      <td>petal length in cm</td>\n",
       "      <td>NaN</td>\n",
       "      <td>petal length in cm</td>\n",
       "      <td>length</td>\n",
       "      <td>numerical&gt;=0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>numerical&gt;=0</td>\n",
       "      <td>length</td>\n",
       "      <td>numerical&gt;=0</td>\n",
       "      <td>length</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>52</td>\n",
       "      <td>Iris</td>\n",
       "      <td>Life</td>\n",
       "      <td>4.       petal width in cm</td>\n",
       "      <td>4</td>\n",
       "      <td>petal width in cm</td>\n",
       "      <td>NaN</td>\n",
       "      <td>petal width in cm</td>\n",
       "      <td>width</td>\n",
       "      <td>numerical&gt;=0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>numerical&gt;=0</td>\n",
       "      <td>width</td>\n",
       "      <td>numerical&gt;=0</td>\n",
       "      <td>width</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>52</td>\n",
       "      <td>Iris</td>\n",
       "      <td>Life</td>\n",
       "      <td>5.       class</td>\n",
       "      <td>5</td>\n",
       "      <td>class</td>\n",
       "      <td>NaN</td>\n",
       "      <td>class</td>\n",
       "      <td>class</td>\n",
       "      <td>categorical</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>categorical</td>\n",
       "      <td>class</td>\n",
       "      <td>categorical</td>\n",
       "      <td>class</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  name  area              Original Column  ID              Column  \\\n",
       "0     52  Iris  Life  1.       sepal length in cm   1  sepal length in cm   \n",
       "1     52  Iris  Life   2.       sepal width in cm   2   sepal width in cm   \n",
       "2     52  Iris  Life  3.       petal length in cm   3  petal length in cm   \n",
       "3     52  Iris  Life   4.       petal width in cm   4   petal width in cm   \n",
       "4     52  Iris  Life               5.       class   5               class   \n",
       "\n",
       "  Description       CleanedColumn ColumnKeyword  ColumnFormat  \\\n",
       "0         NaN  sepal length in cm        length  numerical>=0   \n",
       "1         NaN   sepal width in cm         width  numerical>=0   \n",
       "2         NaN  petal length in cm        length  numerical>=0   \n",
       "3         NaN   petal width in cm         width  numerical>=0   \n",
       "4         NaN               class         class   categorical   \n",
       "\n",
       "  DescriptionKeyword DescriptionFormat   FinalFormat SourceKeyword  \\\n",
       "0                NaN               NaN  numerical>=0        length   \n",
       "1                NaN               NaN  numerical>=0         width   \n",
       "2                NaN               NaN  numerical>=0        length   \n",
       "3                NaN               NaN  numerical>=0         width   \n",
       "4                NaN               NaN   categorical         class   \n",
       "\n",
       "  FinalFormat.1 SourceKeyword.1  Unnamed: 16  Unnamed: 17  \n",
       "0  numerical>=0          length         True         True  \n",
       "1  numerical>=0           width         True         True  \n",
       "2  numerical>=0          length         True         True  \n",
       "3  numerical>=0           width         True         True  \n",
       "4   categorical           class         True         True  "
      ]
     },
     "execution_count": 1972,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analysed_columns_df = pd.read_excel(analysed_columns_file_path)\n",
    "\n",
    "# Display the first few rows to verify the loaded data\n",
    "print(f\"desired_dataset_index: {desired_dataset_index} Last run on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "analysed_columns_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "025 Get Database Table Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1973,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymysql\n",
    "import pandas as pd\n",
    "\n",
    "def get_db_table_content(index: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Connect to the database and retrieve the content of the specified table.\n",
    "    \n",
    "    Parameters:\n",
    "    - index (str): The index in the format 'database.table'\n",
    "    \n",
    "    Returns:\n",
    "    - pd.DataFrame: The content of the table as a DataFrame\n",
    "    \"\"\"\n",
    "    host = 'db.relational-data.org'\n",
    "    port = 3306\n",
    "    user = 'guest'\n",
    "    password = 'relational'\n",
    "\n",
    "    db_name, table_name = index.split('.')\n",
    "    \n",
    "    try:\n",
    "        connection = pymysql.connect(host=host, port=port, user=user, password=password, database=db_name)\n",
    "        with connection.cursor() as cursor:\n",
    "            cursor.execute(f\"SELECT * FROM `{table_name}`\")\n",
    "            data = cursor.fetchall()\n",
    "            columns = [column[0] for column in cursor.description]\n",
    "        \n",
    "        df = pd.DataFrame(data, columns=columns)\n",
    "        print(f\"Successfully retrieved data from {index}\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error retrieving data from {index}: {str(e)}\")\n",
    "        return pd.DataFrame()\n",
    "    finally:\n",
    "        if 'connection' in locals():\n",
    "            connection.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "03 Get dataset file URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1974,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset file URL or database index for dataset Bank Marketing (index 222) is: https://archive.ics.uci.edu/ml/machine-learning-databases/00222/bank-additional.zip\n",
      "Dataset\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from typing import Union, Tuple, List\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "def get_dataset_file_url(excel_file_path: str, dataset_index: Union[int, str], DB_or_dataset: str) -> Union[Tuple[str, str, List[str]], Exception]:\n",
    "    \"\"\"\n",
    "    Load dataset details from an Excel file and return the dataset file URL and name for a specific dataset index.\n",
    "\n",
    "    Parameters:\n",
    "    - excel_file_path (str): The path to the Excel file containing dataset details.\n",
    "    - dataset_index (int or str): The index of the dataset or the database table name.\n",
    "    - DB_or_dataset (str): Indicates whether we're dealing with a dataset or a database table.\n",
    "\n",
    "    Returns:\n",
    "    - Tuple or Exception: A tuple containing the dataset file URL, name, and primary key columns (for DB only), or an exception if something goes wrong.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        datasets_df = pd.read_excel(excel_file_path)\n",
    "        \n",
    "        if 'index' not in datasets_df.columns or 'name' not in datasets_df.columns:\n",
    "            return \"Required columns 'index' and 'name' do not exist in the DataFrame\"\n",
    "\n",
    "        if DB_or_dataset == \"Dataset\":\n",
    "            # Dataset case\n",
    "            dataset_index = int(dataset_index)\n",
    "            dataset_details = datasets_df.loc[datasets_df['index'] == dataset_index]\n",
    "            if dataset_details.empty:\n",
    "                return f\"No dataset found with index {dataset_index}\"\n",
    "\n",
    "            dataset_name = dataset_details['name'].values[0]\n",
    "            dataset_file_url = dataset_details['dataset_file_url'].values[0]\n",
    "            \n",
    "            # Datasets don't have primary keys\n",
    "            primary_key_columns = []\n",
    "\n",
    "        elif DB_or_dataset == \"DB\":\n",
    "            # Database table case\n",
    "            dataset_details = datasets_df.loc[datasets_df['index'] == dataset_index]\n",
    "            if dataset_details.empty:\n",
    "                return f\"No dataset found with index {dataset_index}\"\n",
    "\n",
    "            dataset_name = dataset_details['name'].values[0]\n",
    "            dataset_file_url = dataset_index  # For database tables, the index is the URL\n",
    "            \n",
    "            # Extract primary key columns for DB tables\n",
    "            if 'primary_key' in datasets_df.columns:\n",
    "                primary_key = dataset_details['primary_key'].values[0]\n",
    "                primary_key_columns = [col.strip() for col in str(primary_key).split(',')] if pd.notna(primary_key) else []\n",
    "            else:\n",
    "                primary_key_columns = []\n",
    "                print(\"No primary key column found in the DataFrame\")\n",
    "        \n",
    "        else:\n",
    "            return f\"Invalid DB_or_dataset value: {DB_or_dataset}\"\n",
    "\n",
    "        return (dataset_file_url, dataset_name, primary_key_columns)\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An error occurred: {e}\")\n",
    "        return e\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    excel_file_path = datasets_file_path\n",
    "    dataset_details = get_dataset_file_url(excel_file_path, desired_dataset_index, DB_or_dataset)\n",
    "    \n",
    "    if isinstance(dataset_details, tuple):\n",
    "        dataset_file_url, dataset_name, primary_key_columns = dataset_details\n",
    "        print(f\"The dataset file URL or database index for dataset {dataset_name} (index {desired_dataset_index}) is: {dataset_file_url}\")\n",
    "        if DB_or_dataset == \"DB\":\n",
    "            print(f\"Primary key columns: {primary_key_columns}\")\n",
    "        print(DB_or_dataset)\n",
    "    else:\n",
    "        print(f\"An error occurred or the dataset was not found: {dataset_details}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "04 Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1975,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset file URL or database index for dataset Bank Marketing (index 222) is: https://archive.ics.uci.edu/ml/machine-learning-databases/00222/bank-additional.zip\n",
      "Primary key columns: []\n",
      "Dataset\n",
      "1: bank-additional\\.DS_Store\n",
      "2: bank-additional\\.Rhistory\n",
      "3: bank-additional\\bank-additional-full.csv\n",
      "4: bank-additional\\bank-additional-names.txt\n",
      "5: bank-additional\\bank-additional.csv\n",
      "6: __MACOSX\\._bank-additional\n",
      "7: __MACOSX\\bank-additional\\._.DS_Store\n",
      "Selected File: bank-additional\\bank-additional-full.csv\n",
      "First row for inspection: \"age\";\"job\";\"marital\";\"education\";\"default\";\"housing\";\"loan\";\"contact\";\"month\";\"day_of_week\";\"duration\";\"campaign\";\"pdays\";\"previous\";\"poutcome\";\"emp.var.rate\";\"cons.price.idx\";\"cons.conf.idx\";\"euribor3m\";\"nr.employed\";\"y\"\n",
      "\n",
      "Is first row header ? True\n",
      "Dataset loaded successfully.\n",
      "   age        job  marital    education  default housing loan    contact  \\\n",
      "0   56  housemaid  married     basic.4y       no      no   no  telephone   \n",
      "1   57   services  married  high.school  unknown      no   no  telephone   \n",
      "2   37   services  married  high.school       no     yes   no  telephone   \n",
      "3   40     admin.  married     basic.6y       no      no   no  telephone   \n",
      "4   56   services  married  high.school       no      no  yes  telephone   \n",
      "\n",
      "  month day_of_week  ...  campaign  pdays  previous     poutcome emp.var.rate  \\\n",
      "0   may         mon  ...         1    999         0  nonexistent          1.1   \n",
      "1   may         mon  ...         1    999         0  nonexistent          1.1   \n",
      "2   may         mon  ...         1    999         0  nonexistent          1.1   \n",
      "3   may         mon  ...         1    999         0  nonexistent          1.1   \n",
      "4   may         mon  ...         1    999         0  nonexistent          1.1   \n",
      "\n",
      "   cons.price.idx  cons.conf.idx  euribor3m  nr.employed   y  \n",
      "0          93.994          -36.4      4.857       5191.0  no  \n",
      "1          93.994          -36.4      4.857       5191.0  no  \n",
      "2          93.994          -36.4      4.857       5191.0  no  \n",
      "3          93.994          -36.4      4.857       5191.0  no  \n",
      "4          93.994          -36.4      4.857       5191.0  no  \n",
      "\n",
      "[5 rows x 21 columns]\n",
      "Last run on: 2025-04-04 19:28:36\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import io\n",
    "from io import BytesIO\n",
    "import zipfile\n",
    "import tarfile\n",
    "import gzip\n",
    "import logging\n",
    "import csv\n",
    "from typing import Optional, Union\n",
    "import os\n",
    "import tempfile\n",
    "import shutil\n",
    "import sys\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "def is_header_for_csv(line, delimiter=' '):\n",
    "    \"\"\"\n",
    "    Determine if a line is likely a header by checking if there are no numeric values.\n",
    "    If at least one numeric value is found, the line is considered not a header (i.e., a data line).\n",
    "    \"\"\"\n",
    "    # Regex to match quoted strings or non-whitespace sequences\n",
    "    pattern = re.compile(r'\\\".*?\\\"|\\S+')\n",
    "\n",
    "    elements = pattern.findall(line.replace(delimiter, ' '))  # Replace delimiter with space for easier parsing\n",
    "\n",
    "    # Check if any element is a number\n",
    "    is_numeric_present = any(element.replace('.', '', 1).lstrip('-').isdigit() for element in elements)\n",
    "\n",
    "    # Check for replicated elements\n",
    "    unique_elements = set(elements)\n",
    "    replicated_elements = len(elements) - len(unique_elements)\n",
    "\n",
    "    # If no numeric value is found and there are replicated elements, consider this a header line\n",
    "    is_header = not is_numeric_present and replicated_elements == 0\n",
    "\n",
    "    return is_header\n",
    "\n",
    "\n",
    "def load_csv(file_content: Union[str, bytes], header: Optional[int], na_values: Optional[Union[str, list]]) -> pd.DataFrame:\n",
    "    # Adjust to decode bytes if necessary\n",
    "    if isinstance(file_content, bytes):\n",
    "        try:\n",
    "            file_content_decoded = file_content.decode('utf-8')\n",
    "        except UnicodeDecodeError:\n",
    "            file_content_decoded = file_content.decode('ISO-8859-1')\n",
    "    else:\n",
    "        file_content_decoded = file_content\n",
    "    \n",
    "    # Preprocess to replace multiple tabs with a single tab\n",
    "    #file_content_processed = re.sub('\\t+', '\\t', file_content_decoded)\n",
    "    file_content_processed = re.sub('[ \\t]+', ' ', file_content_decoded)\n",
    "\n",
    "    file_stream = io.StringIO(file_content_processed)\n",
    "        \n",
    "    try:\n",
    "        dialect = csv.Sniffer().sniff(file_stream.readline())\n",
    "        delimiter = dialect.delimiter\n",
    "        file_stream.seek(0)\n",
    "    except csv.Error:\n",
    "        delimiter = ','\n",
    "        logging.info(\"Falling back to default delimiter ',' due to detection failure.\")\n",
    "  \n",
    "    # Use the heuristic to decide if the first line is likely a header\n",
    "    first_line = file_stream.readline()\n",
    "    # Print the first row for inspection\n",
    "    print(\"First row for inspection:\", first_line)\n",
    "    is_header_row = is_header_for_csv(first_line, delimiter)\n",
    "    print('Is first row header ?',is_header_row )\n",
    "    file_stream.seek(0)  # Reset to start of file after reading the first line\n",
    "    \n",
    "    header_decision = 0 if is_header_for_csv(first_line, delimiter) else None\n",
    "    #header_decision = None if is_data_row else 0\n",
    "    # Regex pattern to match quoted strings or non-whitespace sequences\n",
    "    pattern = re.compile(r'\\\".*?\\\"|\\S+')\n",
    "\n",
    "    df = pd.read_csv(file_stream, header=header_decision, delimiter=delimiter, na_values=na_values, keep_default_na=False)\n",
    "   \n",
    "  # Check if the DataFrame needs re-parsing with regex pattern\n",
    "    if len(df.columns) < 2:\n",
    "        file_stream.seek(0)\n",
    "        lines = file_stream.readlines()\n",
    "        parsed_data = [pattern.findall(line) for line in lines]\n",
    "\n",
    "        if header is None and header_decision == 0:\n",
    "            header_row = parsed_data.pop(0)  # Use the first row as header\n",
    "        else:\n",
    "            header_row = None  # Let pandas create default headers or use provided header index\n",
    "\n",
    "        df = pd.DataFrame(parsed_data, columns=header_row)\n",
    "        df = df.apply(pd.to_numeric, errors='ignore')  # Attempt to correct data types\n",
    "    return df\n",
    "\n",
    "def is_header_for_excel(first_row: pd.DataFrame) -> bool:\n",
    "    \"\"\"\n",
    "    Determines if the first row of a DataFrame is likely to be a header by checking the entire row's content.\n",
    "\n",
    "    Args:\n",
    "        first_row (pd.DataFrame): DataFrame containing at least one row of data to inspect.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the first row is likely a header (i.e., contains mostly non-numeric data), False if it's likely data.\n",
    "    \"\"\"\n",
    "    # Check if the majority of the items in the first row are non-numeric\n",
    "    non_numeric_count = first_row.iloc[0].apply(lambda x: not isinstance(x, (int, float))).sum()\n",
    "    \n",
    "    # Determine if the first row is likely a header based on the proportion of non-numeric items\n",
    "    is_header = non_numeric_count > len(first_row.columns) / 2\n",
    "    return is_header\n",
    "\n",
    "\n",
    "def load_excel(file_content: Union[str, bytes, io.BytesIO], na_values: Optional[Union[str, list]] = None, skip_rows: Union[int, list] = 0, parse_dates: bool = False) -> pd.DataFrame:\n",
    "    # Ajuste para o file_content ser um objeto BytesIO, se for bytes\n",
    "    if isinstance(file_content, bytes):\n",
    "        file_content = BytesIO(file_content)  # Envolve bytes em BytesIO\n",
    "    try:\n",
    "        # Load the first row to check if it's likely to be a header\n",
    "        first_row = pd.read_excel(file_content, nrows=1, header=None)\n",
    "        \n",
    "        # Print the first row for inspection\n",
    "        print(\"First row for inspection:\", first_row.iloc[0].values)\n",
    "\n",
    "        # Execute the heuristic\n",
    "        likely_header = is_header_for_excel(first_row)\n",
    "\n",
    "        # Decide on using the first row as header based on heuristic\n",
    "        #header_decision = None if likely_header else 0  \n",
    "        header_decision = 0 if likely_header else None\n",
    "\n",
    "        # Reset file_content to read from the beginning if it's a stream\n",
    "        if isinstance(file_content, io.BytesIO):\n",
    "            print(\"Resetting stream position\")  # Diagnostic print\n",
    "            file_content.seek(0)\n",
    "\n",
    "        # Load the full Excel file with determined header option\n",
    "        df = pd.read_excel(file_content, na_values=na_values, skiprows=skip_rows, header=header_decision, parse_dates=parse_dates)\n",
    "        return df\n",
    "    except ValueError as e:\n",
    "        logging.error(f\"Excel loading error: {e}\")\n",
    "        return pd.DataFrame()\n",
    "        \n",
    "def download_and_extract(url: str) -> str:\n",
    "    \"\"\"\n",
    "    Downloads an archive from the given URL and extracts it into a temporary directory.\n",
    "    Handles .zip, .tar.gz, and .gz files. Returns the path to the directory or file.\n",
    "    \"\"\"\n",
    "    temp_dir = tempfile.mkdtemp()\n",
    "    response = requests.get(url, stream=True)\n",
    "    file_name = os.path.basename(url)\n",
    "    temp_file_path = os.path.join(temp_dir, file_name)\n",
    "    \n",
    "    with open(temp_file_path, 'wb') as file:\n",
    "        shutil.copyfileobj(response.raw, file)\n",
    "    \n",
    "    if file_name.endswith('.zip'):\n",
    "        with zipfile.ZipFile(temp_file_path, 'r') as archive:\n",
    "            archive.extractall(temp_dir)\n",
    "    elif file_name.endswith(('.tar.gz', '.tgz')):\n",
    "        with tarfile.open(temp_file_path, 'r:gz') as archive:\n",
    "            archive.extractall(temp_dir)\n",
    "    elif file_name.endswith('.gz'):\n",
    "        # Handle single .gz files by extracting to the same directory\n",
    "        extracted_file_path = temp_file_path[:-3]  # Remove .gz extension\n",
    "        with gzip.open(temp_file_path, 'rb') as f_in, open(extracted_file_path, 'wb') as f_out:\n",
    "            shutil.copyfileobj(f_in, f_out)\n",
    "        os.remove(temp_file_path)  # Remove the original .gz file\n",
    "        return extracted_file_path  # Direct path to the extracted file for .gz\n",
    "    \n",
    "    os.remove(temp_file_path)  # Clean up archive file after extraction\n",
    "    return temp_dir  # Path to directory with extracted files\n",
    "\n",
    "def select_file_from_extracted(directory: str) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Recursively lists all files in the extracted directory and prompts the user to select one.\n",
    "    \"\"\"\n",
    "    files_list = []\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            files_list.append(os.path.join(root, file))\n",
    "\n",
    "    if not files_list:\n",
    "        logging.error(\"No files found in the directory.\")\n",
    "        return None\n",
    "\n",
    "    for index, file in enumerate(files_list, start=1):\n",
    "        relative_path = os.path.relpath(file, directory)  # Show relative path for clarity\n",
    "        print(f\"{index}: {relative_path}\", flush=True)\n",
    "\n",
    "    try:\n",
    "        file_index = int(input(\"Enter the number of the file you want to load: \")) - 1\n",
    "        if 0 <= file_index < len(files_list):\n",
    "            selected_relative_path = os.path.relpath(files_list[file_index], directory)  # Get relative path\n",
    "            print(f\"Selected File: {selected_relative_path}\")  # Print relative path\n",
    "            return files_list[file_index]  # Return full path for further processing\n",
    "        else:\n",
    "            logging.error(\"Invalid selection.\")\n",
    "    except ValueError:\n",
    "        logging.error(\"Please enter a valid number.\")\n",
    "    \n",
    "    return None\n",
    "\n",
    "def fetch_file_content(url: str) -> str:\n",
    "    response = requests.get(url, stream=True)\n",
    "    try:\n",
    "        content = response.content.decode('utf-8')\n",
    "    except UnicodeDecodeError:\n",
    "        content = response.content.decode('ISO-8859-1')\n",
    "    return content\n",
    "\n",
    "   \n",
    "def load_dataset(file_path_or_url: str, DB_or_dataset: str, na_values: Optional[Union[str, list]] = None, skip_rows: Union[int, list] = 0, parse_dates: bool = False) -> Union[pd.DataFrame, str]:\n",
    "\n",
    "    if DB_or_dataset == \"DB\":\n",
    "        try:\n",
    "            df = get_db_table_content(file_path_or_url)\n",
    "            if len(primary_key_columns) > 1:\n",
    "                composite_pk_name = '|'.join(primary_key_columns)\n",
    "                df[composite_pk_name] = df[primary_key_columns].astype(str).agg('|'.join, axis=1)\n",
    "                # Move the composite key column to the front\n",
    "                cols = df.columns.tolist()\n",
    "                cols = [composite_pk_name] + [col for col in cols if col != composite_pk_name]\n",
    "                df = df[cols]\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Database load error: {e}\")\n",
    "            return None\n",
    "        \n",
    "    # Handle files or URLs       \n",
    "    elif file_path_or_url.startswith(('http://', 'https://')):\n",
    "        if file_path_or_url.endswith(('.csv', '.txt', '.data')):\n",
    "            response = requests.get(file_path_or_url)\n",
    "            file_content = response.content\n",
    "            return load_csv(file_content, header=None, na_values=na_values)\n",
    "        elif file_path_or_url.endswith(('.xlsx', '.xls')):\n",
    "            response = requests.get(file_path_or_url)\n",
    "            return load_excel(response.content, na_values=na_values, skip_rows=skip_rows, parse_dates=parse_dates)\n",
    "    else:\n",
    "        # Handle local file path\n",
    "        if os.path.exists(file_path_or_url):\n",
    "            if file_path_or_url.endswith(('.csv', '.txt', '.data')):\n",
    "                with open(file_path_or_url, 'r', encoding='utf-8') as f:\n",
    "                    file_content = f.read()\n",
    "                return load_csv(file_content, header=None, na_values=na_values)\n",
    "            elif file_path_or_url.endswith(('.xlsx', '.xls')):\n",
    "                with open(file_path_or_url, 'rb') as f:\n",
    "                    file_content = f.read()\n",
    "                return load_excel(file_content, na_values=na_values, skip_rows=skip_rows, parse_dates=parse_dates)\n",
    "        else:\n",
    "            logging.error(f\"File does not exist: {file_path_or_url}\")\n",
    "            return None\n",
    "    # Handling archives\n",
    "    extracted_path = download_and_extract(file_path_or_url)\n",
    "    if os.path.isdir(extracted_path):\n",
    "        selected_file = select_file_from_extracted(extracted_path)\n",
    "        if selected_file and selected_file.endswith(('.csv', '.txt', '.data')):\n",
    "            with open(selected_file, 'rb') as f:\n",
    "                file_content = f.read()\n",
    "            return load_csv(file_content, header=None, na_values=na_values)\n",
    "        elif selected_file and selected_file.endswith(('.xlsx', '.xls')):\n",
    "            with open(selected_file, 'rb') as f:\n",
    "                file_content = f.read()\n",
    "            return load_excel(file_content, na_values=na_values, skip_rows=skip_rows, parse_dates=parse_dates)\n",
    "    elif os.path.isfile(extracted_path):\n",
    "        if extracted_path.endswith(('.csv', '.txt', '.data')):\n",
    "            with open(extracted_path, 'rb') as f:\n",
    "                file_content = f.read()\n",
    "            return load_csv(file_content, header=None, na_values=na_values)\n",
    "        elif extracted_path.endswith(('.xlsx', '.xls')):\n",
    "            with open(extracted_path, 'rb') as f:\n",
    "                file_content = f.read()\n",
    "            return load_excel(file_content, na_values=na_values, skip_rows=skip_rows, parse_dates=parse_dates)\n",
    "        else:\n",
    "            # Attempt to load the file without assuming an extension, particularly useful for .gz extracted files\n",
    "            with open(extracted_path, 'rb') as f:\n",
    "                file_content = f.read()\n",
    "            try:\n",
    "                # Attempt to load as CSV first; this part assumes CSV if no extension is found\n",
    "                return load_csv(file_content, header=None, na_values=na_values)\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Failed to automatically determine file type for {extracted_path}: {e}\")\n",
    "                return None\n",
    "    return None\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    excel_file_path = datasets_file_path\n",
    "    dataset_details = get_dataset_file_url(excel_file_path, desired_dataset_index, DB_or_dataset)\n",
    "\n",
    "    if isinstance(dataset_details, tuple):\n",
    "        dataset_file_url, dataset_name, primary_key_columns = dataset_details\n",
    "        print(f\"The dataset file URL or database index for dataset {dataset_name} (index {desired_dataset_index}) is: {dataset_file_url}\")\n",
    "        print(f\"Primary key columns: {primary_key_columns}\")\n",
    "        print(DB_or_dataset)\n",
    "        dataset_df = load_dataset(dataset_file_url, DB_or_dataset, primary_key_columns)\n",
    "        if dataset_df is not None:\n",
    "            print(\"Dataset loaded successfully.\")\n",
    "            if len(primary_key_columns) > 1:\n",
    "                composite_pk_name = '|'.join(primary_key_columns)\n",
    "                print(f\"\\nComposite Primary Key ({composite_pk_name}):\")\n",
    "                print(dataset_df[[composite_pk_name] + [col for col in dataset_df.columns if col != composite_pk_name]].head())\n",
    "            else:\n",
    "                print(dataset_df.head())\n",
    "        else:\n",
    "            print(\"Failed to load dataset.\")\n",
    "    else:\n",
    "        print(f\"An error occurred or the dataset was not found: {dataset_details}\")\n",
    "       \n",
    "from datetime import datetime\n",
    "print(f\"Last run on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "05 Assign Column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1976,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Successfully assigned column names to the dataset 'Bank Marketing' for index 222\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assigning column names for dataset index: 222\n",
      "Type of dataset_index: <class 'int'>\n",
      "Shape of dataset_df: (41188, 21)\n",
      "Current columns of dataset_df: ['age', 'job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'month', 'day_of_week', 'duration', 'campaign', 'pdays', 'previous', 'poutcome', 'emp.var.rate', 'cons.price.idx', 'cons.conf.idx', 'euribor3m', 'nr.employed', 'y']\n",
      "Extracted column names: ['age', 'job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'month', 'day_of_week', 'duration', 'campaign', 'pdays', 'previous', 'poutcome', 'emp.var.rate', 'cons.price.idx', 'cons.conf.idx', 'euribor3m', 'nr.employed', 'y - has the client subscribed a term deposit?']\n",
      "Final columns of dataset_df: ['age', 'job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'month', 'day_of_week', 'duration', 'campaign', 'pdays', 'previous', 'poutcome', 'emp.var.rate', 'cons.price.idx', 'cons.conf.idx', 'euribor3m', 'nr.employed', 'y - has the client subscribed a term deposit?']\n",
      "Last run on: 2025-04-04 19:28:36\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>job</th>\n",
       "      <th>marital</th>\n",
       "      <th>education</th>\n",
       "      <th>default</th>\n",
       "      <th>housing</th>\n",
       "      <th>loan</th>\n",
       "      <th>contact</th>\n",
       "      <th>month</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>...</th>\n",
       "      <th>campaign</th>\n",
       "      <th>pdays</th>\n",
       "      <th>previous</th>\n",
       "      <th>poutcome</th>\n",
       "      <th>emp.var.rate</th>\n",
       "      <th>cons.price.idx</th>\n",
       "      <th>cons.conf.idx</th>\n",
       "      <th>euribor3m</th>\n",
       "      <th>nr.employed</th>\n",
       "      <th>y - has the client subscribed a term deposit?</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>56</td>\n",
       "      <td>housemaid</td>\n",
       "      <td>married</td>\n",
       "      <td>basic.4y</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>telephone</td>\n",
       "      <td>may</td>\n",
       "      <td>mon</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.857</td>\n",
       "      <td>5191.0</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>57</td>\n",
       "      <td>services</td>\n",
       "      <td>married</td>\n",
       "      <td>high.school</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>telephone</td>\n",
       "      <td>may</td>\n",
       "      <td>mon</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.857</td>\n",
       "      <td>5191.0</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>37</td>\n",
       "      <td>services</td>\n",
       "      <td>married</td>\n",
       "      <td>high.school</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>telephone</td>\n",
       "      <td>may</td>\n",
       "      <td>mon</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.857</td>\n",
       "      <td>5191.0</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>40</td>\n",
       "      <td>admin.</td>\n",
       "      <td>married</td>\n",
       "      <td>basic.6y</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>telephone</td>\n",
       "      <td>may</td>\n",
       "      <td>mon</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.857</td>\n",
       "      <td>5191.0</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>56</td>\n",
       "      <td>services</td>\n",
       "      <td>married</td>\n",
       "      <td>high.school</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>telephone</td>\n",
       "      <td>may</td>\n",
       "      <td>mon</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.857</td>\n",
       "      <td>5191.0</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41183</th>\n",
       "      <td>73</td>\n",
       "      <td>retired</td>\n",
       "      <td>married</td>\n",
       "      <td>professional.course</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>cellular</td>\n",
       "      <td>nov</td>\n",
       "      <td>fri</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>-1.1</td>\n",
       "      <td>94.767</td>\n",
       "      <td>-50.8</td>\n",
       "      <td>1.028</td>\n",
       "      <td>4963.6</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41184</th>\n",
       "      <td>46</td>\n",
       "      <td>blue-collar</td>\n",
       "      <td>married</td>\n",
       "      <td>professional.course</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>cellular</td>\n",
       "      <td>nov</td>\n",
       "      <td>fri</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>-1.1</td>\n",
       "      <td>94.767</td>\n",
       "      <td>-50.8</td>\n",
       "      <td>1.028</td>\n",
       "      <td>4963.6</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41185</th>\n",
       "      <td>56</td>\n",
       "      <td>retired</td>\n",
       "      <td>married</td>\n",
       "      <td>university.degree</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>cellular</td>\n",
       "      <td>nov</td>\n",
       "      <td>fri</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>-1.1</td>\n",
       "      <td>94.767</td>\n",
       "      <td>-50.8</td>\n",
       "      <td>1.028</td>\n",
       "      <td>4963.6</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41186</th>\n",
       "      <td>44</td>\n",
       "      <td>technician</td>\n",
       "      <td>married</td>\n",
       "      <td>professional.course</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>cellular</td>\n",
       "      <td>nov</td>\n",
       "      <td>fri</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>-1.1</td>\n",
       "      <td>94.767</td>\n",
       "      <td>-50.8</td>\n",
       "      <td>1.028</td>\n",
       "      <td>4963.6</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41187</th>\n",
       "      <td>74</td>\n",
       "      <td>retired</td>\n",
       "      <td>married</td>\n",
       "      <td>professional.course</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>cellular</td>\n",
       "      <td>nov</td>\n",
       "      <td>fri</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>999</td>\n",
       "      <td>1</td>\n",
       "      <td>failure</td>\n",
       "      <td>-1.1</td>\n",
       "      <td>94.767</td>\n",
       "      <td>-50.8</td>\n",
       "      <td>1.028</td>\n",
       "      <td>4963.6</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>41188 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       age          job  marital            education  default housing loan  \\\n",
       "0       56    housemaid  married             basic.4y       no      no   no   \n",
       "1       57     services  married          high.school  unknown      no   no   \n",
       "2       37     services  married          high.school       no     yes   no   \n",
       "3       40       admin.  married             basic.6y       no      no   no   \n",
       "4       56     services  married          high.school       no      no  yes   \n",
       "...    ...          ...      ...                  ...      ...     ...  ...   \n",
       "41183   73      retired  married  professional.course       no     yes   no   \n",
       "41184   46  blue-collar  married  professional.course       no      no   no   \n",
       "41185   56      retired  married    university.degree       no     yes   no   \n",
       "41186   44   technician  married  professional.course       no      no   no   \n",
       "41187   74      retired  married  professional.course       no     yes   no   \n",
       "\n",
       "         contact month day_of_week  ...  campaign  pdays  previous  \\\n",
       "0      telephone   may         mon  ...         1    999         0   \n",
       "1      telephone   may         mon  ...         1    999         0   \n",
       "2      telephone   may         mon  ...         1    999         0   \n",
       "3      telephone   may         mon  ...         1    999         0   \n",
       "4      telephone   may         mon  ...         1    999         0   \n",
       "...          ...   ...         ...  ...       ...    ...       ...   \n",
       "41183   cellular   nov         fri  ...         1    999         0   \n",
       "41184   cellular   nov         fri  ...         1    999         0   \n",
       "41185   cellular   nov         fri  ...         2    999         0   \n",
       "41186   cellular   nov         fri  ...         1    999         0   \n",
       "41187   cellular   nov         fri  ...         3    999         1   \n",
       "\n",
       "          poutcome emp.var.rate  cons.price.idx  cons.conf.idx  euribor3m  \\\n",
       "0      nonexistent          1.1          93.994          -36.4      4.857   \n",
       "1      nonexistent          1.1          93.994          -36.4      4.857   \n",
       "2      nonexistent          1.1          93.994          -36.4      4.857   \n",
       "3      nonexistent          1.1          93.994          -36.4      4.857   \n",
       "4      nonexistent          1.1          93.994          -36.4      4.857   \n",
       "...            ...          ...             ...            ...        ...   \n",
       "41183  nonexistent         -1.1          94.767          -50.8      1.028   \n",
       "41184  nonexistent         -1.1          94.767          -50.8      1.028   \n",
       "41185  nonexistent         -1.1          94.767          -50.8      1.028   \n",
       "41186  nonexistent         -1.1          94.767          -50.8      1.028   \n",
       "41187      failure         -1.1          94.767          -50.8      1.028   \n",
       "\n",
       "       nr.employed  y - has the client subscribed a term deposit?  \n",
       "0           5191.0                                             no  \n",
       "1           5191.0                                             no  \n",
       "2           5191.0                                             no  \n",
       "3           5191.0                                             no  \n",
       "4           5191.0                                             no  \n",
       "...            ...                                            ...  \n",
       "41183       4963.6                                            yes  \n",
       "41184       4963.6                                             no  \n",
       "41185       4963.6                                             no  \n",
       "41186       4963.6                                            yes  \n",
       "41187       4963.6                                             no  \n",
       "\n",
       "[41188 rows x 21 columns]"
      ]
     },
     "execution_count": 1976,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def assign_column_names(analysed_columns_df: pd.DataFrame, dataset_index: Union[int, str], dataset_df: pd.DataFrame, dataset_name: str = \"Unknown\") -> Union[pd.DataFrame, Exception]:\n",
    "    \"\"\"\n",
    "    Assign column names to a DataFrame based on a given dataset index from an \"AnalysedColumns\" DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - analysed_columns_df (pd.DataFrame): The DataFrame containing analysed columns information.\n",
    "    - dataset_index (Union[int, str]): The index of the dataset for which to get the column names.\n",
    "    - dataset_df (pd.DataFrame): The DataFrame to which the column names will be assigned.\n",
    "    - dataset_name (str): The name of the dataset. Default is \"Unknown\".\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame or Exception: The DataFrame with assigned column names or an exception if something goes wrong.\n",
    "    \"\"\"\n",
    "    print(f\"Assigning column names for dataset index: {dataset_index}\")\n",
    "    print(f\"Type of dataset_index: {type(dataset_index)}\")\n",
    "    print(f\"Shape of dataset_df: {dataset_df.shape}\")\n",
    "    print(f\"Current columns of dataset_df: {dataset_df.columns.tolist()}\")\n",
    "    \n",
    "    try:\n",
    "        # Check if 'index' and 'Column' columns exist in the DataFrame\n",
    "        if 'index' not in analysed_columns_df.columns or 'Column' not in analysed_columns_df.columns:\n",
    "            return f\"Required columns 'index' or 'Column' do not exist in the DataFrame\"\n",
    "\n",
    "        # Extract the column names for the given dataset index\n",
    "        column_names = analysed_columns_df.loc[analysed_columns_df['index'] == dataset_index, 'Column'].tolist()\n",
    "        print(f\"Extracted column names: {column_names}\")\n",
    "        \n",
    "        if not column_names:\n",
    "            print(f\"No column names found. Unique values in 'index' column: {analysed_columns_df['index'].unique()}\")\n",
    "            return f\"No column names found for dataset index {dataset_index}\"\n",
    "\n",
    "        # Check if there's a mismatch in the number of columns\n",
    "        if len(column_names) != len(dataset_df.columns):\n",
    "            print(f\"Warning: Mismatch in number of columns. DataFrame has {len(dataset_df.columns)} columns, but {len(column_names)} names were provided.\")\n",
    "            print(f\"Current DataFrame columns: {dataset_df.columns.tolist()}\")\n",
    "            print(f\"Provided column names: {column_names}\")\n",
    "\n",
    "            # If there's an extra column (likely the composite primary key), keep it\n",
    "            if len(dataset_df.columns) == len(column_names) + 1 and '|' in dataset_df.columns[0]:\n",
    "                composite_pk_name = dataset_df.columns[0]\n",
    "                new_column_names = [composite_pk_name] + column_names\n",
    "                dataset_df.columns = new_column_names\n",
    "                print(f\"Kept composite primary key column '{composite_pk_name}' and assigned provided names to other columns.\")\n",
    "                \n",
    "                # Remove the individual columns that make up the composite key\n",
    "                individual_pk_columns = composite_pk_name.split('|')\n",
    "                dataset_df = dataset_df.drop(columns=individual_pk_columns)\n",
    "                print(f\"Removed individual primary key columns: {individual_pk_columns}\")\n",
    "            else:\n",
    "                return f\"Cannot assign column names due to mismatch. Please check the column definitions.\"\n",
    "        else:\n",
    "            # Assign the extracted column names to the target DataFrame\n",
    "            dataset_df.columns = column_names\n",
    "\n",
    "        logging.info(f\"Successfully assigned column names to the dataset '{dataset_name}' for index {dataset_index}\")\n",
    "        print(f\"Final columns of dataset_df: {dataset_df.columns.tolist()}\")\n",
    "        return dataset_df\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An error occurred while assigning column names to the dataset '{dataset_name}': {e}\")\n",
    "        print(f\"Error details: {str(e)}\")\n",
    "        return e\n",
    "    \n",
    "# Assume analysed_columns_df contains the analysed columns information\n",
    "# Assume target_df is the DataFrame loaded previously\n",
    "# Assume dataset_name contains the name of the dataset\n",
    "\n",
    "# Call the function to assign the column names\n",
    "dataset_df = assign_column_names(analysed_columns_df, desired_dataset_index, dataset_df, dataset_name)\n",
    "\n",
    "# Display the first few rows to verify the column names have been correctly assigned\n",
    "\n",
    "\n",
    "from datetime import datetime\n",
    "print(f\"Last run on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "dataset_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List of functions in the DataQualityIssues class along with their associated DQI (Data Quality Issue) numbers:\n",
    "\n",
    "is_blank - Not directly associated with a DQI number\n",
    "\n",
    "handle_blank_empty_null_nan - DQI #1 (Missing Data - Completeness)\n",
    "\n",
    "handle_predefined_unacceptable_values - DQI #4 (Ambiguous Data - Accuracy, Consistency)\n",
    "\n",
    "handle_extraneous_data - DQI #5 (Extraneous Data - Consistency, Uniqueness)\n",
    "\n",
    "handle_street_extraneous_data - DQI #5 (Extraneous Data - Consistency, Uniqueness)\n",
    "\n",
    "standardize_date - Not directly associated with a DQI number\n",
    "\n",
    "standardize_date_time - Not directly associated with a DQI number\n",
    "\n",
    "is_valid_time - Not directly associated with a DQI number\n",
    "\n",
    "handle_outdated_temporal_data - DQI #6 (Outdated Temporal Data - Timeliness)\n",
    "\n",
    "handle_outdated_temporal_data_datetime - DQI #6 (Outdated Temporal Data - Timeliness)\n",
    "\n",
    "handle_duplicates - DQI #9 (Duplicates - Uniqueness)\n",
    "\n",
    "handle_excessive_distinct_values - DQI #10 (Structural Conflicts - Consistency, Uniqueness)\n",
    "\n",
    "handle_dates_format - DQI #14 (Different units/representations - Consistency)\n",
    "\n",
    "handle_datetimes_format - DQI #14 (Different units/representations - Consistency)\n",
    "\n",
    "handle_negative_values - DQI #15 (Domain Violation - Accuracy)\n",
    "\n",
    "handle_values_outside_range - DQI #15 (Domain Violation - Accuracy)\n",
    "\n",
    "handle_floating_point_values - DQI #15 (Domain Violation - Accuracy)\n",
    "\n",
    "handle_capitalization_format - DQI #15 (Domain Violation - Accuracy)\n",
    "\n",
    "handle_short_length_values - DQI #15 (Domain Violation - Accuracy)\n",
    "\n",
    "handle_invalid_months - DQI #15 (Domain Violation - Accuracy)\n",
    "\n",
    "handle_invalid_weekdays - DQI #15 (Domain Violation - Accuracy)\n",
    "\n",
    "handle_street_format - DQI #15 (Domain Violation - Accuracy)\n",
    "\n",
    "is_date_valid - Not directly associated with a DQI number\n",
    "\n",
    "handle_invalid_dates - DQI #15 (Domain Violation - Accuracy)\n",
    "\n",
    "is_structurally_valid_date - Not directly associated with a DQI number\n",
    "\n",
    "handle_invalid_datetimes - DQI #15 (Domain Violation - Accuracy)\n",
    "\n",
    "standardize_phone_number - Not directly associated with a DQI number\n",
    "\n",
    "handle_phone_number_format - DQI #15 (Domain Violation - Accuracy)\n",
    "\n",
    "handle_ip_format - DQI #15 (Domain Violation - Accuracy)\n",
    "\n",
    "handle_url_format - DQI #15 (Domain Violation - Accuracy)\n",
    "\n",
    "is_valid_email - Not directly associated with a DQI number\n",
    "\n",
    "handle_email_format - DQI #15 (Domain Violation - Accuracy)\n",
    "\n",
    "handle_binary_values - DQI #15 (Domain Violation - Accuracy)\n",
    "\n",
    "handle_non_numeric_values - DQI #17 (Wrong Data Type - Consistency)\n",
    "\n",
    "handle_non_alphanumeric_values - DQI #17 (Wrong Data Type - Consistency)\n",
    "\n",
    "handle_non_string_values - DQI #17 (Non-String Data Type - Consistency)\n",
    "\n",
    "handle_alphanumeric_consistency - DQI #17 (Wrong Data Type - Consistency)\n",
    "\n",
    "handle_uniqueness_violation - DQI #19 (Uniqueness Violation - Uniqueness)\n",
    "\n",
    "handle_special_characters - DQI #21 (Use of Special Characters - Consistency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "06 Data Quality Issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2004,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last run on: 2025-04-04 19:33:46\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "from datetime import datetime, time  \n",
    "\n",
    "min_valid_year = 1800\n",
    "max_valid_year = 2100\n",
    "\n",
    "class DataQualityIssues:\n",
    "\n",
    "    # Single-character symbols considered as errors\n",
    "    error_symbols = set(string.punctuation)\n",
    "\n",
    "    @staticmethod\n",
    "    def categorize_value(val):\n",
    "        if pd.isna(val):\n",
    "            return 'NULL'\n",
    "        \n",
    "        val_str = str(val).lower().strip()\n",
    "        \n",
    "        if val_str == '':\n",
    "            return 'Empty'\n",
    "        elif val_str == 'null':\n",
    "            return 'NULL'\n",
    "        elif val_str == 'n/a':\n",
    "            return 'N/A'\n",
    "        elif val_str == 'none':\n",
    "            return 'None'\n",
    "        elif val_str in ['\"\"', \"''\"]:\n",
    "            return 'Empty'\n",
    "        elif re.match(r'^\\s+$', val_str): \n",
    "            return 'Blank'\n",
    "        elif val_str in ['\" \"', \"' '\"]:\n",
    "            return 'Blank'\n",
    "        elif len(val_str) == 1 and val_str in DataQualityIssues.error_symbols:\n",
    "            return val_str\n",
    "        else:\n",
    "            return 'Valid'\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def is_blank(x):\n",
    "        category = DataQualityIssues.categorize_value(x)\n",
    "        return category != 'Valid'\n",
    "    \n",
    "    @staticmethod\n",
    "    def represent_value(val):\n",
    "        category = DataQualityIssues.categorize_value(val)\n",
    "        if category == 'Valid':\n",
    "            return str(val)\n",
    "        else:\n",
    "            return category\n",
    "\n",
    "    @staticmethod\n",
    "    def handle_blank_empty_null_nan(df, column):\n",
    "        blank_indices = df[df[column].apply(DataQualityIssues.is_blank)].index\n",
    "        blank_values = [(idx, DataQualityIssues.represent_value(df.loc[idx, column])) for idx in blank_indices]\n",
    "\n",
    "        total_issues = len(blank_values)\n",
    "\n",
    "        if total_issues > 0:\n",
    "            null_values = [v for v in blank_values if v[1] == 'NULL']\n",
    "            non_null_values = [v for v in blank_values if v[1] != 'NULL']\n",
    "\n",
    "            message = f\"{total_issues} Blank/Empty/Null/Missing value(s) at index(es): \"\n",
    "            \n",
    "            if len(null_values) == total_issues:\n",
    "                message += \"All NULL values:\\n\"\n",
    "                if total_issues > 20:\n",
    "                    null_display = null_values[:10] + [('...', '...')] + null_values[-10:]\n",
    "                else:\n",
    "                    null_display = null_values\n",
    "                message += f\"{null_display}\"\n",
    "            else:\n",
    "                if null_values:\n",
    "                    if len(null_values) > 20:\n",
    "                        null_display = null_values[:10] + [('...', '...')] + null_values[-10:]\n",
    "                    else:\n",
    "                        null_display = null_values\n",
    "                    message += f\"\\n{len(null_values)} NULL values: {null_display}\"\n",
    "                \n",
    "                if non_null_values:\n",
    "                    if len(non_null_values) > 20:\n",
    "                        non_null_display = non_null_values[:10] + [('...', '...')] + non_null_values[-10:]\n",
    "                    else:\n",
    "                        non_null_display = non_null_values\n",
    "                    message += f\"\\n{len(non_null_values)} Non-NULL values: {non_null_display}\"\n",
    "\n",
    "            # Prepare data_issues for Summary Sheet\n",
    "            all_unique_values = set(v[1] for v in blank_values)\n",
    "            data_issues = ', '.join(f'\"{v}\"' for v in all_unique_values if v != '...')\n",
    "\n",
    "            return {\n",
    "                \"issue\": True,\n",
    "                \"error_message\": message,\n",
    "                \"dq_issue\": \"DQI #1 (Missing Data - Completeness)\",\n",
    "                \"blank_indices\": blank_indices,\n",
    "                \"null_values\": null_values,\n",
    "                \"non_null_values\": non_null_values,\n",
    "                \"data_issues\": data_issues\n",
    "            }\n",
    "        return {\"issue\": False, \"blank_indices\": []}\n",
    "       \n",
    "    @staticmethod\n",
    "    def handle_predefined_unacceptable_values(df, column):\n",
    "        predefined_unacceptable_values = ['N/A', 'Invalid', 'unknown','nonexistent']\n",
    "        unacceptable_indices_and_values = []\n",
    "\n",
    "        for idx, val in df[column].items():\n",
    "            val_str = str(val).strip()\n",
    "            if val_str in predefined_unacceptable_values:\n",
    "                unacceptable_indices_and_values.append((idx, val))\n",
    "\n",
    "        total_issues = len(unacceptable_indices_and_values)\n",
    "        if unacceptable_indices_and_values:\n",
    "            if total_issues > 20:\n",
    "                first_10 = unacceptable_indices_and_values[:10]\n",
    "                last_10 = unacceptable_indices_and_values[-10:]\n",
    "                display_list = first_10 + [('...', '...')] + last_10\n",
    "                message = f\"{total_issues} Unacceptable value(s) at index(es): {display_list} (displaying only the first and last 10 items)\"\n",
    "            else:\n",
    "                message = f\"{total_issues} Unacceptable value(s) at index(es): {unacceptable_indices_and_values}\"\n",
    "\n",
    "            unacceptable_indices = [idx for idx, _ in unacceptable_indices_and_values]\n",
    "            return {\n",
    "                \"issue\": True,\n",
    "                \"error_message\": message,\n",
    "                \"dq_issue\": \"DQI #4 (Ambiguous Data - Accuracy, Consistency)\",\n",
    "                \"unacceptable_indices\": unacceptable_indices\n",
    "            }\n",
    "        return {\"issue\": False, \"unacceptable_indices\": []}\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def handle_extraneous_data(df, column):\n",
    "        # Define the function to check for extraneous data\n",
    "        def has_extraneous_data(x):\n",
    "            return any(char.isdigit() or char in ['!', '?'] for char in str(x))\n",
    "\n",
    "        # Get indices and values for extraneous data\n",
    "        extraneous_data_indices = df[df[column].apply(has_extraneous_data)].index\n",
    "        extraneous_data_values = df.loc[extraneous_data_indices, column].tolist()\n",
    "        issue_data = list(zip(extraneous_data_indices, extraneous_data_values))\n",
    "        total_issues = len(issue_data)\n",
    "\n",
    "        if issue_data:\n",
    "            if total_issues > 20:\n",
    "                # Keep only the first 10 and last 10 items\n",
    "                first_10 = issue_data[:10]\n",
    "                last_10 = issue_data[-10:]\n",
    "                display_list = first_10 + [('...', '...')] + last_10\n",
    "                message = f\"{total_issues} Extraneous data value(s) at index(es): {display_list} (displaying only the first and last 10 items)\"\n",
    "            else:\n",
    "                message = f\"{total_issues} Extraneous data value(s) at index(es): {issue_data}\"\n",
    "\n",
    "            return {\n",
    "                \"issue\": True,\n",
    "                \"error_message\": message,\n",
    "                \"indices\": extraneous_data_indices,  # Include all indices directly\n",
    "                \"dq_issue\": \"DQI #5 (Extraneous Data - Consistency, Uniqueness)\"\n",
    "            }\n",
    "\n",
    "        return {\"issue\": False}\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def handle_street_extraneous_data(df, column):\n",
    "        \"\"\"\n",
    "        Check for extraneous data in street names, allowing common patterns including numbers,\n",
    "        hyphens, periods, slashes, and commas which are typical in street addresses, but flagging street names\n",
    "        composed solely of numbers as errors.\n",
    "        \"\"\"\n",
    "        def has_extraneous_street_data(x):\n",
    "            # Allow numbers, letters, spaces, hyphens, periods, slashes, and commas\n",
    "            allowed_chars = string.ascii_letters + string.digits + \"' -./,äæúéñßíÅ#\"\n",
    "            # Flag if the string is solely numeric\n",
    "            if x.isdigit():\n",
    "                return True\n",
    "            return any(char not in allowed_chars for char in x)\n",
    "\n",
    "        extraneous_data_indices = df[df[column].apply(lambda x: isinstance(x, str) and has_extraneous_street_data(x))].index\n",
    "        extraneous_data_values = df.loc[extraneous_data_indices, column].tolist()\n",
    "        issue_data = list(zip(extraneous_data_indices, extraneous_data_values))\n",
    "        total_issues = len(issue_data)\n",
    "\n",
    "        if issue_data:\n",
    "            if total_issues > 20:\n",
    "                # Keep only the first 10 and last 10 items\n",
    "                first_10 = issue_data[:10]\n",
    "                last_10 = issue_data[-10:]\n",
    "                display_list = first_10 + [('...', '...')] + last_10\n",
    "                message = f\"{total_issues} Extraneous street data value(s) at index(es): {display_list} (displaying only the first and last 10 items)\"\n",
    "            else:\n",
    "                message = f\"{total_issues} Extraneous street data value(s) at index(es): {issue_data}\"\n",
    "\n",
    "            return {\n",
    "                \"issue\": True,\n",
    "                \"error_message\": message,\n",
    "                \"indices\": extraneous_data_indices,  # Include all indices directly\n",
    "                \"dq_issue\": \"DQI #5 (Extraneous Data - Consistency, Uniqueness)\"\n",
    "            }\n",
    "\n",
    "        return {\"issue\": False}\n",
    "\n",
    "    @staticmethod\n",
    "    def convert_to_strftime_format(deduced_format):\n",
    "        format_mappings = {\n",
    "            \"DDMMYYYY\": \"%d/%m/%Y\",\n",
    "            \"MMDDYYYY\": \"%m/%d/%Y\",\n",
    "            \"YYYYMMDD\": \"%Y/%m/%d\"\n",
    "        }\n",
    "        return format_mappings.get(deduced_format, \"%Y-%m-%d\")  # Default format\n",
    "\n",
    "        \n",
    "    @staticmethod\n",
    "    def standardize_date(date_str, deduced_format):\n",
    "        month_mapping = {\n",
    "            'January': '01', 'February': '02', 'March': '03', 'April': '04', 'May': '05', 'June': '06',\n",
    "            'July': '07', 'August': '08', 'September': '09', 'October': '10', 'November': '11', 'December': '12',\n",
    "            'Jan': '01', 'Feb': '02', 'Mar': '03', 'Apr': '04', 'May': '05', 'Jun': '06',\n",
    "            'Jul': '07', 'Aug': '08', 'Sep': '09', 'Oct': '10', 'Nov': '11', 'Dec': '12'\n",
    "        }\n",
    "        # Remove ordinal suffixes and commas\n",
    "        date_str = re.sub(r'(\\d+)(st|nd|rd|th)', r'\\1', date_str.replace(',', ''))\n",
    "\n",
    "        for month, num in month_mapping.items():\n",
    "            date_str = re.sub(r'\\b' + month + r'\\b', num, date_str, flags=re.IGNORECASE)\n",
    "\n",
    "        # Split the date string into components\n",
    "        date_parts = re.split(r'[-/. ]', date_str)\n",
    "\n",
    "        if len(date_parts) == 3:\n",
    "            if deduced_format == 'YYYYMMDD':\n",
    "                standardized_date = date_parts[0][:4] + date_parts[1].zfill(2) + date_parts[2].zfill(2)\n",
    "            elif deduced_format == 'DDMMYYYY':\n",
    "                standardized_date = date_parts[0].zfill(2) + date_parts[1].zfill(2) + date_parts[2][:4]\n",
    "            elif deduced_format == 'MMDDYYYY':\n",
    "                standardized_date = date_parts[0].zfill(2) + date_parts[1].zfill(2) + date_parts[2][:4]\n",
    "            else:\n",
    "                return None  # Format mismatch\n",
    "        else:\n",
    "            return None  # Format not recognized    \n",
    "        \n",
    "        return standardized_date\n",
    "\n",
    "    @staticmethod\n",
    "    def standardize_date_time(date_str, deduced_format):\n",
    "        \n",
    "       # Convert to string in case the input is not a string (e.g., float, int)\n",
    "        date_str = str(date_str)\n",
    "        \n",
    "        # Handle dates with ordinal suffixes and commas\n",
    "        date_str = re.sub(r'(\\d+)(st|nd|rd|th)', r'\\1', date_str.replace(',', ''))\n",
    "\n",
    "        parts = date_str.split(' ')\n",
    "        \n",
    "        if len(parts) > 1 and parts[-1] in [\"AM\", \"PM\"]:\n",
    "            time_part = ' '.join(parts[-2:])\n",
    "            date_part = ' '.join(parts[:-2])\n",
    "        elif len(parts) > 1 and ':' in parts[-1]:\n",
    "            time_part = parts[-1]\n",
    "            date_part = ' '.join(parts[:-1])\n",
    "        else:\n",
    "            date_part = date_str\n",
    "            time_part = '00:00:00'  # Default time for date-only entries\n",
    "\n",
    "        # Convert textual months to numbers\n",
    "        month_mapping = {\n",
    "            'January': '01', 'February': '02', 'March': '03', 'April': '04', 'May': '05', 'June': '06',\n",
    "            'July': '07', 'August': '08', 'September': '09', 'October': '10', 'November': '11', 'December': '12',\n",
    "            'Jan': '01', 'Feb': '02', 'Mar': '03', 'Apr': '04', 'May': '05', 'Jun': '06',\n",
    "            'Jul': '07', 'Aug': '08', 'Sep': '09', 'Oct': '10', 'Nov': '11', 'Dec': '12'\n",
    "        }\n",
    "\n",
    "        # Only apply month conversion if there's a textual month\n",
    "        needs_conversion = any(month in date_part for month in month_mapping)\n",
    "\n",
    "        if needs_conversion:\n",
    "            for month, num in month_mapping.items():\n",
    "                date_part = re.sub(r'\\b' + month + r'\\b', num, date_part, flags=re.IGNORECASE)\n",
    "\n",
    "        # Process the date part\n",
    "        date_parts = re.split(r'[-/. ]', date_part)\n",
    "\n",
    "        if len(date_parts) == 3:\n",
    "            if deduced_format == 'YYYYMMDD':\n",
    "                standardized_date = date_parts[0][:4] + date_parts[1].zfill(2) + date_parts[2].zfill(2)\n",
    "            elif deduced_format == 'DDMMYYYY':\n",
    "                standardized_date = date_parts[0].zfill(2) + date_parts[1].zfill(2) + date_parts[2][:4]\n",
    "            elif deduced_format == 'MMDDYYYY':\n",
    "                standardized_date = date_parts[0].zfill(2) + date_parts[1].zfill(2) + date_parts[2][:4]\n",
    "            else:\n",
    "                return None  # Format mismatch\n",
    "        else:\n",
    "            return None  # Format not recognized\n",
    "\n",
    "        return standardized_date + ' ' + time_part if time_part and standardized_date else None\n",
    "\n",
    "        \n",
    "    @staticmethod\n",
    "    def is_valid_time(time_str):\n",
    "        if isinstance(time_str, time):  \n",
    "            return True\n",
    "\n",
    "        if not time_str or pd.isna(time_str):\n",
    "            return False\n",
    "\n",
    "        time_str = str(time_str)\n",
    "\n",
    "        # Check for timedelta-like format\n",
    "        if re.match(r'\\d+ days \\d{2}:\\d{2}:\\d{2}', time_str):\n",
    "            return True\n",
    "\n",
    "        time_formats = ['%H:%M', '%I:%M %p', '%H:%M:%S', '%I:%M:%S %p']\n",
    "\n",
    "        for fmt in time_formats:\n",
    "            try:\n",
    "                datetime.strptime(time_str, fmt)\n",
    "                return True\n",
    "            except ValueError:\n",
    "                continue\n",
    "\n",
    "        return False\n",
    "\n",
    "    @staticmethod\n",
    "    def handle_outdated_temporal_data(df, column, min_year, max_year):\n",
    "        outdated_entries = []\n",
    "        # Assume a default format for date standardization\n",
    "        default_format = 'DDMMYYYY'  \n",
    "\n",
    "        for idx, val in df[column].items():\n",
    "            standardized_date = DataQualityIssues.standardize_date(str(val), default_format)\n",
    "            year = None\n",
    "\n",
    "            # Skip the processing if standardized_date is None\n",
    "            if standardized_date is None:\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                if len(standardized_date) == 8 and standardized_date.isdigit():\n",
    "                    # Check for YYYYMMDD format\n",
    "                    if int(standardized_date[4:6]) <= 12 and int(standardized_date[6:]) <= 31:\n",
    "                        year = int(standardized_date[:4])\n",
    "                        date_obj = datetime(year, int(standardized_date[4:6]), int(standardized_date[6:]))\n",
    "                    # Check for DDMMYYYY format\n",
    "                    elif int(standardized_date[:2]) <= 31 and int(standardized_date[2:4]) <= 12:\n",
    "                        year = int(standardized_date[4:])\n",
    "                        date_obj = datetime(year, int(standardized_date[2:4]), int(standardized_date[:2]))\n",
    "\n",
    "                # Check if the year is within the valid range\n",
    "                if year and not (min_year <= year <= max_year):\n",
    "                    # Append the date in a readable 'YYYY-MM-DD' format\n",
    "                    outdated_entries.append((idx, date_obj.strftime('%Y-%m-%d')))\n",
    "\n",
    "            except ValueError as e:\n",
    "                #print(f\"Error processing date at index {idx}: {e}\")\n",
    "                continue\n",
    "\n",
    "        total_issues = len(outdated_entries)\n",
    "        if outdated_entries:\n",
    "            if total_issues > 20:\n",
    "                # Keep only the first 10 and last 10 items\n",
    "                first_10 = outdated_entries[:10]\n",
    "                last_10 = outdated_entries[-10:]\n",
    "                display_list = first_10 + [('...', '...')] + last_10\n",
    "                message = f\"{total_issues} Date value(s) not in [{min_year}-{max_year}] period at index(es): {display_list} (displaying only the first and last 10 items)\"\n",
    "            else:\n",
    "                message = f\"{total_issues} Date value(s) not in [{min_year}-{max_year}] period at index(es): {outdated_entries}\"\n",
    "\n",
    "            return {\n",
    "                \"issue\": True,\n",
    "                \"error_message\": message,\n",
    "                \"dq_issue\": \"DQI #6 (Outdated Temporal Data - Timeliness)\",\n",
    "                \"outdated_entries\": outdated_entries\n",
    "            }\n",
    "\n",
    "        return {\"issue\": False, \"outdated_entries\": []}\n",
    "    \n",
    "    @staticmethod\n",
    "    def handle_outdated_temporal_data_datetime(df, column, min_year, max_year):\n",
    "        outdated_entries = []\n",
    "        # Assume a default format for date standardization\n",
    "        default_format = 'DDMMYYYY'  \n",
    "\n",
    "        for idx, val in df[column].items():\n",
    "            # Split the datetime into date and time parts using standardize_date_time\n",
    "            standardized_datetime = DataQualityIssues.standardize_date_time(str(val), default_format)\n",
    "            if standardized_datetime is None:\n",
    "                continue\n",
    "\n",
    "            parts = standardized_datetime.split(' ')\n",
    "            date_part = parts[0]  # The date part is always the first part\n",
    "\n",
    "            year = None\n",
    "\n",
    "            # Extract the year from the standardized date\n",
    "            if len(date_part) == 8 and date_part.isdigit():\n",
    "                # Check for YYYYMMDD format\n",
    "                if int(date_part[4:6]) <= 12 and int(date_part[6:]) <= 31:\n",
    "                    year = int(date_part[:4])\n",
    "                # Check for DDMMYYYY format\n",
    "                elif int(date_part[:2]) <= 31 and int(date_part[2:4]) <= 12:\n",
    "                    year = int(date_part[4:])\n",
    "\n",
    "            # Check if the year is within the valid range\n",
    "            if year and not (min_year <= year <= max_year):\n",
    "                outdated_entries.append((idx, val))\n",
    "\n",
    "        total_issues = len(outdated_entries)\n",
    "        if outdated_entries:\n",
    "            if total_issues > 20:\n",
    "                # Keep only the first 10 and last 10 items\n",
    "                first_10 = outdated_entries[:10]\n",
    "                last_10 = outdated_entries[-10:]\n",
    "                display_list = first_10 + [('...', '...')] + last_10\n",
    "                message = f\"{total_issues} Datetime value(s) not in [{min_year}-{max_year}] period at index(es): {display_list} (displaying only the first and last 10 items)\"\n",
    "            else:\n",
    "                message = f\"{total_issues} Datetime value(s) not in [{min_year}-{max_year}] period at index(es): {outdated_entries}\"\n",
    "\n",
    "            return {\n",
    "                \"issue\": True,\n",
    "                \"error_message\": message,\n",
    "                \"dq_issue\": \"DQI #6 (Outdated Temporal Data - Timeliness)\"\n",
    "            }\n",
    "\n",
    "        return {\"issue\": False}\n",
    "    \n",
    "    @staticmethod\n",
    "    def handle_duplicates(df, column):\n",
    "        duplicate_values = df[column].duplicated(keep=False)  # Mark all duplicates\n",
    "        issue_data = df[duplicate_values][column].reset_index().values.tolist()\n",
    "        total_issues = len(issue_data)\n",
    "\n",
    "        if issue_data:\n",
    "            if total_issues > 20:\n",
    "                first_10 = issue_data[:10]\n",
    "                last_10 = issue_data[-10:]\n",
    "                display_list = first_10 + [('...', '...')] + last_10\n",
    "                message = f\"{total_issues} Duplicate value(s) at index(es): {display_list} (displaying only the first and last 10 items)\"\n",
    "            else:\n",
    "                message = f\"{total_issues} Duplicate value(s) at index(es): {issue_data}\"\n",
    "\n",
    "            return {\n",
    "                \"issue\": True,\n",
    "                \"error_message\": message,\n",
    "                \"dq_issue\": \"DQI #9 (Duplicates - Uniqueness)\"\n",
    "            }\n",
    "        return {\"issue\": False}\n",
    "    \n",
    "    @staticmethod\n",
    "    def handle_excessive_distinct_values(df, column, threshold=100):\n",
    "        unique_values_count = df[column].nunique(dropna=False)\n",
    "\n",
    "        if unique_values_count > threshold:\n",
    "            sample_values = df[column].dropna().unique()[:10]  # Sample of up to 10 unique values\n",
    "            return {\n",
    "                \"issue\": True,\n",
    "                \"error_message\": f\"Data seems not categorical or has too many categories (> {threshold}).\",\n",
    "                \"dq_issue\": \"DQI #10 (Structural Conflicts - Consistency, Uniqueness)\"\n",
    "            }\n",
    "        return {\"issue\": False}\n",
    "\n",
    "                \n",
    "    @staticmethod\n",
    "    def is_date_valid(date_str, fmt):\n",
    "        try:\n",
    "            # Parse the date string based on the provided format\n",
    "            if fmt == 'YYYYMMDD':\n",
    "                year, month, day = int(date_str[:4]), int(date_str[4:6]), int(date_str[6:8])\n",
    "            elif fmt == 'DDMMYYYY':\n",
    "                day, month, year = int(date_str[:2]), int(date_str[2:4]), int(date_str[4:8])\n",
    "            elif fmt == 'MMDDYYYY':\n",
    "                month, day, year = int(date_str[:2]), int(date_str[2:4]), int(date_str[4:8])\n",
    "            else:\n",
    "                return False\n",
    "\n",
    "            # Construct datetime object to validate the date\n",
    "            date(year, month, day)\n",
    "\n",
    "            return min_valid_year <= year <= max_valid_year\n",
    "        except ValueError as e:\n",
    "            # print(f\"Failed date parsing for {date_str} in format {fmt}: {e}\")\n",
    "            # print(f\"Date parsing resulted in an error: {e}\")\n",
    "            return False\n",
    "\n",
    "    @staticmethod\n",
    "    def is_valid_in_any_format(date_str):\n",
    "        formats = ['%Y%m%d', '%d%m%Y', '%m%d%Y']  # Add other potential formats\n",
    "        for fmt in formats:\n",
    "            try:\n",
    "                datetime.strptime(date_str, fmt)\n",
    "                return True\n",
    "            except ValueError:\n",
    "                continue\n",
    "        return False\n",
    "        \n",
    "    @staticmethod\n",
    "    def handle_invalid_dates(df, column):\n",
    "        invalid_entries = []\n",
    "        # Assume a default format for date standardization\n",
    "        default_format = 'DDMMYYYY'  \n",
    "\n",
    "        for idx, val in df[column].items():\n",
    "            if isinstance(val, (date, datetime)):\n",
    "                continue  \n",
    "\n",
    "            standardized_date = DataQualityIssues.standardize_date(str(val), default_format)\n",
    "            #print(f\"Index: {idx}, Original Value: {val}, Standardized Date: {standardized_date}\")\n",
    "           \n",
    "            # Skip the validation if standardized_date is None\n",
    "            if standardized_date is None:\n",
    "                continue\n",
    "            \n",
    "            # Check if date is structurally valid in any format\n",
    "            if not (DataQualityIssues.is_date_valid(standardized_date, 'DDMMYYYY') or\n",
    "                    DataQualityIssues.is_date_valid(standardized_date, 'MMDDYYYY') or\n",
    "                    DataQualityIssues.is_date_valid(standardized_date, 'YYYYMMDD')):\n",
    "                invalid_entries.append((idx, val))\n",
    "        \n",
    "        total_issues = len(invalid_entries)\n",
    "        if invalid_entries:\n",
    "            if total_issues > 20:\n",
    "                # Keep only the first 10 and last 10 items\n",
    "                first_10 = invalid_entries[:10]\n",
    "                last_10 = invalid_entries[-10:]\n",
    "                display_list = first_10 + [('...', '...')] + last_10\n",
    "                # Convert the list to a string while keeping the square brackets\n",
    "                issue_data = str(display_list)\n",
    "                message = f\"{total_issues} Invalid date value(s) at index(es): {issue_data} (displaying only the first and last 10 items)\"\n",
    "            else:\n",
    "                # Convert the list to a string while keeping the square brackets\n",
    "                issue_data = str(invalid_entries)\n",
    "                message = f\"{total_issues} Invalid date value(s) at index(es): {issue_data}\"\n",
    "            \n",
    "            return {\n",
    "                \"issue\": True,\n",
    "                \"error_message\": message,\n",
    "                \"dq_issue\":  \"DQI #13 (Temporal mismatch - Accuracy, Timeliness)\"\n",
    "            }\n",
    "        \n",
    "        return {\"issue\": False}\n",
    "\n",
    "    @staticmethod\n",
    "    def is_structurally_valid_date(date_str, expected_format):\n",
    "        # Check the date in any format for structural validity\n",
    "        return DataQualityIssues.is_date_valid(date_str, expected_format)\n",
    "\n",
    "    @staticmethod\n",
    "    def handle_invalid_datetimes(df, column, deduced_format):\n",
    "        #print(f\"Handling invalid datetimes. Deduced format: {deduced_format}\")\n",
    "        invalid_datetime_errors = []\n",
    "\n",
    "        for idx, val in df[column].items():\n",
    "            #print(f\"Processing index {idx}, value: {val}\")\n",
    "            standardized_datetime = DataQualityIssues.standardize_date_time(val, deduced_format)\n",
    "\n",
    "            if not standardized_datetime:\n",
    "                #print(f\"Standardization failed for: {val}\")\n",
    "                invalid_datetime_errors.append((idx, val))\n",
    "                continue\n",
    "\n",
    "            parts = standardized_datetime.split(' ')\n",
    "            date_str = parts[0]\n",
    "            time_str = parts[1] if len(parts) > 1 else '00:00:00'\n",
    "\n",
    "            #print(f\"Standardized datetime: {standardized_datetime}, Date: {date_str}, Time: {time_str}\")\n",
    "\n",
    "            if not DataQualityIssues.is_valid_time(time_str):\n",
    "                #print(f\"Invalid time for: {val}\")\n",
    "                invalid_datetime_errors.append((idx, val))\n",
    "            elif not DataQualityIssues.is_structurally_valid_date(date_str, deduced_format):\n",
    "                #print(f\"Date structure invalid for: {val}\")\n",
    "                invalid_datetime_errors.append((idx, val))\n",
    "\n",
    "        total_issues = len(invalid_datetime_errors)\n",
    "        if invalid_datetime_errors:\n",
    "            #print(f\"Total invalid datetime values: {total_issues}\")\n",
    "            if total_issues > 20:\n",
    "                # Keep only the first 10 and last 10 items\n",
    "                first_10 = invalid_datetime_errors[:10]\n",
    "                last_10 = invalid_datetime_errors[-10:]\n",
    "                display_list = first_10 + [('...', '...')] + last_10\n",
    "                issue_data = str(display_list)\n",
    "                message = f\"{total_issues} Invalid datetime value(s) at index(es): {issue_data} (displaying only the first and last 10 items)\"\n",
    "            else:\n",
    "                issue_data = str(invalid_datetime_errors)\n",
    "                message = f\"{total_issues} Invalid datetime value(s) at index(es): {issue_data}\"\n",
    "\n",
    "            return {\n",
    "                \"issue\": True,\n",
    "                \"error_message\": message,\n",
    "                \"dq_issue\":  \"DQI #13 (Temporal mismatch - Accuracy, Timeliness)\"\n",
    "            }\n",
    "\n",
    "        #print(\"No invalid datetimes found.\")\n",
    "        return {\"issue\": False}\n",
    "\n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def handle_invalid_times(df, column):\n",
    "        invalid_time_errors = []\n",
    "        for idx, time_str in df[column].items():\n",
    "            if not DataQualityIssues.is_valid_time(time_str):\n",
    "                invalid_time_errors.append((idx, time_str))\n",
    "\n",
    "        total_issues = len(invalid_time_errors)\n",
    "        if invalid_time_errors:\n",
    "            if total_issues > 20:\n",
    "                # Keep only the first 10 and last 10 items\n",
    "                first_10 = invalid_time_errors[:10]\n",
    "                last_10 = invalid_time_errors[-10:]\n",
    "                display_list = first_10 + [('...', '...')] + last_10\n",
    "                # Convert the list to a string while keeping the square brackets\n",
    "                issue_data = str(display_list)\n",
    "                message = f\"{total_issues} Invalid time value(s) at index(es): {issue_data} (displaying only the first and last 10 items)\"\n",
    "            else:\n",
    "                # Convert the list to a string while keeping the square brackets\n",
    "                issue_data = str(invalid_time_errors)\n",
    "                message = f\"{total_issues} Invalid time value(s) at index(es): {issue_data}\"\n",
    "\n",
    "            return {\n",
    "                \"issue\": True,\n",
    "                \"error_message\": message,\n",
    "                \"dq_issue\": \"DQI #13 (Temporal mismatch - Accuracy, Timeliness)\"\n",
    "            }\n",
    "\n",
    "        return {\"issue\": False}\n",
    "\n",
    "    @staticmethod\n",
    "    def deduce_regional_format(date_samples):\n",
    "        month_mapping = {\n",
    "            'January': '01', 'February': '02', 'March': '03', 'April': '04', 'May': '05', 'June': '06',\n",
    "            'July': '07', 'August': '08', 'September': '09', 'October': '10', 'November': '11', 'December': '12',\n",
    "            'Jan': '01', 'Feb': '02', 'Mar': '03', 'Apr': '04', 'May': '05', 'Jun': '06',\n",
    "            'Jul': '07', 'Aug': '08', 'Sep': '09', 'Oct': '10', 'Nov': '11', 'Dec': '12'\n",
    "        }\n",
    "        standardized_dates = []\n",
    "        format_counts = {'DDMMYYYY': 0, 'MMDDYYYY': 0, 'YYYYMMDD': 0}\n",
    "\n",
    "        for date in date_samples:\n",
    "            # Convert textual months to numbers\n",
    "            for month, num in month_mapping.items():\n",
    "                date = re.sub(r'\\b' + month + r'\\b', num, date, flags=re.IGNORECASE)\n",
    "\n",
    "            # Handle continuous string dates (without separators)\n",
    "            if len(date) == 8 and date.isdigit():\n",
    "                # Check if the first four characters represent a plausible year\n",
    "                if 1800 <= int(date[:4]) <= 2100:\n",
    "                    format_counts['YYYYMMDD'] += 1\n",
    "                # Check if the first two characters represent a plausible day and next two a month\n",
    "                elif 1 <= int(date[:2]) <= 31 and 1 <= int(date[2:4]) <= 12:\n",
    "                    format_counts['DDMMYYYY'] += 1\n",
    "                # Check if the first two characters represent a plausible month and next two a day\n",
    "                elif 1 <= int(date[:2]) <= 12 and 1 <= int(date[2:4]) <= 31:\n",
    "                    format_counts['MMDDYYYY'] += 1\n",
    "            else:\n",
    "                date_parts = re.split(r'[-/. ]', date)\n",
    "                if len(date_parts) == 3:\n",
    "                    # Check if the year is the first or last component\n",
    "                    if len(date_parts[0]) == 4:\n",
    "                        format_counts['YYYYMMDD'] += 1\n",
    "                    elif len(date_parts[2]) == 4:\n",
    "                        format_counts['DDMMYYYY'] += 1\n",
    "                    else:\n",
    "                        # Default to MMDDYYYY if year is not first or last\n",
    "                        format_counts['MMDDYYYY'] += 1\n",
    "\n",
    "                    standardized_date = '-'.join(date_parts)  # Rejoin the date parts for standardized dates\n",
    "                    standardized_dates.append(standardized_date)\n",
    "\n",
    "        preferred_format = max(format_counts, key=format_counts.get)\n",
    "        print()\n",
    "        print(f\"Deduced format: {preferred_format}\")  # Diagnostic print\n",
    "        print(f\"Processed date sample: {date}, detected format counts: {format_counts}\")\n",
    "        print()\n",
    "\n",
    "        return preferred_format, standardized_dates\n",
    "    \n",
    "    @staticmethod\n",
    "    def handle_dates_format(df, column, expected_format):\n",
    "        format_errors = []\n",
    "        for idx, val in df[column].items():\n",
    "            if isinstance(val, (date, datetime)):\n",
    "                # If it's already a date or datetime object, it's considered valid\n",
    "                continue\n",
    "\n",
    "            standardized_date = DataQualityIssues.standardize_date(str(val), expected_format)\n",
    "            #print(f\"Index: {idx}, Original Value: {val}, Standardized Date: {standardized_date}\")\n",
    "\n",
    "            # Check if standardized_date is None\n",
    "            if standardized_date is None:\n",
    "                # If the date can't be standardized, skip it (it will be caught in other checks)\n",
    "                continue\n",
    "            \n",
    "            # Check if the date is valid according to the expected format\n",
    "            if not DataQualityIssues.is_date_valid(standardized_date, expected_format):\n",
    "                format_errors.append((idx, val))\n",
    "        total_issues = len(format_errors)\n",
    "\n",
    "        if format_errors:\n",
    "            if total_issues > 20:\n",
    "                # Keep only the first 10 and last 10 items\n",
    "                first_10 = format_errors[:10]\n",
    "                last_10 = format_errors[-10:]\n",
    "                display_list = first_10 + [('...', '...')] + last_10\n",
    "                message = f\"{total_issues} Date value(s) without format '{expected_format}' in [{min_valid_year}-{max_valid_year}] period at index(es): {display_list} (displaying only the first and last 10 items)\"\n",
    "            else:\n",
    "                message = f\"{total_issues} Date value(s) without format '{expected_format}' in [{min_valid_year}-{max_valid_year}] period at index(es): {format_errors}\"\n",
    "           \n",
    "            return{\n",
    "                \"issue\": True,\n",
    "                \"error_message\": message,\n",
    "                \"dq_issue\": \"DQI #14 (Different units/representations - Consistency)\"\n",
    "            }\n",
    "        return {\"issue\": False}\n",
    "\n",
    "    @staticmethod\n",
    "    def handle_datetimes_format(df, column, expected_format):\n",
    "        format_errors = []\n",
    " \n",
    "        for idx, val in df[column].items():\n",
    "            standardized_datetime = DataQualityIssues.standardize_date_time(str(val), expected_format)\n",
    "\n",
    "            if not standardized_datetime:\n",
    "                print(f\"Index {idx}: Standardization failed.\")\n",
    "                continue\n",
    "\n",
    "            date_str = standardized_datetime.split(' ')[0]\n",
    "            \n",
    "             # Check if the date is valid according to the expected format\n",
    "            if DataQualityIssues.is_valid_in_any_format(date_str) and not DataQualityIssues.is_structurally_valid_date(date_str, expected_format):\n",
    "                format_errors.append((idx, val))\n",
    "       \n",
    "        total_issues = len(format_errors)\n",
    "        if format_errors:\n",
    "            if total_issues > 20:\n",
    "                # Keep only the first 10 and last 10 items\n",
    "                first_10 = format_errors[:10]\n",
    "                last_10 = format_errors[-10:]\n",
    "                display_list = first_10 + [('...', '...')] + last_10\n",
    "                message = f\"{total_issues} Datetime value(s) without format '{expected_format}' in [{min_valid_year}-{max_valid_year}] period at index(es): {display_list} (displaying only the first and last 10 items)\"\n",
    "            else:\n",
    "                message = f\"{total_issues} Datetime value(s) without format '{expected_format}' in [{min_valid_year}-{max_valid_year}] period at index(es): {format_errors}\"\n",
    "           \n",
    "            return{\n",
    "                \"issue\": True,\n",
    "                \"error_message\": message,\n",
    "                \"dq_issue\": \"DQI #14 (Different units/representations - Consistency)\"\n",
    "            }\n",
    "\n",
    "        return {\"issue\": False}\n",
    "\n",
    "    @staticmethod\n",
    "    def handle_negative_values(df, column):\n",
    "        # Function to check if a value is negative\n",
    "        def is_negative(x):\n",
    "            try:\n",
    "                num_val = float(x)\n",
    "                return num_val < 0  # Check if the value is negative\n",
    "            except (ValueError, TypeError):\n",
    "                return False  # Non-numeric values are not considered here\n",
    "\n",
    "        # Get indices and values for negative values\n",
    "        negative_indices = df[df[column].apply(is_negative)].index\n",
    "        negative_values = df.loc[negative_indices, column].tolist()\n",
    "        issue_data = list(zip(negative_indices, negative_values))\n",
    "        total_issues = len(issue_data)\n",
    "\n",
    "        if issue_data:\n",
    "            if total_issues > 20:\n",
    "                # Keep only the first 10 and last 10 items\n",
    "                first_10 = issue_data[:10]\n",
    "                last_10 = issue_data[-10:]\n",
    "                display_list = first_10 + [('...', '...')] + last_10\n",
    "                message = f\"{total_issues} Negative value(s) at index(es): {display_list} (displaying only the first and last 10 items)\"\n",
    "            else:\n",
    "                message = f\"{total_issues} Negative value(s) at index(es): {issue_data}\"\n",
    "\n",
    "            return {\n",
    "                \"issue\": True,\n",
    "                \"error_message\": message,\n",
    "                \"dq_issue\": \"DQI #15 (Domain Violation - Accuracy)\"\n",
    "            }\n",
    "\n",
    "        return {\"issue\": False}\n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def handle_values_outside_range(df, column, min_value, max_value):\n",
    "        # Function to check if a value is within the specified range\n",
    "        def is_outside_range(x):\n",
    "            try:\n",
    "                num_val = float(x)\n",
    "                return num_val < min_value or num_val > max_value\n",
    "            except (ValueError, TypeError):\n",
    "                return False  # Non-numeric values are not considered here\n",
    "\n",
    "        # Get indices and values for values outside the specified range\n",
    "        outside_range_indices = df[df[column].apply(is_outside_range)].index\n",
    "        outside_range_values = df.loc[outside_range_indices, column].tolist()\n",
    "        issue_data = list(zip(outside_range_indices, outside_range_values))\n",
    "        total_issues = len(issue_data)\n",
    "\n",
    "        if issue_data:\n",
    "            if total_issues > 20:\n",
    "                # Keep only the first 10 and last 10 items\n",
    "                first_10 = issue_data[:10]\n",
    "                last_10 = issue_data[-10:]\n",
    "                display_list = first_10 + [('...', '...')] + last_10\n",
    "                message = f\"{total_issues} Value(s) outside range [{min_value}, {max_value}] at index(es): {display_list} (displaying only the first and last 10 items)\"\n",
    "            else:\n",
    "                message = f\"{total_issues} Value(s) outside range [{min_value}, {max_value}] at index(es): {issue_data}\"\n",
    "\n",
    "            return {\n",
    "                \"issue\": True,\n",
    "                \"error_message\": message,\n",
    "                \"dq_issue\": \"DQI #15 (Domain Violation - Accuracy)\"\n",
    "            }\n",
    "\n",
    "        return {\"issue\": False}\n",
    "       \n",
    "           \n",
    "    @staticmethod\n",
    "    def handle_floating_point_values(df, column):\n",
    "        \"\"\"\n",
    "        Flags floating-point numbers in a column.\n",
    "\n",
    "        Parameters:\n",
    "        - df (pd.DataFrame): The pandas DataFrame to analyze.\n",
    "        - column (str): The name of the column to check.\n",
    "\n",
    "        Returns:\n",
    "        - dict: A dictionary indicating if there are floating-point values, with their indices and values, and the associated DQI.\n",
    "        \"\"\"\n",
    "        floating_point_indices = []\n",
    "        for idx, value in df[column].items():\n",
    "            try:\n",
    "                # If it's a floating point and not an integer\n",
    "                if float(value) and not float(value).is_integer():\n",
    "                    floating_point_indices.append((idx, value))\n",
    "            except ValueError:\n",
    "                # Ignore non-numeric values, handle them in other checks\n",
    "                continue\n",
    "\n",
    "        total_issues = len(floating_point_indices)\n",
    "        if floating_point_indices:\n",
    "            if total_issues > 20:\n",
    "                # Keep only the first 10 and last 10 items\n",
    "                first_10 = floating_point_indices[:10]\n",
    "                last_10 = floating_point_indices[-10:]\n",
    "                display_list = first_10 + [('...', '...')] + last_10\n",
    "                message = f\"{total_issues} Floating-point number(s) at index(es): {display_list} (displaying only the first and last 10 items)\"\n",
    "            else:\n",
    "                message = f\"{total_issues} Floating-point number(s) at index(es): {floating_point_indices}\"\n",
    "\n",
    "            return {\n",
    "                \"issue\": True,\n",
    "                \"error_message\": message,\n",
    "                \"dq_issue\": \"DQI #15 (Domain Violation - Accuracy)\"\n",
    "            }\n",
    "\n",
    "        return {\"issue\": False}\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def handle_capitalization_format(df, column, linking_words):\n",
    "        \"\"\"\n",
    "        Check if the values in the column adhere to capitalization and format standards.\n",
    "        \"\"\"\n",
    "        def is_capitalization_issue(word):\n",
    "            # Allow all-uppercase words, ordinal numbers, linking words, and suffixes\n",
    "            if (word.isupper() or \n",
    "                re.match(r\"^\\d+(st|nd|rd|th)$\", word.lower()) or \n",
    "                word.lower() in linking_words or\n",
    "                word.lower() in ['ii', 'iii', 'iv', 'v', 'jr', 'sr']):\n",
    "                return False\n",
    "            \n",
    "            # Special handling for 'PhD' and 'MD'\n",
    "            if word.lower() in ['phd', 'md', 'c/']:\n",
    "                return word != word.upper()\n",
    "            \n",
    "            # Handle special cases like 'McDonald', 'O'Riley', etc.\n",
    "            if re.match(r\"^(Mc|Mac|O'|D')[A-Z]\", word):\n",
    "                return False\n",
    "\n",
    "            # Handle possessive forms\n",
    "            if word.endswith(\"'s\"):\n",
    "                return is_capitalization_issue(word[:-2])\n",
    "        \n",
    "            # For all other words, check if it's title case\n",
    "            return word != word.title()\n",
    "\n",
    "        format_issues = []\n",
    "        for idx, val in df[column].items():\n",
    "            # Split the string into words, preserving important punctuation\n",
    "            words = re.findall(r\"\\b[\\w'.,-]+\\b|#\\S+\", str(val))\n",
    "            if any(is_capitalization_issue(word) for word in words):\n",
    "                format_issues.append((idx, val))\n",
    "\n",
    "        total_issues = len(format_issues)\n",
    "        if format_issues:\n",
    "            if total_issues > 20:\n",
    "                first_10 = format_issues[:10]\n",
    "                last_10 = format_issues[-10:]\n",
    "                display_list = first_10 + [('...', '...')] + last_10\n",
    "                issue_data = str(display_list)\n",
    "                message = f\"{total_issues} Capitalization/Format issue(s) at index(es): {issue_data} (displaying only the first and last 10 items)\"\n",
    "            else:\n",
    "                issue_data = str(format_issues)\n",
    "                message = f\"{total_issues} Capitalization/Format issue(s) at index(es): {issue_data}\"\n",
    "\n",
    "            return {\n",
    "                \"issue\": True,\n",
    "                \"error_message\": message,\n",
    "                \"dq_issue\": \"DQI #15 (Domain Violation - Accuracy)\",\n",
    "                \"issue_indices\": [idx for idx, _ in format_issues]\n",
    "            }\n",
    "        return {\"issue\": False, \"issue_indices\": []}\n",
    "\n",
    "    @staticmethod\n",
    "    def handle_capitalization_format_country(df, column, linking_words):\n",
    "        \"\"\"\n",
    "        Check if the values in the column adhere to capitalization and format standards,\n",
    "        with exceptions for certain patterns typical in country names.\n",
    "        \"\"\"\n",
    "        def is_exceptional_case(word, prev_word=None):\n",
    "            # Directly return False (no issue) for all-uppercase words or ordinal numbers\n",
    "            if word.isupper() or re.match(r\"^\\d+(st|nd|rd|th)$\", word.lower()):\n",
    "                return False\n",
    "            # Check if the word is a linking word or part of an exception pattern (e.g., within parentheses or after a hyphen)\n",
    "            if word.lower() in linking_words or (prev_word and prev_word.endswith('-')):\n",
    "                return True\n",
    "            return False\n",
    "        \n",
    "        def is_capitalization_issue(words):\n",
    "            # Check for capitalization issue while considering exceptions\n",
    "            for i, word in enumerate(words):\n",
    "                prev_word = words[i-1] if i > 0 else None\n",
    "                # Skip words that are part of an exceptional case\n",
    "                if is_exceptional_case(word, prev_word):\n",
    "                    continue\n",
    "                # Identify words that fail the capitalization check (excluding exceptions)\n",
    "                if not word.istitle() and not word.isupper():\n",
    "                    return True\n",
    "            return False\n",
    "\n",
    "        format_issues = []\n",
    "        for idx, val in df[column].items():\n",
    "            original_val = str(val)\n",
    "            # Split the value considering spaces and treating content in parentheses as a single block\n",
    "            words = re.findall(r'\\b\\w+\\b', original_val)\n",
    "            if is_capitalization_issue(words):\n",
    "                format_issues.append((idx, val))\n",
    "\n",
    "        total_issues = len(format_issues)\n",
    "        if format_issues:\n",
    "            # Prepare the message with a simplified display if there are many issues\n",
    "            display_list = format_issues[:10] + [('...', '...')] if total_issues > 20 else format_issues\n",
    "            issue_data = str(display_list)\n",
    "            message = f\"{total_issues} Capitalization/Format issue(s) at index(es): \" + issue_data\n",
    "            return {\n",
    "                \"issue\": True,\n",
    "                \"error_message\": message,\n",
    "                \"dq_issue\": \"DQI #15 (Domain Violation - Accuracy)\"\n",
    "            }\n",
    "\n",
    "        return {\"issue\": False}\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def handle_short_length_values(df, column, min_length):\n",
    "        \"\"\"\n",
    "        Flags values in a column that are shorter than the specified minimum length, excluding non-alphanumeric characters.\n",
    "\n",
    "        Parameters:\n",
    "        - df (pd.DataFrame): The pandas DataFrame to analyze.\n",
    "        - column (str): The name of the column to check.\n",
    "        - min_length (int): The minimum acceptable length for values.\n",
    "\n",
    "        Returns:\n",
    "        - dict: A dictionary indicating if there are short values, with their indices and values, and the associated DQI.\n",
    "        \"\"\"\n",
    "        def is_short_and_alphanumeric(x):\n",
    "            return isinstance(x, str) and len(x) < min_length and x.isalnum()\n",
    "\n",
    "        short_values = df[df[column].apply(is_short_and_alphanumeric)]\n",
    "        issue_data = list(zip(short_values.index, short_values[column]))\n",
    "        total_issues = len(issue_data)\n",
    "\n",
    "        if issue_data:\n",
    "            if total_issues > 20:\n",
    "                # Keep only the first 10 and last 10 items\n",
    "                first_10 = issue_data[:10]\n",
    "                last_10 = issue_data[-10:]\n",
    "                display_list = first_10 + [('...', '...')] + last_10\n",
    "                message = f\"{total_issues} Short length alphanumeric value(s) at index(es): {display_list} (displaying only the first and last 10 items)\"\n",
    "            else:\n",
    "                message = f\"{total_issues} Short length alphanumeric value(s) at index(es): {issue_data}\"\n",
    "\n",
    "            return {\n",
    "                \"issue\": True,\n",
    "                \"error_message\": message,\n",
    "                \"dq_issue\": \"DQI #15 (Domain Violation - Accuracy)\"\n",
    "            }\n",
    "\n",
    "        return {\"issue\": False}\n",
    "    \n",
    "    @staticmethod\n",
    "    def normalize_month(month_str):\n",
    "        \"\"\"\n",
    "        Normalize the given month string to its full month name if valid.\n",
    "\n",
    "        Parameters:\n",
    "        - month_str (str or float): The month string or number to normalize.\n",
    "\n",
    "        Returns:\n",
    "        - str/None: Normalized month name if valid, None otherwise.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Convert to string if it's a float\n",
    "            if isinstance(month_str, float):\n",
    "                month_str = str(int(month_str))\n",
    "            else:\n",
    "                month_str = str(month_str).strip()\n",
    "\n",
    "            if month_str.replace('.', '').isdigit():\n",
    "                month_val = int(float(month_str))\n",
    "                if 1 <= month_val <= 12:\n",
    "                    return datetime(2000, month_val, 1).strftime('%B')\n",
    "            elif len(month_str) == 3:\n",
    "                return datetime.strptime(month_str.title(), '%b').strftime('%B')\n",
    "            else:\n",
    "                datetime.strptime(month_str.title(), '%B')\n",
    "                return month_str.title()\n",
    "        except ValueError:\n",
    "            return None\n",
    "    \n",
    "    @staticmethod\n",
    "    def handle_invalid_months(df, column):\n",
    "        \"\"\"\n",
    "        Check if all values in the specified column are valid representations of months.\n",
    "\n",
    "        Parameters:\n",
    "        - df (pd.DataFrame): The pandas DataFrame to analyze.\n",
    "        - column (str): The name of the column to analyze.\n",
    "\n",
    "        Returns:\n",
    "        - dict: A dictionary indicating if there are invalid month values, with their indices and values.\n",
    "        \"\"\"\n",
    "        invalid_entries = []\n",
    "        for idx, val in df[column].items():\n",
    "            if not DataQualityIssues.normalize_month(str(val)):\n",
    "                invalid_entries.append((idx, val))\n",
    "\n",
    "        total_issues = len(invalid_entries)\n",
    "        if invalid_entries:\n",
    "            if total_issues > 20:\n",
    "                # Keep only the first 10 and last 10 items\n",
    "                first_10 = invalid_entries[:10]\n",
    "                last_10 = invalid_entries[-10:]\n",
    "                display_list = first_10 + [('...', '...')] + last_10\n",
    "                message = f\"{total_issues} Invalid month value(s) at index(es): {display_list} (displaying only the first and last 10 items)\"\n",
    "            else:\n",
    "                message = f\"{total_issues} Invalid month value(s) at index(es): {invalid_entries}\"\n",
    "\n",
    "            return {\n",
    "                \"issue\": True,\n",
    "                \"error_message\": message,\n",
    "                \"dq_issue\": \"DQI #15 (Domain Violation - Accuracy)\"\n",
    "            }\n",
    "\n",
    "        return {\"issue\": False}\n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def handle_invalid_weekdays(df, column):\n",
    "        \"\"\"\n",
    "        Check if all values in the specified column are valid representations of weekdays.\n",
    "\n",
    "        Parameters:\n",
    "        - df (pd.DataFrame): The pandas DataFrame to analyze.\n",
    "        - column (str): The name of the column to analyze.\n",
    "\n",
    "        Returns:\n",
    "        - dict: A dictionary indicating if there are invalid weekday values, with their indices and values.\n",
    "        \"\"\"\n",
    "        invalid_entries = []\n",
    "        weekdays = [\"Sunday\", \"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\"]\n",
    "        weekday_abbr = [\"Sun\", \"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\"]\n",
    "        weekday_abbr_short = [\"su\", \"mo\", \"tu\", \"we\", \"th\", \"fr\", \"sa\"]\n",
    "\n",
    "        for idx, val in df[column].items():\n",
    "            val_str = str(val).strip().lower()\n",
    "            # Normalize full names and abbreviations to full weekday name\n",
    "            if val_str.title() in weekdays or val_str.title() in weekday_abbr or val_str in weekday_abbr_short:\n",
    "                continue\n",
    "            elif val_str.isdigit() and 0 <= int(val_str) <= 7:\n",
    "                continue\n",
    "            else:\n",
    "                invalid_entries.append((idx, val))\n",
    "\n",
    "        total_issues = len(invalid_entries)\n",
    "        if invalid_entries:\n",
    "            if total_issues > 20:\n",
    "                # Keep only the first 10 and last 10 items\n",
    "                first_10 = invalid_entries[:10]\n",
    "                last_10 = invalid_entries[-10:]\n",
    "                display_list = first_10 + [('...', '...')] + last_10\n",
    "                message = f\"{total_issues} Invalid weekday value(s) at index(es): {display_list} (displaying only the first and last 10 items)\"\n",
    "            else:\n",
    "                message = f\"{total_issues} Invalid weekday value(s) at index(es): {invalid_entries}\"\n",
    "\n",
    "            return {\n",
    "                \"issue\": True,\n",
    "                \"error_message\": message,\n",
    "                \"dq_issue\": \"DQI #15 (Domain Violation - Accuracy)\"\n",
    "            }\n",
    "\n",
    "        return {\"issue\": False}\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def handle_street_format(df, column):\n",
    "        \"\"\"\n",
    "        Check if the street names conform to a typical street format.\n",
    "        \"\"\"\n",
    "        invalid_format_indices = []\n",
    "        street_format_regex = re.compile(r'^[\\dA-Za-zÀ-ÖØ-öø-ÿ .,\\-\\'#]+(?:\\s[A-Za-zÀ-ÖØ-öø-ÿ]+)*$')\n",
    "\n",
    "        for idx, val in df[column].items():\n",
    "            if not isinstance(val, str) or not street_format_regex.match(val):\n",
    "                invalid_format_indices.append((idx, val))\n",
    "\n",
    "        total_issues = len(invalid_format_indices)\n",
    "        if invalid_format_indices:\n",
    "            if total_issues > 20:\n",
    "                # Keep only the first 10 and last 10 items\n",
    "                first_10 = invalid_format_indices[:10]\n",
    "                last_10 = invalid_format_indices[-10:]\n",
    "                display_list = first_10 + [('...', '...')] + last_10\n",
    "                issue_data = \", \".join([f\"({idx}, '{value}')\" for idx, value in display_list])\n",
    "                message = f\"{total_issues} Incorrect street format issue(s) at index(es): {issue_data} (displaying only the first and last 10 items)\"\n",
    "            else:\n",
    "                issue_data = \", \".join([f\"({idx}, '{value}')\" for idx, value in invalid_format_indices])\n",
    "                message = f\"{total_issues} Incorrect street format issue(s) at index(es): {issue_data}\"\n",
    "\n",
    "            return {\n",
    "                \"issue\": True,\n",
    "                \"error_message\": message,\n",
    "                \"dq_issue\": \"DQI #15 (Domain Violation - Accuracy)\"\n",
    "            }\n",
    "\n",
    "        return {\"issue\": False}\n",
    "\n",
    "    @staticmethod\n",
    "    def standardize_phone_number(phone_number):\n",
    "        \"\"\"\n",
    "        Standardize the phone number by removing common separators.\n",
    "        \"\"\"\n",
    "        return re.sub(r'[()\\-+ ]', '', str(phone_number))\n",
    "\n",
    "    @staticmethod\n",
    "    def handle_phone_number_format(df, column):\n",
    "        \"\"\"\n",
    "        Check if phone numbers in the specified column conform to expected formats.\n",
    "        \"\"\"\n",
    "        incorrect_format = []\n",
    "\n",
    "        for idx, phone_number in df[column].items():\n",
    "            if phone_number is None or isinstance(phone_number, str) and phone_number.strip() == '':\n",
    "                incorrect_format.append((idx, phone_number))\n",
    "                continue\n",
    "\n",
    "            cleaned_number = DataQualityIssues.standardize_phone_number(phone_number)\n",
    "            \n",
    "            if not (cleaned_number.isdigit() and 3 <= len(cleaned_number) <= 15):\n",
    "                incorrect_format.append((idx, phone_number))\n",
    "\n",
    "        total_issues = len(incorrect_format)\n",
    "        if incorrect_format:\n",
    "            if total_issues > 20:\n",
    "                # Keep only the first 10 and last 10 items\n",
    "                first_10 = incorrect_format[:10]\n",
    "                last_10 = incorrect_format[-10:]\n",
    "                display_list = first_10 + [('...', '...')] + last_10\n",
    "                # Convert the list to a string while keeping the square brackets\n",
    "                issue_data = str(display_list)\n",
    "                message = f\"{total_issues} Incorrect telephone number format issue(s) at index(es): {issue_data} (displaying only the first and last 10 items)\"\n",
    "            else:\n",
    "                # Convert the list to a string while keeping the square brackets\n",
    "                issue_data = str(incorrect_format)\n",
    "                message = f\"{total_issues} Incorrect telephone number format issue(s) at index(es): {issue_data}\"\n",
    "\n",
    "            return {\n",
    "                \"issue\": True,\n",
    "                \"error_message\": message,\n",
    "                \"dq_issue\": \"DQI #15 (Domain Violation - Accuracy)\"\n",
    "            }\n",
    "\n",
    "        return {\"issue\": False}\n",
    "\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def handle_ip_format(df, column):\n",
    "        # Regular expression patterns for IPv4 and IPv6\n",
    "        ipv4_pattern = re.compile(r'^(?:[0-9]{1,3}\\.){3}[0-9]{1,3}$')\n",
    "        ipv6_pattern = re.compile(r'^([0-9a-fA-F]{1,4}:){7}[0-9a-fA-F]{1,4}$')\n",
    "\n",
    "        incorrect_indices_and_values = []\n",
    "\n",
    "        for idx, ip in df[column].items():\n",
    "            ip_str = str(ip).strip()  # Convert to string and strip whitespace\n",
    "            if not (ipv4_pattern.match(ip_str) or ipv6_pattern.match(ip_str)):\n",
    "                incorrect_indices_and_values.append((idx, ip))\n",
    "\n",
    "        total_issues = len(incorrect_indices_and_values)\n",
    "        if incorrect_indices_and_values:\n",
    "            if total_issues > 20:\n",
    "                # Keep only the first 10 and last 10 items\n",
    "                first_10 = incorrect_indices_and_values[:10]\n",
    "                last_10 = incorrect_indices_and_values[-10:]\n",
    "                display_list = first_10 + [('...', '...')] + last_10\n",
    "                # Convert the list to a string while keeping the square brackets\n",
    "                issue_data = str(display_list)\n",
    "                message = f\"{total_issues} Invalid IP format issue(s) at index(es): {issue_data} (displaying only the first and last 10 items)\"\n",
    "            else:\n",
    "                # Convert the list to a string while keeping the square brackets\n",
    "                issue_data = str(incorrect_indices_and_values)\n",
    "                message = f\"{total_issues} Invalid IP format issue(s) at index(es): {issue_data}\"\n",
    "\n",
    "            return {\n",
    "                \"issue\": True,\n",
    "                \"error_message\": message,\n",
    "                \"dq_issue\": \"DQI #15 (Domain Violation - Accuracy)\"\n",
    "            }\n",
    "\n",
    "        return {\"issue\": False}\n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def handle_url_format(df, column):\n",
    "        url_pattern = re.compile(\n",
    "            r'^(https?:\\/\\/)?'  # protocol\n",
    "            r'((([a-zA-Z0-9-]+\\.)+[a-zA-Z]{2,})|'  # domain name\n",
    "            r'localhost|'  # localhost\n",
    "            r'\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3})'  # or ip\n",
    "            r'(?::\\d+)?(\\/[-a-zA-Z0-9@:%_\\+.~#?&//=]*)?$'  # port and path\n",
    "        )\n",
    "        incorrect_indices_and_values = []\n",
    "\n",
    "        for idx, url in df[column].items():\n",
    "            url_str = str(url)  # Convert to string\n",
    "            if not url_pattern.match(url_str):\n",
    "                incorrect_indices_and_values.append((idx, url))\n",
    "\n",
    "        total_issues = len(incorrect_indices_and_values)\n",
    "        if incorrect_indices_and_values:\n",
    "            if total_issues > 20:\n",
    "                # Keep only the first 10 and last 10 items\n",
    "                first_10 = incorrect_indices_and_values[:10]\n",
    "                last_10 = incorrect_indices_and_values[-10:]\n",
    "                display_list = first_10 + [('...', '...')] + last_10\n",
    "                # Convert the list to a string while keeping the square brackets\n",
    "                issue_data = str(display_list)\n",
    "                message = f\"{total_issues} Invalid URL format issue(s) at index(es): {issue_data} (displaying only the first and last 10 items)\"\n",
    "            else:\n",
    "                # Convert the list to a string while keeping the square brackets\n",
    "                issue_data = str(incorrect_indices_and_values)\n",
    "                message = f\"{total_issues} Invalid URL format issue(s) at index(es): {issue_data}\"\n",
    "\n",
    "            return {\n",
    "                \"issue\": True,\n",
    "                \"error_message\": message,\n",
    "                \"dq_issue\": \"DQI #15 (Domain Violation - Accuracy)\"\n",
    "            }\n",
    "\n",
    "        return {\"issue\": False}\n",
    "\n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def is_valid_email(email):\n",
    "        email_str = str(email)  # Convert to string\n",
    "\n",
    "        # Check the length of the email\n",
    "        if len(email_str) > 254:\n",
    "            return False\n",
    "\n",
    "        # Check if '@' is present\n",
    "        if '@' not in email_str:\n",
    "            return False\n",
    "\n",
    "        # Enhanced email regex pattern\n",
    "        email_pattern = re.compile(\n",
    "            r'^[a-zA-Z0-9.!#$%&\\'*+/=?^_`{|}~-]+@'  # local part\n",
    "            r'(?:[a-zA-Z0-9-]+)'  # domain name part (subdomains allowed)\n",
    "            r'(?:\\.[a-zA-Z0-9-]+)*'  # additional subdomains\n",
    "            r'\\.[a-zA-Z]{2,}$'  # TLD part\n",
    "        )\n",
    "\n",
    "        # Check for consecutive dots in local part\n",
    "        if '..' in email.split('@')[0]:\n",
    "            return False\n",
    "\n",
    "        # Extract and check the domain part\n",
    "        domain_part = email.split('@')[1]\n",
    "        if '--' in domain_part or domain_part.startswith('-') or domain_part.endswith('-'):\n",
    "            return False\n",
    "\n",
    "        return email_pattern.match(email) is not None\n",
    "\n",
    "    @staticmethod\n",
    "    def handle_email_format(df, column):\n",
    "        incorrect_indices_and_values = []\n",
    "        \n",
    "        for idx, email in df[column].items():\n",
    "            if not DataQualityIssues.is_valid_email(email):\n",
    "                incorrect_indices_and_values.append((idx, email))\n",
    "\n",
    "        total_issues = len(incorrect_indices_and_values)\n",
    "        if incorrect_indices_and_values:\n",
    "            if total_issues > 20:\n",
    "                # Keep only the first 10 and last 10 items\n",
    "                first_10 = incorrect_indices_and_values[:10]\n",
    "                last_10 = incorrect_indices_and_values[-10:]\n",
    "                display_list = first_10 + [('...', '...')] + last_10\n",
    "                # Convert the list to a string while keeping the square brackets\n",
    "                issue_data = str(display_list)\n",
    "                message = f\"{total_issues} Invalid email format issue(s) at index(es): {issue_data} (displaying only the first and last 10 items)\"\n",
    "            else:\n",
    "                # Convert the list to a string while keeping the square brackets\n",
    "                issue_data = str(incorrect_indices_and_values)\n",
    "                message = f\"{total_issues} Invalid email format issue(s) at index(es): {issue_data}\"\n",
    "\n",
    "            return {\n",
    "                \"issue\": True,\n",
    "                \"error_message\": message,\n",
    "                \"dq_issue\": \"DQI #15 (Domain Violation - Accuracy)\"\n",
    "            }\n",
    "\n",
    "        return {\"issue\": False}\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def handle_binary_values(df, column):\n",
    "        \"\"\"\n",
    "        Check if the values in the specified column are valid binary values or if the column has exactly two unique values.\n",
    "\n",
    "        Parameters:\n",
    "        - df (pd.DataFrame): The pandas DataFrame to analyze.\n",
    "        - column (str): The name of the column to check.\n",
    "\n",
    "        Returns:\n",
    "        - dict: A dictionary indicating if there are non-binary values, with their indices and values, and the associated DQI.\n",
    "        \"\"\"\n",
    "        # Calculate unique non-null values\n",
    "        unique_values = df[column].dropna().unique()\n",
    "        \n",
    "        # If there are exactly two unique values, consider it binary and return no issue\n",
    "        if len(unique_values) == 2:\n",
    "            return {\"issue\": False}\n",
    "        \n",
    "        # Define acceptable binary values (including lowercase)\n",
    "        true_values = ['1', 'True', 'Yes', 'T', 'Y']\n",
    "        false_values = ['0', 'False', 'No', 'F','N']\n",
    "\n",
    "        non_binary_indices = []\n",
    "\n",
    "        for idx, val in df[column].items():\n",
    "            str_val = str(val).strip().capitalize()  # Capitalize to match 'True', 'False', etc.\n",
    "            if str_val not in true_values + false_values:\n",
    "                non_binary_indices.append((idx, val))\n",
    "\n",
    "        total_issues = len(non_binary_indices)\n",
    "        if non_binary_indices:\n",
    "            if total_issues > 20:\n",
    "                # Keep only the first 10 and last 10 items\n",
    "                first_10 = non_binary_indices[:10]\n",
    "                last_10 = non_binary_indices[-10:]\n",
    "                display_list = first_10 + [('...', '...')] + last_10\n",
    "                # Convert the list to a string while keeping the square brackets\n",
    "                issue_data = str(display_list)\n",
    "                message = f\"{total_issues} Non-binary value(s) at index(es): {issue_data} (displaying only the first and last 10 items)\"\n",
    "            else:\n",
    "                # Convert the list to a string while keeping the square brackets\n",
    "                issue_data = str(non_binary_indices)\n",
    "                message = f\"{total_issues} Non-binary value(s) at index(es): {issue_data}\"\n",
    "\n",
    "            return {\n",
    "                \"issue\": True,\n",
    "                \"error_message\": message,\n",
    "                \"dq_issue\": \"DQI #15 (Domain Violation - Accuracy)\"\n",
    "            }\n",
    "\n",
    "        return {\"issue\": False}\n",
    "\n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def handle_non_numeric_values(df, column):\n",
    "        # Define a function to check if a value is non-numeric and not a special placeholder\n",
    "        def is_non_numeric_and_not_placeholder(x):\n",
    "            special_placeholders = [\"''\", '\"\"', \"``\"]  # List of special placeholder strings\n",
    "            x_str = str(x).strip()\n",
    "            if x_str in special_placeholders:\n",
    "                return False  # Do not flag special placeholders as non-numeric\n",
    "\n",
    "            try:\n",
    "                float(x_str)  # Attempt to convert to float\n",
    "                return False  # Conversion successful, value is numeric\n",
    "            except (ValueError, TypeError):\n",
    "                return True  # Conversion failed, value is non-numeric\n",
    "\n",
    "        # Get indices and values for non-numeric values\n",
    "        non_numeric_indices = df[df[column].apply(is_non_numeric_and_not_placeholder)].index\n",
    "        non_numeric_values = df.loc[non_numeric_indices, column].tolist()\n",
    "        issue_data = list(zip(non_numeric_indices, non_numeric_values))\n",
    "        total_issues = len(issue_data)\n",
    "\n",
    "        if issue_data:\n",
    "            if total_issues > 20:\n",
    "                first_10 = issue_data[:10]\n",
    "                last_10 = issue_data[-10:]\n",
    "                display_list = first_10 + [('...', '...')] + last_10\n",
    "                message = f\"{total_issues} Non-numeric value(s) at index(es): {display_list} (displaying only the first and last 10 items)\"\n",
    "            else:\n",
    "                message = f\"{total_issues} Non-numeric value(s) at index(es): {issue_data}\"\n",
    "\n",
    "            return {\n",
    "                \"issue\": True,\n",
    "                \"error_message\": message,\n",
    "                \"dq_issue\": \"DQI #17 (Wrong Data Type - Consistency)\"\n",
    "            }\n",
    "        return {\"issue\": False}\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def handle_non_alphanumeric_values(df, column):\n",
    "        # Define a function to check for strictly non-alphanumeric characters (excluding spaces and hyphens)\n",
    "        def is_strictly_non_alphanumeric(x):\n",
    "            return any(char not in string.ascii_letters + string.digits + ' -' for char in x)\n",
    "\n",
    "        # Get indices and values that are strictly non-alphanumeric\n",
    "        non_alphanumeric_indices = df[df[column].apply(lambda x: isinstance(x, str) and is_strictly_non_alphanumeric(x))].index\n",
    "        non_alphanumeric_values = df.loc[non_alphanumeric_indices, column].tolist()\n",
    "        issue_data = list(zip(non_alphanumeric_indices, non_alphanumeric_values))\n",
    "\n",
    "        total_issues = len(issue_data)\n",
    "        if issue_data:\n",
    "            if total_issues > 20:\n",
    "                # Keep only the first 10 and last 10 items\n",
    "                first_10 = issue_data[:10]\n",
    "                last_10 = issue_data[-10:]\n",
    "                display_list = first_10 + [('...', '...')] + last_10\n",
    "                # Convert the list to a string while keeping the square brackets\n",
    "                issue_data = str(display_list)\n",
    "                message = f\"{total_issues} Non-alphanumeric value(s) at index(es): {issue_data} (displaying only the first and last 10 items)\"\n",
    "            else:\n",
    "                # Convert the list to a string while keeping the square brackets\n",
    "                issue_data = str(issue_data)\n",
    "                message = f\"{total_issues} Non-alphanumeric value(s) at index(es): {issue_data}\"\n",
    "\n",
    "            return {\n",
    "                \"issue\": True,\n",
    "                \"error_message\": message,\n",
    "                \"dq_issue\": \"DQI #17 (Wrong Data Type - Consistency)\"\n",
    "            }\n",
    "\n",
    "        return {\"issue\": False}\n",
    "\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def handle_non_string_values(df, column):\n",
    "        \"\"\"\n",
    "        Flags values in a column that are not of string type.\n",
    "\n",
    "        Parameters:\n",
    "        - df (pd.DataFrame): The pandas DataFrame to analyze.\n",
    "        - column (str): The name of the column to check.\n",
    "\n",
    "        Returns:\n",
    "        - dict: A dictionary indicating if there are non-string values, with their indices and values, and the associated DQI.\n",
    "        \"\"\"\n",
    "        non_string_indices = df[df[column].apply(lambda x: not isinstance(x, str))].index\n",
    "        non_string_values = df.loc[non_string_indices, column].tolist()\n",
    "        issue_data = list(zip(non_string_indices, non_string_values))\n",
    "\n",
    "        total_issues = len(issue_data)\n",
    "        if issue_data:\n",
    "            if total_issues > 20:\n",
    "                # Keep only the first 10 and last 10 items\n",
    "                first_10 = issue_data[:10]\n",
    "                last_10 = issue_data[-10:]\n",
    "                display_list = first_10 + [('...', '...')] + last_10\n",
    "                # Convert the list to a string while keeping the square brackets\n",
    "                issue_data = str(display_list)\n",
    "                message = f\"{total_issues} Non-string value(s) at index(es): {issue_data} (displaying only the first and last 10 items)\"\n",
    "            else:\n",
    "                # Convert the list to a string while keeping the square brackets\n",
    "                issue_data = str(issue_data)\n",
    "                message = f\"{total_issues} Non-string value(s) at index(es): {issue_data}\"\n",
    "\n",
    "            return {\n",
    "                \"issue\": True,\n",
    "                \"error_message\": message,\n",
    "                \"dq_issue\": \"DQI #17 (Non-String Data Type - Consistency)\"\n",
    "            }\n",
    "\n",
    "        return {\"issue\": False}\n",
    "\n",
    "    @staticmethod\n",
    "    def handle_alphanumeric_consistency(df, column):\n",
    "        \"\"\"\n",
    "        Check if the alphanumeric values in the column have consistent total character length,\n",
    "        excluding numeric values from this check.\n",
    "        \"\"\"\n",
    "        def is_alphanumeric(val):\n",
    "            val_str = str(val).strip()\n",
    "            return val_str.isalnum() and not val_str.isdigit()\n",
    "\n",
    "        def get_total_length(val):\n",
    "            return len(str(val).strip())\n",
    "        \n",
    "        # Filter out negative and floating-point numbers\n",
    "        filtered_df = df[df[column].apply(lambda x: not isinstance(x, (float, int)) or x >= 0 and float(x).is_integer())]\n",
    "\n",
    "        # Sample the first 10 alphanumeric values\n",
    "        id_samples = filtered_df[filtered_df[column].apply(is_alphanumeric)][column].head(10)\n",
    "\n",
    "        length_set = {len(val) for val in id_samples if val.isalnum()}\n",
    "        alphanumeric_consistent = len(length_set) == 1\n",
    "\n",
    "        if not alphanumeric_consistent:\n",
    "            return {\"issue\": False}  # Return no issue if first 10 samples are not consistent\n",
    "        \n",
    "        # Determine the consistent length from the samples\n",
    "        length_set = {get_total_length(val) for val in id_samples}\n",
    "\n",
    "        consistent_length = length_set.pop()\n",
    "\n",
    "        # Check all alphanumeric values against the consistent length\n",
    "        inconsistent_indices = []\n",
    "        for idx, val in filtered_df[filtered_df[column].apply(is_alphanumeric)][column].items():\n",
    "            val_str = str(val).strip()\n",
    "            if get_total_length(val_str) != consistent_length:\n",
    "                inconsistent_indices.append((idx, val))\n",
    "\n",
    "        total_issues = len(inconsistent_indices)\n",
    "        if total_issues > 0:\n",
    "            if total_issues > 20:\n",
    "                first_10 = inconsistent_indices[:10]\n",
    "                last_10 = inconsistent_indices[-10:]\n",
    "                display_list = first_10 + [('...', '...')] + last_10\n",
    "                issue_data = str(display_list)\n",
    "                message = f\"{total_issues} Inconsistent length in alphanumeric value(s) at index(es): {issue_data} (displaying only the first and last 10 items)\"\n",
    "            else:\n",
    "                issue_data = str(inconsistent_indices)\n",
    "                message = f\"{total_issues} Inconsistent length in alphanumeric value(s) at index(es): {issue_data}\"\n",
    "\n",
    "            return {\n",
    "                \"issue\": True,\n",
    "                \"error_message\": message,\n",
    "                \"dq_issue\": \"DQI #17 (Wrong Data Type - Consistency)\"\n",
    "            }\n",
    "        return {\"issue\": False}\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def handle_uniqueness_violation(df, column):\n",
    "        unique_violation_indices = df[df[column].duplicated()].index\n",
    "        unique_violation_values = df.loc[unique_violation_indices, column].tolist()\n",
    "        issue_data = list(zip(unique_violation_indices, unique_violation_values))\n",
    "        total_issues = len(issue_data)\n",
    "\n",
    "        if issue_data:\n",
    "            if total_issues > 20:\n",
    "                first_10 = issue_data[:10]\n",
    "                last_10 = issue_data[-10:]\n",
    "                display_list = first_10 + [('...', '...')] + last_10\n",
    "                message = f\"{total_issues} Uniqueness violation(s) at index(es): {display_list} (displaying only the first and last 10 items)\"\n",
    "            else:\n",
    "                message = f\"{total_issues} Uniqueness violation(s) at index(es): {issue_data}\"\n",
    "\n",
    "            return {\n",
    "                \"issue\": True,\n",
    "                \"error_message\": message,\n",
    "                \"dq_issue\": \"DQI #19 (Uniqueness Violation - Uniqueness)\"\n",
    "            }\n",
    "        return {\"issue\": False}\n",
    "\n",
    "    @staticmethod\n",
    "    def handle_special_characters(df, column):\n",
    "        \"\"\"\n",
    "        Check for special characters in the specified column.\n",
    "        Allow periods in names for titles (e.g., Mr., Mrs.).\n",
    "        \"\"\"\n",
    "        def has_special_chars(x):\n",
    "            return any(char in x for char in ['!', '@', '#', '$', '%', '^', '&', '*', '(', ')', '+', '=', '{', '}', '[', ']', '|', '\\\\', ':', ';', '\"', \"'\", '<', '>', ',', '/', '?']) and '.' not in x\n",
    "\n",
    "        # Get indices and values with special characters\n",
    "        special_chars_indices = df[df[column].apply(lambda x: isinstance(x, str) and has_special_chars(x))].index\n",
    "        special_chars_values = df.loc[special_chars_indices, column].tolist()\n",
    "        issue_data = list(zip(special_chars_indices, special_chars_values))\n",
    "\n",
    "        total_issues = len(issue_data)\n",
    "        if issue_data:\n",
    "            if total_issues > 20:\n",
    "                # Keep only the first 10 and last 10 items\n",
    "                first_10 = issue_data[:10]\n",
    "                last_10 = issue_data[-10:]\n",
    "                display_list = first_10 + [('...', '...')] + last_10\n",
    "                issue_data = \", \".join([f\"({idx}, '{value}')\" for idx, value in display_list])\n",
    "                message = f\"{total_issues} Special character(s) at index(es): {issue_data} (displaying only the first and last 10 items)\"\n",
    "            else:\n",
    "                issue_data = \", \".join([f\"({idx}, '{value}')\" for idx, value in issue_data])\n",
    "                message = f\"{total_issues} Special character(s) at index(es): {issue_data}\"\n",
    "\n",
    "            return {\n",
    "                \"issue\": True,\n",
    "                \"error_message\": message,\n",
    "                \"dq_issue\": \"DQI #21 (Use of Special Characters - Consistency)\"\n",
    "            }\n",
    "\n",
    "        return {\"issue\": False}\n",
    "\n",
    "from datetime import datetime\n",
    "print(f\"Last run on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "07 Check Numerical ge zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1978,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error(s) found: \n",
      "DQI #1 (Missing Data - Completeness):\n",
      " 11 Blank/Empty/Null/Missing value(s) at index(es): \n",
      "3 NULL values: [(6, 'NULL'), (10, 'NULL'), (12, 'NULL')]\n",
      "8 Non-NULL values: [(5, '?'), (8, 'Empty'), (9, 'Empty'), (13, 'Empty'), (14, 'Empty'), (15, ';'), (16, '*'), (17, '-')]\n",
      "\n",
      "DQI #15 (Domain Violation - Accuracy):\n",
      " 1 Negative value(s) at index(es): [(7, -1)]\n",
      "\n",
      "DQI #17 (Wrong Data Type - Consistency):\n",
      " 1 Non-numeric value(s) at index(es): [(3, 'a3')]\n",
      "\n",
      "Range of values: (-1.0:5.67).\n",
      "Last run on: 2025-04-04 19:28:37\n"
     ]
    }
   ],
   "source": [
    "def check_numerical_ge_zero(df: pd.DataFrame, column: str) -> dict:\n",
    "    \"\"\"\n",
    "    Check if all the values in the specified numerical column are greater than or equal to zero,\n",
    "    and also flag any non-numeric values and Blank/Empty/Null/Missing values.\n",
    "    Additionally, report the range of numeric values.\n",
    "    \"\"\"\n",
    "    # First, handle Blank/Empty/Null/Missing values\n",
    "    result_blank = DataQualityIssues.handle_blank_empty_null_nan(df, column)\n",
    "    \n",
    "    # Filtering df based on blank checks\n",
    "    blank_indices = list(result_blank[\"blank_indices\"])\n",
    "    df_filtered = df[~df.index.isin(blank_indices)]\n",
    "\n",
    "    # Only proceed with other checks if the value is not Blank/Empty/Null/Missing\n",
    "    #df_filtered = df[~df[column].apply(lambda x: pd.isnull(x) or str(x).strip() == '' or str(x).lower() == 'null')]\n",
    "    \n",
    "    result_negative = DataQualityIssues.handle_negative_values(df_filtered, column)\n",
    "    result_non_numeric = DataQualityIssues.handle_non_numeric_values(df_filtered, column)\n",
    "\n",
    "    error_summary_parts = []\n",
    "\n",
    "    if result_blank['issue']:\n",
    "        error_summary_parts.append(result_blank['dq_issue'] + ':\\n ' + result_blank['error_message']+ '\\n')\n",
    "\n",
    "    if result_negative['issue']:\n",
    "        error_summary_parts.append(result_negative['dq_issue'] + ':\\n ' + result_negative['error_message']+ '\\n')\n",
    "\n",
    "    if result_non_numeric['issue']:\n",
    "        error_summary_parts.append(result_non_numeric['dq_issue'] + ':\\n ' + result_non_numeric['error_message']+ '\\n')\n",
    "\n",
    "   # Convert the column to numeric, ignoring errors to leave non-numeric as NaN\n",
    "    numeric_values = pd.to_numeric(df_filtered[column], errors='coerce')\n",
    "\n",
    "    # Filter out non-numeric values to avoid them in min/max calculations\n",
    "    numeric_values_filtered = numeric_values.dropna()\n",
    "\n",
    "    # Calculate the range (min and max) of the numeric values\n",
    "    min_value = numeric_values_filtered.min()\n",
    "    max_value = numeric_values_filtered.max()\n",
    "    \n",
    "    # Count correct values\n",
    "    correct_values_count = df_filtered[df_filtered[column].apply(lambda x: pd.to_numeric(x, errors='coerce') >= 0)].shape[0]\n",
    "\n",
    "    if not error_summary_parts:\n",
    "        range_summary = f\"in the range ({min_value}:{max_value})\" if not numeric_values_filtered.empty else \"No valid numerical values found.\"\n",
    "        return f\"All {correct_values_count} values are numerical and greater or equal to 0 {range_summary}.\"\n",
    "\n",
    "    error_summary = \"Error(s) found: \\n\" + \"\\n\".join(error_summary_parts)\n",
    "    range_summary = f\"Range of values: ({min_value}:{max_value}).\" if not numeric_values_filtered.empty else \"No valid numerical values found.\"\n",
    "\n",
    "\n",
    "    return f\"{error_summary}\\n{range_summary}\"\n",
    "\n",
    "# Test function\n",
    "df_test = pd.DataFrame({'hours-per-week': [1, 2, 3, 'a3', 5, '?', None, -1, '', \" \", 'null', 5.67, np.nan, '    ',  \"''\", ';', '*', '-']})\n",
    "#df_test = pd.DataFrame({'hours-per-week': [1, 2, 3]})\n",
    "result = check_numerical_ge_zero(df_test, 'hours-per-week')\n",
    "print(result)\n",
    "\n",
    "from datetime import datetime\n",
    "print(f\"Last run on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "08 Check Numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1979,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error(s) found: \n",
      "DQI #1 (Missing Data - Completeness):\n",
      " 6 Blank/Empty/Null/Missing value(s) at index(es): \n",
      "2 NULL values: [(6, 'NULL'), (10, 'NULL')]\n",
      "4 Non-NULL values: [(5, '?'), (8, 'Empty'), (9, 'Empty'), (12, 'Empty')]\n",
      "\n",
      "DQI #17 (Wrong Data Type - Consistency):\n",
      " 1 Non-numeric value(s) at index(es): [(3, 'a3')]\n",
      "Range of values: (-1.0:5.67).\n",
      "Last run on: 2025-04-04 19:28:37\n"
     ]
    }
   ],
   "source": [
    "def check_numerical(df: pd.DataFrame, column: str) -> str:\n",
    "    \"\"\"\n",
    "    Check if all the values in the specified numerical column are numerical.\n",
    "    Additionally, report the range of numeric values.\n",
    "    \"\"\"\n",
    "    # First, handle Blank/Empty/Null/Missing values\n",
    "    result_blank = DataQualityIssues.handle_blank_empty_null_nan(df, column)\n",
    "\n",
    "    # Filtering df based on blank checks\n",
    "    blank_indices = list(result_blank[\"blank_indices\"])\n",
    "    df_filtered = df[~df.index.isin(blank_indices)]\n",
    "    \n",
    "    result_non_numeric = DataQualityIssues.handle_non_numeric_values(df_filtered, column)\n",
    "\n",
    "    error_summary_parts = []\n",
    "\n",
    "    if result_blank['issue']:\n",
    "        error_summary_parts.append(result_blank['dq_issue'] + ':\\n ' + result_blank['error_message']+ '\\n')\n",
    "\n",
    "    if result_non_numeric['issue']:\n",
    "        error_summary_parts.append(result_non_numeric['dq_issue'] + ':\\n ' + result_non_numeric['error_message'])\n",
    "\n",
    "    # Convert the column to numeric, ignoring errors to leave non-numeric as NaN\n",
    "    numeric_values = pd.to_numeric(df_filtered[column], errors='coerce')\n",
    "\n",
    "    # Filter out non-numeric values to avoid them in min/max calculations\n",
    "    numeric_values_filtered = numeric_values.dropna()\n",
    "\n",
    "    # Calculate the range (min and max) of the numeric values\n",
    "    min_value = numeric_values_filtered.min()\n",
    "    max_value = numeric_values_filtered.max()\n",
    "\n",
    "    # Count correct values\n",
    "    correct_values_count = numeric_values_filtered.notna().sum()\n",
    "\n",
    "    if not error_summary_parts:\n",
    "        range_summary = f\"in the range ({min_value}:{max_value})\" if not numeric_values_filtered.empty else \"No valid numeric values found.\"\n",
    "        return f\"All {correct_values_count} values are numerical {range_summary}.\"\n",
    "\n",
    "    error_summary = \"Error(s) found: \\n\" + \"\\n\".join(error_summary_parts)\n",
    "    range_summary = f\"Range of values: ({min_value}:{max_value}).\" if not numeric_values_filtered.empty else \"No valid numeric values found.\"\n",
    "\n",
    "    return f\"{error_summary}\\n{range_summary}\"\n",
    "\n",
    "# Test function\n",
    "#df_test = pd.DataFrame({'hours': [1, 2, 3, -2, 4.777]})\n",
    "df_test = pd.DataFrame({'hours': [1, 2, 3, 'a3', 5, '?', None, -1, '', \" \", 'null', 5.67, '  ']})\n",
    "result = check_numerical(df_test, 'hours')\n",
    "print(result)\n",
    "\n",
    "from datetime import datetime\n",
    "print(f\"Last run on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "09 Check Numerical between"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1980,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error(s) found: \n",
      "DQI #1 (Missing Data - Completeness):\n",
      " 6 Blank/Empty/Null/Missing value(s) at index(es): \n",
      "2 NULL values: [(6, 'NULL'), (10, 'NULL')]\n",
      "4 Non-NULL values: [(5, '?'), (8, 'Empty'), (9, 'Empty'), (11, 'Empty')]\n",
      "\n",
      "DQI #15 (Domain Violation - Accuracy):\n",
      " 3 Value(s) outside range [0, 130] at index(es): [(2, 131), (7, -1), (12, 140.0)]\n",
      "\n",
      "DQI #17 (Wrong Data Type - Consistency):\n",
      " 1 Non-numeric value(s) at index(es): [(3, 'a3')]\n",
      "\n",
      "Actual range of values: (-1.0 : 140.0)\n",
      "Last run on: 2025-04-04 19:28:37\n"
     ]
    }
   ],
   "source": [
    "def check_numerical_between(df: pd.DataFrame, column: str, min_value: float, max_value: float) -> str:\n",
    "    \"\"\"\n",
    "    Check if all values in the specified column are numerical and fall within the given range.\n",
    "    Additionally, report the actual range of numeric values.\n",
    "    \"\"\"\n",
    "    # First, handle Blank/Empty/Null/Missing values\n",
    "    result_blank = DataQualityIssues.handle_blank_empty_null_nan(df, column)\n",
    "\n",
    "    # Only proceed with other checks if the value is not Blank/Empty/Null/Missing\n",
    "    # Filtering df based on blank checks\n",
    "    blank_indices = list(result_blank[\"blank_indices\"])\n",
    "    df_filtered = df[~df.index.isin(blank_indices)]\n",
    "\n",
    "    # Checks for non-numeric and out-of-range values\n",
    "    result_non_numeric = DataQualityIssues.handle_non_numeric_values(df_filtered, column)\n",
    "    result_outside_range = DataQualityIssues.handle_values_outside_range(df_filtered, column, min_value, max_value)\n",
    "\n",
    "    error_summary_parts = []\n",
    "\n",
    "    if result_blank['issue']:\n",
    "        error_summary_parts.append(result_blank['dq_issue'] + ':\\n ' + result_blank['error_message'] + '\\n')\n",
    "\n",
    "    if result_outside_range['issue']:\n",
    "        error_summary_parts.append(result_outside_range['dq_issue'] + ':\\n ' + result_outside_range['error_message'] + '\\n')\n",
    "\n",
    "    if result_non_numeric['issue']:\n",
    "        error_summary_parts.append(result_non_numeric['dq_issue'] + ':\\n ' + result_non_numeric['error_message'] + '\\n')\n",
    "\n",
    "    # Convert the column to numeric, ignoring errors to leave non-numeric as NaN\n",
    "    numeric_values = pd.to_numeric(df_filtered[column], errors='coerce')\n",
    "\n",
    "    # Filter out non-numeric values to avoid them in min/max calculations\n",
    "    numeric_values_filtered = numeric_values.dropna()\n",
    "\n",
    "    # Calculate the range (min and max) of the numeric values\n",
    "    actual_min_value = numeric_values_filtered.min()\n",
    "    actual_max_value = numeric_values_filtered.max()\n",
    "\n",
    "    range_summary = f\"Actual range of values: ({actual_min_value} : {actual_max_value})\" if not numeric_values_filtered.empty else \"No valid numeric values found.\"\n",
    "\n",
    "    if not error_summary_parts:\n",
    "        return f\"All {len(numeric_values_filtered)} values are numerical and valid in the range [{min_value}, {max_value}].\\n{range_summary}\"\n",
    "\n",
    "    error_summary = \"Error(s) found: \\n\" + \"\\n\".join(error_summary_parts)\n",
    "\n",
    "    return f\"{error_summary}\\n{range_summary}\"\n",
    "\n",
    "# Test function\n",
    "df_test = pd.DataFrame({'age': [1, 2, 131, 'a3', 5, '?', None, -1, '', \"\", 'NULL', '  ', 140.0, 129.0]})\n",
    "result = check_numerical_between(df_test, 'age', 0, 130)\n",
    "print(result)\n",
    "\n",
    "from datetime import datetime\n",
    "print(f\"Last run on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10 Check ID column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1981,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking primary key: id_column\n",
      "Checking column 'id_column' (Foreign Key: False)\n",
      "Column id_column is not FK\n",
      "Error(s) found: \n",
      "DQI #1 (Missing Data - Completeness):\n",
      " 7 Blank/Empty/Null/Missing value(s) at index(es): \n",
      "2 NULL values: [(35, 'NULL'), (40, 'NULL')]\n",
      "5 Non-NULL values: [(32, '?'), (38, 'Empty'), (39, 'Empty'), (41, 'Empty'), (42, 'None')]\n",
      "\n",
      "DQI #9 (Duplicates - Uniqueness):\n",
      " 5 Duplicate value(s) at index(es): [[0, 'AB123CD456'], [20, 'Duplicate'], [21, 'Duplicate'], [23, 'AB123CD456'], [31, 'AB123CD456']]\n",
      "\n",
      "DQI #15 (Domain Violation - Accuracy):\n",
      " 1 Negative value(s) at index(es): [(36, -1)]\n",
      "\n",
      "DQI #17 (Wrong Data Type - Consistency):\n",
      " 5 Inconsistent length in alphanumeric value(s) at index(es): [(20, 'Duplicate'), (21, 'Duplicate'), (22, 'WrongLength1'), (30, 'AB123CD48'), (34, 'Aaa')]\n",
      "\n",
      "DQI #19 (Uniqueness Violation - Uniqueness):\n",
      " 3 Uniqueness violation(s) at index(es): [(21, 'Duplicate'), (23, 'AB123CD456'), (31, 'AB123CD456')]\n",
      "\n",
      "Alphanumeric range of values: (-- : WrongLength1)\n",
      "Last run on: 2025-04-04 19:28:37\n"
     ]
    }
   ],
   "source": [
    "def check_id_attributes(df: pd.DataFrame, column: str, is_foreign_key: bool = False) -> str:\n",
    "    print(f\"Checking {'composite ' if '|' in column else ''}{'foreign' if is_foreign_key else 'primary'} key: {column}\")\n",
    "    \"\"\"\n",
    "    Check if the values in the specified column are suitable for use as a Primary Key (PK) or Foreign Key (FK).\n",
    "    This function checks for blank, non-numeric, negative values, and uniqueness violations.\n",
    "    For PKs, it also checks for duplicates.\n",
    "    Additionally, report the range of numeric or alphanumeric ID values.\n",
    "    \"\"\"\n",
    "    print(f\"Checking column '{column}' (Foreign Key: {is_foreign_key})\")\n",
    "    \n",
    "    # Calculate the total number of values in the column\n",
    "    total_values_count = df[column].size\n",
    "\n",
    "    # Handle Blank/Empty/Null/Missing values\n",
    "    result_blank = DataQualityIssues.handle_blank_empty_null_nan(df, column)\n",
    "\n",
    "    # Filtering df based on blank checks\n",
    "    blank_indices = list(result_blank[\"blank_indices\"])\n",
    "    df_filtered = df[~df.index.isin(blank_indices)]\n",
    " \n",
    "    # Handle negative values in the filtered DataFrame\n",
    "    result_negative = DataQualityIssues.handle_negative_values(df_filtered, column)\n",
    "\n",
    "    # Check for floating-point numbers in the filtered DataFrame\n",
    "    result_floating_point = DataQualityIssues.handle_floating_point_values(df_filtered, column)\n",
    "\n",
    "    # Check if values are alphanumeric and consistent in length in the filtered DataFrame\n",
    "    result_alphanumeric_consistency = DataQualityIssues.handle_alphanumeric_consistency(df_filtered, column)\n",
    "  \n",
    "    # Handle duplicates and uniqueness violations only if it's not a foreign key\n",
    "    if not is_foreign_key:\n",
    "        result_duplicates = DataQualityIssues.handle_duplicates(df_filtered, column)\n",
    "        result_uniqueness = DataQualityIssues.handle_uniqueness_violation(df_filtered, column)\n",
    "        print(f\"Column {column} is not FK\")\n",
    "    else:\n",
    "        result_duplicates = {\"issue\": False}\n",
    "        result_uniqueness = {\"issue\": False}\n",
    "        print(f\"Skipping duplicate and uniqueness checks for FK column '{column}'\")\n",
    "\n",
    "    error_summary_parts = []\n",
    "\n",
    "    # Append results to error summary\n",
    "    if result_blank['issue']:\n",
    "        error_summary_parts.append(result_blank['dq_issue'] + ':\\n ' + result_blank['error_message'] + '\\n')\n",
    "\n",
    "    if result_duplicates['issue']:\n",
    "        error_summary_parts.append(result_duplicates['dq_issue'] + ':\\n ' + result_duplicates['error_message'] + '\\n')\n",
    "\n",
    "    if result_negative['issue']:\n",
    "        error_summary_parts.append(result_negative['dq_issue'] + ':\\n ' + result_negative['error_message'] + '\\n')\n",
    "\n",
    "    if result_floating_point['issue']:\n",
    "        error_summary_parts.append(result_floating_point['dq_issue'] + ':\\n ' + result_floating_point['error_message'] + '\\n')\n",
    "\n",
    "    if result_alphanumeric_consistency['issue']:\n",
    "        error_summary_parts.append(result_alphanumeric_consistency['dq_issue'] + ':\\n ' + result_alphanumeric_consistency['error_message'] + '\\n')\n",
    "\n",
    "    if not is_foreign_key and result_uniqueness['issue']:\n",
    "        error_summary_parts.append(result_uniqueness['dq_issue'] + ':\\n ' + result_uniqueness['error_message'] + '\\n')\n",
    "\n",
    "    # Calculate the range (min and max) of the values\n",
    "    filtered_values = df_filtered[column].dropna()\n",
    "    \n",
    "    if filtered_values.empty:\n",
    "        range_summary = \"No valid values found.\"\n",
    "    elif filtered_values.dtype.kind in 'biufc':  # Check if all values are numeric\n",
    "        min_value = filtered_values.min()\n",
    "        max_value = filtered_values.max()\n",
    "        range_summary = f\"Numeric range of values: ({min_value} : {max_value})\"\n",
    "    else:\n",
    "        alphanumeric_values = filtered_values.astype(str)\n",
    "        alphanumeric_min_value = alphanumeric_values.min()\n",
    "        alphanumeric_max_value = alphanumeric_values.max()\n",
    "        range_summary = f\"Alphanumeric range of values: ({alphanumeric_min_value} : {alphanumeric_max_value})\"\n",
    "\n",
    "    if not error_summary_parts:\n",
    "        if is_foreign_key:\n",
    "            return f\"All {total_values_count} ID values are valid for use as a Foreign Key, and duplicate values are allowed.\\n{range_summary}\"\n",
    "        else:\n",
    "            return f\"All {total_values_count} ID values are unique and valid, and thus suitable for use as a Primary Key.\\n{range_summary}\"\n",
    "\n",
    "    error_summary = \"Error(s) found: \\n\" + \"\\n\".join(error_summary_parts)\n",
    "    if is_foreign_key:\n",
    "        return f\"{error_summary}\\nNote: This is a Foreign Key, so duplicate values are allowed.\\n{range_summary}\"\n",
    "    else:\n",
    "        return f\"{error_summary}\\n{range_summary}\"\n",
    "\n",
    "# Example usage\n",
    "'''df_test = pd.DataFrame({'id_column': [1, 2, 3, 5, None, -1, 1, '', \" \", 'null', 5.67,'&']})'''\n",
    "\n",
    "'''df_test = pd.DataFrame({'id_column': [1, 2, 3, 5, 'NULL']})'''\n",
    "\n",
    "df_test = pd.DataFrame({\n",
    "    'id_column': ['AB123CD456', 'AB123CD457', 'AB123CD458', 'AB123CD459', 'AB123CD460', \n",
    "                  'AB123CD461', 'AB123CD462', 'AB123CD463', 'AB123CD464', 'AB123CD465', \n",
    "                  'AB123CD466', 'AB123CD467', 'AB123CD468', 'AB123CD469', 'AB123CD470', \n",
    "                  'AB123CD471', 'AB123CD472', 'AB123CD473', 'AB123CD474', 'AB123CD475', \n",
    "                  'Duplicate', 'Duplicate', 'WrongLength1', 'AB123CD456', 'AB123CD479', \n",
    "                  'AB123CD480', 'AB123CD481', 'AB123CD4-2', 'AB123CD*83', 'AB123C_484', \n",
    "                  'AB123CD48', 'AB123CD456', '?','--', 'Aaa', None, -1, 1, '', \" \", 'NULL', '\"\"', 'None']})\n",
    "\n",
    "'''df_test = pd.DataFrame({\n",
    "    'id_column': ['AB123CD456', 'AB123CD457', 'AB123CD458', 'AB123CD459', 'AB123CD460', \n",
    "                  'AB123CD461', 'AB123CD462', 'AB123CD463', 'AB123CD464', 'AB123CD465', \n",
    "                  'AB123CD466', 'AB123CD467', 'AB123CD468', 'AB123CD469', 'AB123CD470', \n",
    "                  'AB123CD471', 'AB123CD472', 'AB123CD473']})'''\n",
    "\n",
    "result = check_id_attributes(df_test, 'id_column')\n",
    "print(result)\n",
    "\n",
    "from datetime import datetime\n",
    "print(f\"Last run on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11 Check String"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1982,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error(s) found: \n",
      "DQI #1 (Missing Data - Completeness):\n",
      " 6 Blank/Empty/Null/Missing value(s) at index(es): \n",
      "2 NULL values: [(1, 'NULL'), (2, 'NULL')]\n",
      "4 Non-NULL values: [(3, 'Empty'), (4, 'Empty'), (5, 'Empty'), (11, '*')]\n",
      "\n",
      "DQI #17 (Non-String Data Type - Consistency):\n",
      " 4 Non-string value(s) at index(es): [(6, 123), (7, 5.67), (8, True), (9, {'key': 'value'})]\n",
      "String range (lexicographical): (  Goodbye  : {'key': 'value'})\n",
      "Frequency Distribution:\n",
      "     text_column  Frequency\n",
      "           Empty          3\n",
      "            NULL          2\n",
      "        Goodbye           1\n",
      "               *          1\n",
      "             123          1\n",
      "            5.67          1\n",
      "           Hello          1\n",
      "            True          1\n",
      "{'key': 'value'}          1\n",
      "Last run on: 2025-04-04 19:28:37\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def check_string(df: pd.DataFrame, column: str) -> str:\n",
    "    \"\"\"\n",
    "    Check if all the values in the specified column are non-empty, non-null strings.\n",
    "    Additionally, report the range of string values based on lexicographical order.\n",
    "    \"\"\"\n",
    "    # Calculate the total number of values in the column\n",
    "    total_values_count = df[column].size\n",
    "\n",
    "    if column not in df.columns:\n",
    "        return f\"Column {column} does not exist in the DataFrame\"\n",
    "    \n",
    "    try:\n",
    "        # First, handle Blank/Empty/Null/Missing values\n",
    "        result_blank = DataQualityIssues.handle_blank_empty_null_nan(df, column)\n",
    "\n",
    "        # Filtering df based on blank checks\n",
    "        blank_indices = list(result_blank[\"blank_indices\"])\n",
    "        df_filtered = df[~df.index.isin(blank_indices)]\n",
    "        result_non_string = DataQualityIssues.handle_non_string_values(df_filtered, column)\n",
    "\n",
    "        error_summary_parts = []\n",
    "        \n",
    "        if result_blank['issue']:\n",
    "            error_summary_parts.append(result_blank['dq_issue'] + ':\\n ' + result_blank['error_message'] + '\\n')\n",
    "        \n",
    "        if result_non_string['issue']:\n",
    "            error_summary_parts.append(result_non_string['dq_issue'] + ':\\n ' + result_non_string['error_message'] + '\\n')\n",
    "\n",
    "        # Calculate the lexicographical range of the string values\n",
    "        string_values = df_filtered[column].dropna().astype(str)\n",
    "        lex_min_value = string_values.min()\n",
    "        lex_max_value = string_values.max()\n",
    "\n",
    "        range_summary = f\"String range (lexicographical): ({lex_min_value} : {lex_max_value})\" if not string_values.empty else \"No valid string values found.\"\n",
    "\n",
    "        value_counts = df[column].apply(DataQualityIssues.represent_value).value_counts().to_dict()\n",
    "        frequency_table = pd.DataFrame(value_counts.items(), columns=[column, 'Frequency'])\n",
    "        frequency_table = frequency_table.sort_values(by=['Frequency', column], ascending=[False, True]).reset_index(drop=True)\n",
    "\n",
    "        if len(frequency_table) > 20:\n",
    "            top_rows = frequency_table.head(10)\n",
    "            bottom_rows = frequency_table.tail(10)\n",
    "            ellipsis_row = pd.DataFrame([(\"...\", \"...\")], columns=[column, 'Frequency'])\n",
    "            display_table = pd.concat([top_rows, ellipsis_row, bottom_rows], ignore_index=True)\n",
    "        else:\n",
    "            display_table = frequency_table\n",
    "\n",
    "        frequency_distribution = f\"\\nFrequency Distribution:\\n{display_table.to_string(index=False)}\"\n",
    "\n",
    "        if error_summary_parts:\n",
    "            return \"Error(s) found: \\n\" + \"\\n\".join(error_summary_parts) + range_summary + frequency_distribution\n",
    "        else:\n",
    "            return f\"All {total_values_count} string values are valid. {range_summary}\\n{frequency_distribution}\"\n",
    "    except Exception as e:\n",
    "        return str(e)\n",
    "\n",
    "\n",
    "# Example usage\n",
    "df_bad_string_data = pd.DataFrame({\n",
    "    'text_column': [\n",
    "        'Hello',          # Valid string\n",
    "        None,             # Null value\n",
    "        np.nan,           # NaN value (also treated as null)\n",
    "        '',               # Empty string\n",
    "        ' ', \"   \",       # String with only spaces\n",
    "        123,              # Integer (non-string data type)\n",
    "        5.67,             # Float (non-string data type)\n",
    "        True,             # Boolean (non-string data type)\n",
    "        {'key': 'value'}, # Dictionary (non-string data type)\n",
    "        '  Goodbye ',     # Whitespace-padded string\n",
    "        '*'               # Missing value\n",
    "    ]\n",
    "}, columns=['text_column'])\n",
    "\n",
    "'''df_bad_string_data = pd.DataFrame({\n",
    "    'text_column': [\n",
    "        'Hello',          # Valid string\n",
    "        '  Goodbye  '     # Whitespace-padded string\n",
    "    ]\n",
    "}, columns=['text_column'])'''\n",
    "\n",
    "\n",
    "result = check_string(df_bad_string_data, 'text_column')\n",
    "print(result)\n",
    "\n",
    "from datetime import datetime\n",
    "print(f\"Last run on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12 Check Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2006,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error(s) found: \n",
      "DQI #1 (Missing Data - Completeness):\n",
      " 10 Blank/Empty/Null/Missing value(s) at index(es): \n",
      "2 NULL values: [(4, 'NULL'), (13, 'NULL')]\n",
      "8 Non-NULL values: [(3, 'Empty'), (12, '?'), (14, 'Empty'), (15, 'Empty'), (16, 'Empty'), (18, 'N/A'), (19, 'Empty'), (20, '?')]\n",
      "\n",
      "DQI #4 (Ambiguous Data - Accuracy, Consistency):\n",
      " 1 Unacceptable value(s) at index(es): [(21, 'nonexistent')]\n",
      "\n",
      "Categorical format with 17 unique value(s):\n",
      "   Category  Frequency\n",
      "      Empty          5\n",
      "          ?          2\n",
      "       NULL          2\n",
      "       bird          2\n",
      "        cat          2\n",
      "        dog          2\n",
      "       fish          2\n",
      "        100          1\n",
      "        200          1\n",
      "        N/A          1\n",
      "         NA          1\n",
      "nonexistent          1\n",
      "Last run on: 2025-04-04 19:34:23\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def check_categorical(df, column, threshold=100):\n",
    "    # Calculate the total number of values in the column\n",
    "    total_values_count = df[column].size\n",
    "    error_summary_parts = []\n",
    "        \n",
    "    # Handle Blank/Empty/Null/Missing values\n",
    "    result_blank = DataQualityIssues.handle_blank_empty_null_nan(df, column)\n",
    "    blank_indices = list(result_blank[\"blank_indices\"])\n",
    "\n",
    "    if result_blank['issue']:\n",
    "        error_summary_parts.append(result_blank['dq_issue'] + ':\\n ' + result_blank['error_message'] + '\\n')\n",
    "\n",
    "    df_filtered_for_dqi4 = df[~df.index.isin(blank_indices)]\n",
    "\n",
    "    # Handle predefined unacceptable values\n",
    "    result_unacceptable_values = DataQualityIssues.handle_predefined_unacceptable_values(df_filtered_for_dqi4, column)\n",
    "    unacceptable_indices = list(result_unacceptable_values[\"unacceptable_indices\"])\n",
    "\n",
    "    if result_unacceptable_values['issue']:\n",
    "        error_summary_parts.append(result_unacceptable_values['dq_issue'] + ':\\n ' + result_unacceptable_values['error_message'] + '\\n')\n",
    "\n",
    "    # Combine all excluded indices\n",
    "    all_excluded_indices = list(set(blank_indices + unacceptable_indices))\n",
    "\n",
    "    # Check for excessive distinct values after filtering out unacceptable values\n",
    "    df_filtered_for_dqi10 = df[~df.index.isin(all_excluded_indices)]\n",
    "    result_excessive_values = DataQualityIssues.handle_excessive_distinct_values(df_filtered_for_dqi10, column, threshold)\n",
    "\n",
    "    if result_excessive_values['issue']:\n",
    "        error_summary_parts.append(result_excessive_values['dq_issue'] + ':\\n ' + result_excessive_values['error_message'] + '\\n')\n",
    "\n",
    "    # Display frequency distribution with a maximum of 20 entries (10 from the beginning and 10 from the end)\n",
    "    frequency_table = df[column].apply(DataQualityIssues.represent_value).value_counts().reset_index()\n",
    "    frequency_table.columns = ['Category', 'Frequency']\n",
    "    \n",
    "    # Sort first by Frequency (descending), then by Category (alphabetically)\n",
    "    frequency_table = frequency_table.sort_values(by=['Frequency', 'Category'], ascending=[False, True]).reset_index(drop=True)\n",
    "\n",
    "    # Select the first 10 and last 10 rows\n",
    "    top_rows = frequency_table.head(10)\n",
    "    bottom_rows = frequency_table.tail(10)\n",
    "    if len(frequency_table) > 20:\n",
    "        ellipsis_row = pd.DataFrame([(\"...\", \"...\")], columns=['Category', 'Frequency'])\n",
    "        display_table = pd.concat([top_rows, ellipsis_row, bottom_rows], ignore_index=True)\n",
    "    else:\n",
    "        display_table = frequency_table\n",
    "\n",
    "    frequency_distribution_str = f\"\\nCategorical format with {df[column].nunique(dropna=False)} unique value(s):\\n{display_table.to_string(index=False)}\"\n",
    "\n",
    "    # If no errors are found, return a message stating that all values are correct along with frequency distribution\n",
    "    if not error_summary_parts:\n",
    "        return f\"All {total_values_count} values are correctly categorical.\\n{frequency_distribution_str}\"\n",
    "\n",
    "    # Combine frequency distribution with error summary (if any)\n",
    "    error_summary = \"Error(s) found: \\n\" + \"\\n\".join(error_summary_parts) if error_summary_parts else \"\"\n",
    "    return error_summary + frequency_distribution_str\n",
    "\n",
    "# Example usage\n",
    "df_test = pd.DataFrame({'categorical_column': ['cat', 'dog', 'bird', '  ', 'Null', 100, 200, 'cat', 'dog', 'fish', 'fish', 'bird', '?', None, '', \"\", \"''\",'NA','N/A', \" \", \" ? \",'nonexistent']})\n",
    "result = check_categorical(df_test, 'categorical_column')\n",
    "print(result)\n",
    "\n",
    "from datetime import datetime\n",
    "print(f\"Last run on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13 Check Month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1984,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error(s) found: \n",
      "DQI #1 (Missing Data - Completeness):\n",
      " 2 Blank/Empty/Null/Missing value(s) at index(es): \n",
      "1 NULL values: [(11, 'NULL')]\n",
      "1 Non-NULL values: [(14, '%')]\n",
      "\n",
      "DQI #15 (Domain Violation - Accuracy):\n",
      " 4 Invalid month value(s) at index(es): [(3, '0'), (4, '13'), (10, 'not a month'), (12, 'mn')]\n",
      "\n",
      "Frequency Distribution:\n",
      "   Month  Frequency\n",
      " January          4\n",
      "February          2\n",
      "December          2\n",
      "   March          1\n",
      "November          1\n",
      "Last run on: 2025-04-04 19:28:37\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "def check_month(df, column):\n",
    "    error_summary_parts = []\n",
    "    month_counts = {}\n",
    "\n",
    "    # Calculate the total number of values in the column\n",
    "    total_values_count = df[column].size\n",
    "    \n",
    "    # First, handle Blank/Empty/Null/Missing values\n",
    "    result_blank = DataQualityIssues.handle_blank_empty_null_nan(df, column)\n",
    "\n",
    "    if result_blank['issue']:\n",
    "        error_summary_parts.append(result_blank['dq_issue'] + ':\\n ' + result_blank['error_message'] + '\\n')\n",
    "\n",
    "    # Handling invalid months\n",
    "    # Filtering df based on blank checks\n",
    "    blank_indices = list(result_blank[\"blank_indices\"])\n",
    "    df_filtered = df[~df.index.isin(blank_indices)]\n",
    "\n",
    "    result_invalid_months = DataQualityIssues.handle_invalid_months(df_filtered, column)\n",
    "\n",
    "    if result_invalid_months['issue']:\n",
    "        error_summary_parts.append(result_invalid_months['dq_issue'] + ':\\n ' + result_invalid_months['error_message'] + '\\n')\n",
    "\n",
    "    # Create frequency distribution for valid months\n",
    "    for val in df_filtered[column].dropna():\n",
    "        normalized_month = DataQualityIssues.normalize_month(str(val))\n",
    "        if normalized_month:\n",
    "            month_counts[normalized_month] = month_counts.get(normalized_month, 0) + 1\n",
    "\n",
    "    # Compile frequency distribution\n",
    "    frequency_table = pd.DataFrame([\n",
    "        (month, month_to_number(month), count) \n",
    "        for month, count in month_counts.items()\n",
    "    ], columns=['Month', 'MonthNum', 'Frequency'])\n",
    "    frequency_table = frequency_table.sort_values(by=['Frequency', 'MonthNum'], ascending=[False, True]).reset_index(drop=True)\n",
    "    frequency_distribution = f\"\\nFrequency Distribution:\\n{frequency_table[['Month', 'Frequency']].to_string(index=False)}\"\n",
    "\n",
    "    # If no errors are found, return a message stating that all values are valid along with frequency distribution\n",
    "    if not error_summary_parts:\n",
    "        return f\"All {total_values_count} month values are valid.\\n{frequency_distribution}\"\n",
    "\n",
    "    # Compile the final result message\n",
    "    error_summary = \"Error(s) found: \\n\" + \"\\n\".join(error_summary_parts) if error_summary_parts else \"\"\n",
    "    return error_summary + frequency_distribution\n",
    "\n",
    "def month_to_number(month_name):\n",
    "    \"\"\"\n",
    "    Convert a month name to its corresponding numeric value.\n",
    "\n",
    "    Parameters:\n",
    "    - month_name (str): The full name of the month.\n",
    "\n",
    "    Returns:\n",
    "    - int: Numeric representation of the month.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return datetime.strptime(month_name, '%B').month\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "# Example usage\n",
    "#df_test = pd.DataFrame({'month_column': [1, '3', 12, 'Jan', 'January', 'feb', 'NOV', 'Dec','FEBRUARY']})\n",
    "df_test = pd.DataFrame({'month_column': [1, '3', 12, '0', '13', 'Jan', 'January', 'feb', 'NOV', 'Dec', 'not a month', None, 'mn', 'FEBRUARY',\"%\", 1.0]})\n",
    "result = check_month(df_test, 'month_column')\n",
    "print(result)\n",
    "\n",
    "from datetime import datetime\n",
    "print(f\"Last run on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "14 Check Weekday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1985,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error(s) found: \n",
      "DQI #1 (Missing Data - Completeness):\n",
      " 2 Blank/Empty/Null/Missing value(s) at index(es): \n",
      "1 NULL values: [(7, 'NULL')]\n",
      "1 Non-NULL values: [(13, '$')]\n",
      "\n",
      "DQI #15 (Domain Violation - Accuracy):\n",
      " 3 Invalid weekday value(s) at index(es): [(6, 'not a weekday'), (11, -1), (12, 'Mn')]\n",
      "\n",
      "Frequency Distribution:\n",
      "  Weekday  Frequency\n",
      "   Monday          3\n",
      "Wednesday          2\n",
      "   Sunday          2\n",
      "  Tuesday          1\n",
      " Saturday          1\n",
      "Last run on: 2025-04-04 19:28:37\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def normalize_weekday(weekday_str):\n",
    "    \"\"\"\n",
    "    Normalize the given weekday string to its full weekday name if valid.\n",
    "\n",
    "    Parameters:\n",
    "    - weekday_str (str): The weekday string to normalize.\n",
    "\n",
    "    Returns:\n",
    "    - str/None: Normalized weekday name if valid, None otherwise.\n",
    "    \"\"\"\n",
    "    weekdays = [\"Sunday\", \"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\"]\n",
    "    weekday_abbr = [\"Sun\", \"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\"]\n",
    "    weekday_abbr_short = [\"su\", \"mo\", \"tu\", \"we\", \"th\", \"fr\", \"sa\"]\n",
    "\n",
    "    weekday_str_norm = weekday_str.lower()\n",
    "\n",
    "    # Normalize full names and abbreviations to full weekday name\n",
    "    if weekday_str_norm.title() in weekdays:\n",
    "        return weekday_str_norm.title()\n",
    "    elif weekday_str_norm.title() in weekday_abbr:\n",
    "        return weekdays[weekday_abbr.index(weekday_str_norm.title())]\n",
    "    elif weekday_str_norm in weekday_abbr_short:\n",
    "        return weekdays[weekday_abbr_short.index(weekday_str_norm)]\n",
    "    elif weekday_str_norm.isdigit():\n",
    "        # Convert numeric representation to weekday name\n",
    "        weekday_num = int(weekday_str_norm)\n",
    "        if 1 <= weekday_num <= 7:\n",
    "            return weekdays[weekday_num - 1]  # Adjusting for 1-indexed weekdays\n",
    "    return None\n",
    "\n",
    "def check_weekday(df, column):\n",
    "    error_summary_parts = []\n",
    "    weekday_counts = {}\n",
    "\n",
    "    # Calculate the total number of values in the column\n",
    "    total_values_count = df[column].size\n",
    "\n",
    "    # First, handle Blank/Empty/Null/Missing values\n",
    "    result_blank = DataQualityIssues.handle_blank_empty_null_nan(df, column)\n",
    "\n",
    "    if result_blank['issue']:\n",
    "        error_summary_parts.append(result_blank['dq_issue'] + ':\\n ' + result_blank['error_message'] + '\\n')\n",
    "\n",
    "    # Handling invalid weekdays\n",
    "    # Filtering df based on blank checks\n",
    "    blank_indices = list(result_blank[\"blank_indices\"])\n",
    "    df_filtered = df[~df.index.isin(blank_indices)]\n",
    "\n",
    "    result_invalid_weekdays = DataQualityIssues.handle_invalid_weekdays(df_filtered, column)\n",
    "\n",
    "    if result_invalid_weekdays['issue']:\n",
    "        error_summary_parts.append(result_invalid_weekdays['dq_issue'] + ':\\n ' + result_invalid_weekdays['error_message'] + '\\n')\n",
    "\n",
    "    # Create frequency distribution for valid weekdays\n",
    "    for val in df_filtered[column].dropna():\n",
    "        normalized_weekday = normalize_weekday(str(val))\n",
    "        if normalized_weekday:\n",
    "            weekday_counts[normalized_weekday] = weekday_counts.get(normalized_weekday, 0) + 1\n",
    "\n",
    "    # Compile frequency distribution\n",
    "    frequency_table = pd.DataFrame(weekday_counts.items(), columns=['Weekday', 'Frequency'])\n",
    "    frequency_table = frequency_table.sort_values(by='Frequency', ascending=False).reset_index(drop=True)\n",
    "    frequency_distribution = f\"\\nFrequency Distribution:\\n{frequency_table.to_string(index=False)}\"\n",
    "\n",
    "    # If no errors are found, return a message stating that all values are valid along with frequency distribution\n",
    "    if not error_summary_parts:\n",
    "        return f\"All {total_values_count} weekday values are valid.\\n{frequency_distribution}\"\n",
    "\n",
    "    # Compile the final result message\n",
    "    error_summary = \"Error(s) found: \\n\" + \"\\n\".join(error_summary_parts) if error_summary_parts else \"\"\n",
    "    return error_summary + frequency_distribution\n",
    "\n",
    "# Example usage\n",
    "df_test = pd.DataFrame({'weekday_column': ['Monday', 'Tue', 2, 'wed', 'Sunday', 'sun', 'not a weekday', None, 'Mo', 'WED', 7, -1, 'Mn',\"$\"]})\n",
    "#df_test = pd.DataFrame({'weekday_column': ['Monday', 'Tue', 2, 'wed', 'Sunday', 'sun', 'Mo', 'WED', 7, 0]})\n",
    "\n",
    "result = check_weekday(df_test, 'weekday_column')\n",
    "print(result)\n",
    "\n",
    "from datetime import datetime\n",
    "print(f\"Last run on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "15 Check Date "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1986,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking dates in column: date_column\n",
      "\n",
      "Deduced format: DDMMYYYY\n",
      "Processed date sample: 15 10 2021, detected format counts: {'DDMMYYYY': 9, 'MMDDYYYY': 0, 'YYYYMMDD': 1}\n",
      "\n",
      "Deduced date format: DDMMYYYY\n",
      "Outdated Date: Index 33, Date Object: 2121-04-03 00:00:00\n",
      "Outdated Date: Index 34, Date Object: 2222-05-14 00:00:00\n",
      "Outdated Date: Index 35, Date Object: 1500-01-01 00:00:00\n",
      "Outdated Date: Index 36, Date Object: 2321-12-31 00:00:00\n",
      "Outdated Date: Index 37, Date Object: 2121-12-25 00:00:00\n",
      "Outdated Date: Index 38, Date Object: 0301-01-01 00:00:00\n",
      "Outdated Date: Index 39, Date Object: 1719-02-01 00:00:00\n",
      "Error(s) found: \n",
      "DQI #1 (Missing Data - Completeness):\n",
      " 6 Blank/Empty/Null/Missing value(s) at index(es): \n",
      "3 NULL values: [(27, 'NULL'), (28, 'NULL'), (29, 'NULL')]\n",
      "3 Non-NULL values: [(24, 'Empty'), (25, 'Empty'), (30, '#')]\n",
      "\n",
      "DQI #6 (Outdated Temporal Data - Timeliness):\n",
      " 7 Date value(s) not in [1800-2100] period at index(es): [(33, '2121-04-03'), (34, '2222-05-14'), (35, '1500-01-01'), (36, '2321-12-31'), (37, '2121-12-25'), (38, '0301-01-01'), (39, '1719-02-01')]\n",
      "\n",
      "DQI #13 (Temporal mismatch - Accuracy, Timeliness):\n",
      " 8 Invalid date value(s) at index(es): [(17, '32/01/2021'), (18, '29/02/2021'), (19, '31/11/2021'), (20, '00/01/2021'), (21, '01/00/2021'), (22, '2021/13/01'), (23, 'not a date'), (26, '2021-02-30')]\n",
      "\n",
      "DQI #14 (Different units/representations - Consistency):\n",
      " 6 Date value(s) without format 'DDMMYYYY' in [1800-2100] period at index(es): [(3, '12/31/2021'), (4, '2021/12/25'), (11, '2021/04/7'), (12, '2021/08/15'), (13, '2021/11/03'), (14, '12/30/2021')]\n",
      "\n",
      "Date range: 0301-01-01 to 2321-12-31\n",
      "Last run on: 2025-04-04 19:28:37\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "min_valid_year = 1800\n",
    "max_valid_year = 2100\n",
    "\n",
    "def extract_indices_from_error_message(error_message):\n",
    "    # Regular expression to find tuples in the format (index, 'date')\n",
    "    return [int(m.group(1)) for m in re.finditer(r\"\\((\\d+), '.*?'\\)\", error_message)]\n",
    "\n",
    "def check_date(df, column, sample_size=10):\n",
    "    print(f\"Checking dates in column: {column}\")\n",
    "    total_values_count = df[column].size\n",
    "    error_summary_parts = []\n",
    "    problematic_indices = set()\n",
    "    valid_standardized_dates = []\n",
    "    outdated_entries = []  \n",
    "    earliest_date_str = \"N/A\"\n",
    "    latest_date_str = \"N/A\"\n",
    "\n",
    "    # Handle Blank/Empty/Null/Missing values\n",
    "    result_blank = DataQualityIssues.handle_blank_empty_null_nan(df, column)\n",
    "    if result_blank['issue']:\n",
    "        error_summary_parts.append(result_blank['dq_issue'] + ':\\n ' + result_blank['error_message'] + '\\n') \n",
    "        problematic_indices.update(result_blank[\"blank_indices\"])\n",
    "\n",
    "    df_filtered = df[~df.index.isin(problematic_indices)]\n",
    "\n",
    "    # Deduce regional format\n",
    "    date_samples = df_filtered[column].astype(str).str.strip()\n",
    "    date_samples = date_samples[:sample_size]\n",
    "    deduced_date_format, _ = DataQualityIssues.deduce_regional_format(date_samples)\n",
    "    print(f\"Deduced date format: {deduced_date_format}\")\n",
    "\n",
    "    deduced_strftime_format = DataQualityIssues.convert_to_strftime_format(deduced_date_format)\n",
    "    #print(f\"deduced_strftime_format: {deduced_strftime_format}\")\n",
    "\n",
    "    # Handling outdated temporal data\n",
    "    result_outdated = DataQualityIssues.handle_outdated_temporal_data(df_filtered, column, min_valid_year, max_valid_year)\n",
    "    if result_outdated['issue']:\n",
    "        error_summary_parts.append(result_outdated['dq_issue'] + ':\\n ' + result_outdated['error_message'] + '\\n')\n",
    "        outdated_indices = extract_indices_from_error_message(result_outdated['error_message'])\n",
    "        problematic_indices.update(outdated_indices)\n",
    "        # Capture outdated entries to be included in valid date range\n",
    "        outdated_entries = result_outdated.get('outdated_entries', [])\n",
    "\n",
    "    df_filtered = df[~df.index.isin(problematic_indices)]\n",
    "    #print(f\"Remaining rows after outdated check: {len(df_filtered)}\")\n",
    "  \n",
    "    # Handling invalid dates\n",
    "    result_invalid_dates = DataQualityIssues.handle_invalid_dates(df_filtered, column)\n",
    "    if result_invalid_dates['issue']:\n",
    "        error_summary_parts.append(result_invalid_dates['dq_issue'] + ':\\n ' + result_invalid_dates['error_message'] + '\\n')\n",
    "        invalid_indices = extract_indices_from_error_message(result_invalid_dates['error_message'])\n",
    "        problematic_indices.update(invalid_indices)\n",
    "\n",
    "    df_filtered = df[~df.index.isin(problematic_indices)]\n",
    "    #print(f\"Remaining rows after invalid dates check: {len(df_filtered)}\")\n",
    "\n",
    "    # Handling format issues for valid dates\n",
    "    result_format_issues = DataQualityIssues.handle_dates_format(df_filtered, column, deduced_date_format)\n",
    "    if result_format_issues['issue']:\n",
    "        error_summary_parts.append(result_format_issues['dq_issue'] + ':\\n ' + result_format_issues['error_message'] + '\\n')\n",
    "\n",
    "    df_filtered = df[~df.index.isin(problematic_indices)]\n",
    "    #print(f\"Remaining rows after format check: {len(df_filtered)}\")\n",
    " \n",
    "    # Iterate through the dataset to extract standardized dates\n",
    "    for idx, val in df[column].items():\n",
    "        standardized_date = DataQualityIssues.standardize_date(str(val), 'DDMMYYYY')\n",
    "        \n",
    "        if standardized_date and len(standardized_date) == 8 and standardized_date.isdigit():\n",
    "            try:\n",
    "                # Ensure year is parsed first for DDMMYYYY format and validate within a larger year range\n",
    "                if standardized_date[:4].isdigit() and 0000 <= int(standardized_date[:4]) <= 3000:\n",
    "                    year, month, day = int(standardized_date[:4]), int(standardized_date[4:6]), int(standardized_date[6:])\n",
    "                else:\n",
    "                    day, month, year = int(standardized_date[:2]), int(standardized_date[2:4]), int(standardized_date[4:])\n",
    "                \n",
    "                date_obj = datetime(year, month, day)\n",
    "                valid_standardized_dates.append(date_obj)\n",
    "                #print(f\"Index: {idx}, Original Value: {val}, Standardized Date: {standardized_date}, Date Object: {date_obj}\")\n",
    "\n",
    "            except ValueError as e:\n",
    "                #print(f\"Failed to convert standardized date: {standardized_date} at Index: {idx} - Error: {e}\")\n",
    "                continue\n",
    "\n",
    "    # Now, also include outdated dates in the valid date list\n",
    "    if outdated_entries:\n",
    "        for idx, val in outdated_entries:\n",
    "            standardized_date = DataQualityIssues.standardize_date(str(val), 'DDMMYYYY')\n",
    "            if standardized_date and len(standardized_date) == 8 and standardized_date.isdigit():\n",
    "                try:\n",
    "                    if 0000 <= int(standardized_date[:4]) <= 3000:\n",
    "                        year, month, day = int(standardized_date[:4]), int(standardized_date[4:6]), int(standardized_date[6:])\n",
    "                    else:\n",
    "                        day, month, year = int(standardized_date[:2]), int(standardized_date[2:4]), int(standardized_date[4:])\n",
    "                    date_obj = datetime(year, month, day)\n",
    "                    valid_standardized_dates.append(date_obj)\n",
    "                    print(f\"Outdated Date: Index {idx}, Date Object: {date_obj}\")\n",
    "                except ValueError:\n",
    "                    continue\n",
    "\n",
    "    # Handling the earliest and latest dates\n",
    "    try:\n",
    "        if valid_standardized_dates:\n",
    "            # Ensure all dates are considered, including outdated dates\n",
    "            all_dates = valid_standardized_dates + [date_obj for _, date_obj in outdated_entries]\n",
    "\n",
    "            # Convert all standardized dates to `datetime` objects, if they're not already\n",
    "            all_dates = [\n",
    "                datetime.strptime(str(date), \"%Y-%m-%d\") if isinstance(date, str) else date\n",
    "                for date in all_dates\n",
    "            ]\n",
    "            \n",
    "            # Now calculate the minimum and maximum dates\n",
    "            earliest_date = min(all_dates)\n",
    "            latest_date = max(all_dates)\n",
    "\n",
    "            # Display the earliest and latest dates\n",
    "            earliest_date_str = earliest_date.strftime('%Y-%m-%d')\n",
    "            latest_date_str = latest_date.strftime('%Y-%m-%d')\n",
    "            date_range_summary = f\"Date range: {earliest_date_str} to {latest_date_str}\"\n",
    "        else:\n",
    "            date_range_summary = \"No valid date values found.\"\n",
    "\n",
    "    except Exception as e:\n",
    "        #print(f\"Error converting dates: {e}\")\n",
    "        date_range_summary = \"Error processing date range.\"\n",
    "\n",
    "    # Create the date range summary\n",
    "    if earliest_date_str != \"N/A\" and latest_date_str != \"N/A\":\n",
    "        date_range_summary = f\"Date range: {earliest_date_str} to {latest_date_str}\"\n",
    "    else:\n",
    "        date_range_summary = \"No valid date range could be determined.\"\n",
    "\n",
    "    if error_summary_parts:\n",
    "        error_summary = \"Error(s) found: \\n\" + \"\\n\".join(error_summary_parts) + '\\n' + date_range_summary\n",
    "    else:\n",
    "        if earliest_date_str != \"N/A\" and latest_date_str != \"N/A\":\n",
    "            error_summary = f\"All {total_values_count} date values are valid in the {deduced_date_format} format in the range {earliest_date_str} to {latest_date_str}.\"\n",
    "        else:\n",
    "            error_summary = f\"All {total_values_count} date values are in the correct format, but no valid date range could be determined.\"\n",
    "\n",
    "    return error_summary\n",
    "\n",
    "# Example usage\n",
    "dates = [\n",
    "    '3/4/2021', '14/5/2021', '01/01/2021', '12/31/2021', '2021/12/25', # Valid dates\n",
    "    '14 January 2021', '1 Feb 2021', '28 Mar 2021', '4 Apr 2021', '15 Oct 2021', '23 Nov 2021', # Textual months\n",
    "    '2021/04/7', '2021/08/15', '2021/11/03', '12/30/2021', '07/04/2021', '11/11/2021', # Various formats\n",
    "    '32/01/2021', '29/02/2021', '31/11/2021', '00/01/2021', '01/00/2021', '2021/13/01', # Invalid dates\n",
    "    'not a date', '', ' ', '2021-02-30', np.nan, None, 'Null', \"#\", # Non-date and empty strings\n",
    "    '1 Feb 2021','1 February 2021',\n",
    "    '3/4/2121', '14/5/2222', '01/01/1500', '31/12/2321', '2121/12/25', '0301-01-01', '1 feb 1719'  # Dates before 1800 or greater than 2100\n",
    "]\n",
    "\n",
    "'''dates = [\n",
    "'2010-12-01','2010-12-02','2010-12-03','2010-12-14','2010-12-05',\n",
    "'2010-12-06','2010-12-07','2010-12-08','2010-12-09','2010-12-10',\n",
    "'2010-12-11','2010-12-12','2010-12-13','2010-12-14']'''\n",
    "\n",
    "'''dates = [\n",
    "    '01/01/2021', '02/01/2021', '03/01/2021', \n",
    "    '04/01/2021', '05/01/2021', '06/01/2021',\n",
    "    '07/01/2021', '08/01/2021', '09/01/2021', \n",
    "    '10/01/2021', '11/01/2021', '13/01/2021']'''\n",
    "   \n",
    "df_test_dates = pd.DataFrame({'date_column': dates})\n",
    "result = check_date(df_test_dates, 'date_column')\n",
    "print(result)\n",
    "\n",
    "print(f\"Last run on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "16 Check DateTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1987,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Deduced format: DDMMYYYY\n",
      "Processed date sample: 10/01/2021, detected format counts: {'DDMMYYYY': 10, 'MMDDYYYY': 0, 'YYYYMMDD': 0}\n",
      "\n",
      "Error(s) found: \n",
      "DQI #1 (Missing Data - Completeness):\n",
      " 6 Blank/Empty/Null/Missing value(s) at index(es): \n",
      "3 NULL values: [(27, 'NULL'), (28, 'NULL'), (29, 'NULL')]\n",
      "3 Non-NULL values: [(30, 'Empty'), (31, 'Empty'), (34, '@')]\n",
      "\n",
      "DQI #6 (Outdated Temporal Data - Timeliness):\n",
      " 5 Datetime value(s) not in [1800-2100] period at index(es): [(35, '3/4/2121 13:00'), (36, '14/5/2222 13:05'), (37, '01/01/1500 13:00:10'), (38, '31/12/2321 13:20'), (39, '2121/12/25 13:00:12')]\n",
      "\n",
      "DQI #14 (Different units/representations - Consistency):\n",
      " 4 Datetime value(s) without format 'DDMMYYYY' in [1800-2100] period at index(es): [(10, '2021-01-11 23:45'), (12, '2021/01/12 23:00'), (14, 'January 14, 2021 12:00'), (24, '2021/01/23')]\n",
      "\n",
      "DQI #13 (Temporal mismatch - Accuracy, Timeliness):\n",
      " 9 Invalid datetime value(s) at index(es): [(13, '13/01/2021 12:60'), (16, '2021/16/01 14:00'), (18, '18/01/2021 25:00'), (19, '2021-01-19T15:30'), (22, '21/01/2021 16:00:60'), (23, 'not a datetime'), (25, '24/01/2021 26:30'), (32, '29/02/2021 15:20'), (33, '2021-02-30 15:20:05')]\n",
      "\n",
      "Last run on: 2025-04-04 19:28:37\n"
     ]
    }
   ],
   "source": [
    "def extract_indices_from_error_message(error_message):\n",
    "    # Regular expression to find tuples in the format (index, 'value')\n",
    "    return [int(m.group(1)) for m in re.finditer(r\"\\((\\d+), '.*?'\\)\", error_message)]\n",
    "\n",
    "def update_format_counts(standardized_date, format_counts):\n",
    "    if len(standardized_date) == 8:\n",
    "        day, month, year = int(standardized_date[:2]), int(standardized_date[2:4]), int(standardized_date[4:])\n",
    "        if 1 <= day <= 31 and 1 <= month <= 12:\n",
    "            format_counts['DDMMYYYY'] += 1\n",
    "        if 1 <= month <= 12 and 1 <= day <= 31:\n",
    "            format_counts['MMDDYYYY'] += 1\n",
    "        year, month, day = int(standardized_date[:4]), int(standardized_date[4:6]), int(standardized_date[6:])\n",
    "        if min_valid_year <= year <= max_valid_year and 1 <= month <= 12 and 1 <= day <= 31:\n",
    "            format_counts['YYYYMMDD'] += 1\n",
    "    return format_counts\n",
    "\n",
    "def check_datetime(df, column, sample_size=10):\n",
    "    # Calculate the total number of values in the column\n",
    "    total_values_count = df[column].size\n",
    "    error_summary_parts = []\n",
    "\n",
    "    # Handle Blank/Empty/Null/Missing values\n",
    "    result_blank = DataQualityIssues.handle_blank_empty_null_nan(df, column)\n",
    "    if result_blank['issue']:\n",
    "        error_summary_parts.append(result_blank['dq_issue'] + ':\\n ' + result_blank['error_message'] + '\\n')\n",
    "    \n",
    "    # Filtering df based on blank checks\n",
    "    blank_indices = list(result_blank[\"blank_indices\"])\n",
    "    df_filtered = df[~df.index.isin(blank_indices)]\n",
    "\n",
    "    # Deduce regional format\n",
    "    datetime_samples = df_filtered[column].astype(str).str.strip()\n",
    "    date_samples = [sample.split(' ')[0] for sample in datetime_samples][:sample_size]\n",
    "    deduced_date_format, _ = DataQualityIssues.deduce_regional_format(date_samples)\n",
    "    deduced_strftime_format = DataQualityIssues.convert_to_strftime_format(deduced_date_format) + \" %H:%M\"\n",
    "    \n",
    "    # Handling outdated temporal datetimes\n",
    "    result_outdated = DataQualityIssues.handle_outdated_temporal_data_datetime(df_filtered, column, min_valid_year, max_valid_year)\n",
    "    if result_outdated['issue']:\n",
    "        error_summary_parts.append(result_outdated['dq_issue'] + ':\\n ' + result_outdated['error_message'] + '\\n')\n",
    "        outdated_indices = extract_indices_from_error_message(result_outdated['error_message'])\n",
    "        df_filtered = df_filtered[~df_filtered.index.isin(outdated_indices)]\n",
    "\n",
    "    # Handling format issues for valid dates\n",
    "    result_format_issues = DataQualityIssues.handle_datetimes_format(df_filtered, column, deduced_date_format)\n",
    "    if result_format_issues['issue']:\n",
    "        error_summary_parts.append(result_format_issues['dq_issue'] + ':\\n ' + result_format_issues['error_message'] + '\\n')\n",
    "        invalid_indices = extract_indices_from_error_message(result_format_issues['error_message'])\n",
    "        df_filtered = df_filtered[~df_filtered.index.isin(invalid_indices )]\n",
    "  \n",
    "    # Handle invalid datetime formats\n",
    "    result_invalid_formats = DataQualityIssues.handle_invalid_datetimes(df_filtered, column, deduced_date_format)\n",
    "    if result_invalid_formats['issue']:\n",
    "        error_summary_parts.append(result_invalid_formats['dq_issue'] + ':\\n ' + result_invalid_formats['error_message'] + '\\n')\n",
    "\n",
    "    # Find the earliest and latest datetime values\n",
    "    valid_datetimes = pd.to_datetime(df_filtered[column], errors='coerce', format=deduced_strftime_format).dropna()\n",
    "\n",
    "    earliest_datetime = valid_datetimes.min()\n",
    "    latest_datetime = valid_datetimes.max()\n",
    "\n",
    "    # Convert earliest and latest datetime to the deduced date format\n",
    "    datetime_range_summary = \"\"\n",
    "    if not valid_datetimes.empty:\n",
    "        earliest_datetime_str = earliest_datetime.strftime(deduced_strftime_format)\n",
    "        latest_datetime_str = latest_datetime.strftime(deduced_strftime_format)\n",
    "        datetime_range_summary = f\"Date range: {earliest_datetime_str} to {latest_datetime_str}\\n\"\n",
    "    else:\n",
    "        datetime_range_summary = \"No valid datetime values found.\\n\"\n",
    "\n",
    "   # Compile final result message\n",
    "    error_summary = \"Error(s) found: \\n\" + \"\\n\".join(error_summary_parts) if error_summary_parts else f\"All {total_values_count} datetime values are valid in the {deduced_date_format} format in the range {earliest_datetime_str} to {latest_datetime_str}.\"\n",
    "    return error_summary\n",
    "\n",
    "# Example usage\n",
    "timestamps = [\n",
    "    '01/01/2021 13:00', '02/01/2021 14:30', '03/01/2021 15:45', \n",
    "    '04/01/2021 16:00', '05/01/2021 17:15', '06/01/2021 18:30',\n",
    "    '07/01/2021 19:45', '08/01/2021 20:00', '09/01/2021 21:15', \n",
    "    '10/01/2021 22:30', '2021-01-11 23:45', '11-01-2021 10:00 PM', \n",
    "    '2021/01/12 23:00', '13/01/2021 12:60', 'January 14, 2021 12:00', \n",
    "    '15/01/2021', '2021/16/01 14:00', '17-01-2021', '18/01/2021 25:00', \n",
    "    '2021-01-19T15:30', '20th Jan 2021 16:00', '20 Jan 2021 16:00:10', '21/01/2021 16:00:60', \n",
    "    'not a datetime', '2021/01/23', '24/01/2021 26:30', '14 January 2021 12:00',\n",
    "    np.nan, None, 'Null', '', '  ', '29/02/2021 15:20','2021-02-30 15:20:05', '@',\n",
    "    '3/4/2121 13:00', '14/5/2222 13:05', '01/01/1500 13:00:10', '31/12/2321 13:20', \n",
    "    '2121/12/25 13:00:12' # Dates before 1800 or greater than 2100\n",
    "]\n",
    "\n",
    "'''timestamps = [\n",
    "    '01/01/2021 13:00', '02/01/2021 14:30', '03/01/2021 15:45', \n",
    "    '04/01/2021 16:00', '05/01/2021 17:15', '06/01/2021 18:30',\n",
    "    '07/01/2021 19:45', '08/01/2021 20:00', '09/01/2021 21:15', \n",
    "    '10/01/2021 22:30']'''\n",
    "\n",
    "'''timestamps = ['2010-12-01 08:26:00','2010-12-02 08:26:00','2010-12-03 08:28:00','2010-12-01 08:26:00','2010-12-01 08:26:45','2010-12-01 08:26:50',\n",
    "              '2010-12-01 08:27:00','2010-12-01 08:27:00','2010-12-01 08:26:15','2010-12-01 08:26:00','2010-12-01 08:26:05','2010-12-01 08:26:10',\n",
    "              '2010-12-01 08:26:20','2010-12-01 08:26:06']'''\n",
    "\n",
    "df_test_timestamps = pd.DataFrame({'timestamp_column': timestamps})\n",
    "result = check_datetime(df_test_timestamps, 'timestamp_column')\n",
    "print(result)\n",
    "\n",
    "from datetime import datetime\n",
    "print(f\"Last run on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "17 Check Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1988,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error(s) found: \n",
      "DQI #1 (Missing Data - Completeness):\n",
      " 3 Blank/Empty/Null/Missing value(s) at index(es): \n",
      "1 NULL values: [(7, 'NULL')]\n",
      "2 Non-NULL values: [(6, 'Empty'), (10, '*')]\n",
      "\n",
      "DQI #13 (Temporal mismatch - Accuracy, Timeliness):\n",
      " 4 Invalid time value(s) at index(es): [(1, '13:61'), (5, 'invalid'), (8, '02:30 PN'), (9, '25:03')]\n",
      "\n",
      "Time range: (03:05:00 to 19:41:00)\n",
      "\n",
      "Last run on: 2025-04-04 19:28:37\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def parse_timedelta(time_str):\n",
    "    match = re.match(r'(\\d+) days (\\d{2}):(\\d{2}):(\\d{2})', time_str)\n",
    "    if match:\n",
    "        days, hours, minutes, seconds = map(int, match.groups())\n",
    "        total_seconds = days * 86400 + hours * 3600 + minutes * 60 + seconds\n",
    "        return time(hour=total_seconds // 3600 % 24, \n",
    "                    minute=(total_seconds % 3600) // 60, \n",
    "                    second=total_seconds % 60)\n",
    "    return None\n",
    "\n",
    "def parse_time(time_str):\n",
    "    # If the input is already a time object, return it directly\n",
    "    if isinstance(time_str, time):\n",
    "        return time_str\n",
    "    \n",
    "    if pd.isna(time_str) or time_str == '':\n",
    "        return None\n",
    "    \n",
    "    time_str = str(time_str)\n",
    "\n",
    "    # Check for timedelta-like format\n",
    "    timedelta_time = parse_timedelta(time_str)\n",
    "    if timedelta_time:\n",
    "        return timedelta_time\n",
    "    \n",
    "    time_formats = ['%H:%M', '%I:%M %p', '%H:%M:%S', '%I:%M:%S %p']\n",
    "    for fmt in time_formats:\n",
    "        try:\n",
    "            return datetime.strptime(time_str, fmt).time()\n",
    "        except ValueError:\n",
    "            continue\n",
    "    return None  # Return None for invalid time formats\n",
    "\n",
    "        \n",
    "def check_time(df, column):\n",
    "    error_summary_parts = []\n",
    "    \n",
    "    # Calculate the total number of values in the column\n",
    "    total_values_count = df[column].size\n",
    "    \n",
    "    # Handle Blank/Empty/Null/Missing values\n",
    "    result_blank = DataQualityIssues.handle_blank_empty_null_nan(df, column)\n",
    "    if result_blank['issue']:\n",
    "        error_summary_parts.append(result_blank['dq_issue'] + ':\\n ' + result_blank['error_message'] + '\\n')\n",
    "    \n",
    "    # Filtering df based on blank checks\n",
    "    blank_indices = list(result_blank[\"blank_indices\"])\n",
    "    df_filtered = df[~df.index.isin(blank_indices)].copy()\n",
    "\n",
    "    # Handle invalid time formats\n",
    "    result_invalid_times = DataQualityIssues.handle_invalid_times(df_filtered, column)\n",
    "    if result_invalid_times['issue']:\n",
    "        error_summary_parts.append(result_invalid_times['dq_issue'] + ':\\n ' + result_invalid_times['error_message'] + '\\n')\n",
    "\n",
    "    # Convert time strings to datetime.time objects\n",
    "    #df_filtered['time_converted'] = df_filtered[column].apply(parse_time)\n",
    "    df_filtered.loc[:, 'time_converted'] = df_filtered[column].apply(parse_time)\n",
    "\n",
    "    valid_times = df_filtered['time_converted'].dropna()\n",
    "    earliest_time = min(valid_times, default=None)\n",
    "    latest_time = max(valid_times, default=None)\n",
    "\n",
    "    time_range_summary = \"\"\n",
    "    if earliest_time and latest_time:\n",
    "        time_range_summary = f\"\\nTime range: ({earliest_time} to {latest_time})\\n\"\n",
    "    else:\n",
    "        time_range_summary = \"No valid time values found.\\n\"\n",
    "\n",
    "    final_summary = \"\"\n",
    "    if error_summary_parts:\n",
    "        final_summary = \"Error(s) found: \\n\" + \"\\n\".join(error_summary_parts) + time_range_summary\n",
    "    else:\n",
    "        final_summary = f\"All {total_values_count} time values are valid in the range {earliest_time} to {latest_time}.\"\n",
    "\n",
    "    return final_summary\n",
    "\n",
    "# Example usage\n",
    "#df_test = pd.DataFrame({'time_column': ['12:30', '02:30 PM', '14:30:15', '03:05 AM']})\n",
    "df_test = pd.DataFrame({'time_column': ['12:30', '13:61', '02:30 PM', '14:30:15', '03:05 AM', 'invalid', '', None, '02:30 PN', '25:03',\"*\",'19:41:00']})\n",
    "\n",
    "result = check_time(df_test, 'time_column')\n",
    "print(result)\n",
    "\n",
    "from datetime import datetime\n",
    "print(f\"Last run on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "18 Check Model Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1989,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error(s) found: \n",
      "DQI #1 (Missing Data - Completeness):\n",
      " 4 Blank/Empty/Null/Missing value(s) at index(es): \n",
      "1 NULL values: [(6, 'NULL')]\n",
      "3 Non-NULL values: [(3, 'Empty'), (4, 'Empty'), (5, '?')]\n",
      "\n",
      "Frequency Distribution:\n",
      "model_name_column  Frequency\n",
      "                           1\n",
      "                           1\n",
      "               11          1\n",
      "            32/60          1\n",
      "           470v/7          1\n",
      "   50-850-iidn420          1\n",
      "         580-5840          1\n",
      "    90/80-model-3          1\n",
      "                ?          1\n",
      "             NULL          1\n",
      "           vs-100          1\n",
      "\n",
      "Range of Values: ( to None)\n",
      "Last run on: 2025-04-04 19:28:37\n"
     ]
    }
   ],
   "source": [
    "def check_model_name(df, column):\n",
    "    error_summary_parts = []\n",
    "    \n",
    "    total_values_count = df[column].size\n",
    "    result_blank = DataQualityIssues.handle_blank_empty_null_nan(df, column)\n",
    "    \n",
    "    # Filtering df based on blank checks\n",
    "    blank_indices = list(result_blank[\"blank_indices\"])\n",
    "    df_filtered = df[~df.index.isin(blank_indices)]\n",
    "\n",
    "    if result_blank['issue']:\n",
    "        error_summary_parts.append(result_blank['dq_issue'] + ':\\n ' + result_blank['error_message']+ '\\n')\n",
    "\n",
    "    error_summary = \"Error(s) found: \\n\" + \"\\n\".join(error_summary_parts) if error_summary_parts else \"\"\n",
    "\n",
    "    # Frequency distribution calculation\n",
    "    def represent_value(val):\n",
    "        return 'NULL' if pd.isna(val) else str(val)\n",
    "\n",
    "    model_counts = df[column].apply(represent_value).value_counts().to_dict()\n",
    "    frequency_table = pd.DataFrame(model_counts.items(), columns=[column, 'Frequency'])\n",
    "    frequency_table = frequency_table.sort_values(by=['Frequency', column], ascending=[False, True]).reset_index(drop=True)\n",
    "\n",
    "    if len(frequency_table) > 20:\n",
    "        top_rows = frequency_table.head(10)\n",
    "        bottom_rows = frequency_table.tail(10)\n",
    "        ellipsis_row = pd.DataFrame([(\"...\", \"...\")], columns=[column, 'Frequency'])\n",
    "        display_table = pd.concat([top_rows, ellipsis_row, bottom_rows], ignore_index=True)\n",
    "    else:\n",
    "        display_table = frequency_table\n",
    "\n",
    "    frequency_distribution = f\"\\nFrequency Distribution:\\n{display_table.to_string(index=False)}\\n\"\n",
    "\n",
    "    # Get the range of values (smallest and largest)\n",
    "    sorted_df = df.sort_values(by=column)\n",
    "    smallest_model = sorted_df.iloc[0][column]\n",
    "    biggest_model = sorted_df.iloc[-1][column]\n",
    "    range_of_values = f\"\\nRange of Values: ({smallest_model} to {biggest_model})\"\n",
    "\n",
    "    return error_summary + frequency_distribution + range_of_values if error_summary_parts else f\"All {total_values_count} {column} values are valid.\\n{frequency_distribution}{range_of_values}\"\n",
    "\n",
    "# Test the function with your dataframe\n",
    "df_test = pd.DataFrame({\n",
    "    'model_name_column': [\n",
    "        '32/60', '470v/7', 'vs-100', '', '  ', '?', None, \n",
    "        '90/80-model-3', '11',  '50-850-ii'\n",
    "        'dn420', '580-5840'\n",
    "    ]\n",
    "})\n",
    "result = check_model_name(df_test, 'model_name_column')\n",
    "print(result)\n",
    "\n",
    "print(f\"Last run on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "18.5 Check Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1990,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error(s) found: \n",
      "DQI #1 (Missing Data - Completeness):\n",
      " 5 Blank/Empty/Null/Missing value(s) at index(es): \n",
      "2 NULL values: [(22, 'NULL'), (23, 'NULL')]\n",
      "3 Non-NULL values: [(4, 'Empty'), (5, 'Empty'), (6, '?')]\n",
      "\n",
      "DQI #5 (Extraneous Data - Consistency, Uniqueness):\n",
      " 3 Extraneous data value(s) at index(es): [(7, 'John3 Doe'), (8, 'Emily!'), (9, '11')]\n",
      "\n",
      "DQI #15 (Domain Violation - Accuracy):\n",
      " 3 Capitalization/Format issue(s) at index(es): [(1, 'jane doe'), (18, 'ORiley'), (24, \"D'arcy O'Connor\")]\n",
      "\n",
      "Frequency Distribution:\n",
      "                              name_column Frequency\n",
      "                                     NULL         2\n",
      "                                                  1\n",
      "                                                  1\n",
      "                                       11         1\n",
      "                                        ?         1\n",
      "                               Anne-Marie         1\n",
      "                          D'arcy O'Connor         1\n",
      "                                   Emily!         1\n",
      "                    Jean Paul Gautier, Jr         1\n",
      "                                 John Doe         1\n",
      "                                      ...       ...\n",
      "José Augusto Napoleão Ferreira dos Santos         1\n",
      "         João Paulo Pereira e Souza Filho         1\n",
      "                                  Madonna         1\n",
      "                              Mary Joe MD         1\n",
      "                                 McDonald         1\n",
      "                                Mr. Smith         1\n",
      "                                   ORiley         1\n",
      "                                 jane doe         1\n",
      "                                 van Unen         1\n",
      "                                von Brown         1\n",
      "\n",
      "Range of Values: (   to von Brown)\n",
      "Last run on: 2025-04-04 19:28:37\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def check_name(df, column):\n",
    "    error_summary_parts = []\n",
    "    linking_words = {'the', 'and', 'of', 'do', 'da', 'de', 'del', 'dos', 'e', 'md', 'ii', 'iii', 'iv', 'v', 'jr', 'sr', 'phd','von', 'van','y'}  # Set of lowercase linking words and suffixes\n",
    "        \n",
    "    # Calculate the total number of values in the column\n",
    "    total_values_count = df[column].size\n",
    "\n",
    "    # First, handle Blank/Empty/Null/Missing values\n",
    "    result_blank = DataQualityIssues.handle_blank_empty_null_nan(df, column)\n",
    "\n",
    "    # Check if first 10 values are all lowercase\n",
    "    first_10_lowercase = df[column].head(10).str.islower().all()\n",
    "\n",
    "    # Filtering df based on blank checks\n",
    "    blank_indices = list(result_blank[\"blank_indices\"])\n",
    "    df_filtered = df[~df.index.isin(blank_indices)]\n",
    "    \n",
    "    # Handling special characters and extraneous data\n",
    "    result_extraneous = DataQualityIssues.handle_extraneous_data(df_filtered, column)\n",
    "\n",
    "    if result_blank['issue']:\n",
    "        error_summary_parts.append(result_blank['dq_issue'] + ':\\n ' + result_blank['error_message'] + '\\n')\n",
    "\n",
    "    indices_to_exclude = set()\n",
    "\n",
    "    if result_extraneous['issue']:\n",
    "        # Convert indices to integers (if necessary)\n",
    "        indices_to_exclude.update(result_extraneous.get('indices', []))\n",
    "        #print(\"indices_to_exclude\",indices_to_exclude)\n",
    "        error_summary_parts.append(result_extraneous['dq_issue'] + ':\\n ' + result_extraneous['error_message'] + '\\n')\n",
    "\n",
    "    if indices_to_exclude:\n",
    "        #print(f\"Indices to be excluded: {sorted(indices_to_exclude)}\")    \n",
    "        # Filter out rows with extraneous data\n",
    "        df_filtered = df_filtered.loc[~df_filtered.index.isin(indices_to_exclude)]    \n",
    " \n",
    "    # Perform capitalization format check only if not all first 10 names are lowercase\n",
    "    if not first_10_lowercase:\n",
    "        result_capitalization_format = DataQualityIssues.handle_capitalization_format(df_filtered, column, linking_words)\n",
    "        if result_capitalization_format['issue']:\n",
    "            error_summary_parts.append(result_capitalization_format['dq_issue'] + ':\\n ' + result_capitalization_format['error_message']+ '\\n')\n",
    "    \n",
    "    # Compile the final result message\n",
    "    error_summary = \"Error(s) found: \\n\" + \"\\n\".join(error_summary_parts) if error_summary_parts else \"\"\n",
    "\n",
    "    # Frequency distribution calculation\n",
    "    def represent_value(val):\n",
    "        return 'NULL' if pd.isna(val) else str(val)\n",
    "\n",
    "    name_counts = df[column].apply(represent_value).value_counts().to_dict()\n",
    "    frequency_table = pd.DataFrame(name_counts.items(), columns=[column, 'Frequency'])\n",
    "    frequency_table = frequency_table.sort_values(by=['Frequency', column], ascending=[False, True]).reset_index(drop=True)\n",
    "\n",
    "    if len(frequency_table) > 20:\n",
    "        top_rows = frequency_table.head(10)\n",
    "        bottom_rows = frequency_table.tail(10)\n",
    "        ellipsis_row = pd.DataFrame([(\"...\", \"...\")], columns=[column, 'Frequency'])\n",
    "        display_table = pd.concat([top_rows, ellipsis_row, bottom_rows], ignore_index=True)\n",
    "    else:\n",
    "        display_table = frequency_table\n",
    "\n",
    "    error_summary = \"Error(s) found: \\n\" + \"\\n\".join(error_summary_parts) if error_summary_parts else \"\"\n",
    "    frequency_distribution = f\"\\nFrequency Distribution:\\n{display_table.to_string(index=False)}\\n\"\n",
    "\n",
    "    # Get the range of values (smallest and largest)\n",
    "    '''sorted_df = df.sort_values(by=column)\n",
    "    smallest_name = sorted_df.iloc[0][column]\n",
    "    biggest_name = sorted_df.iloc[-1][column]\n",
    "    range_of_values = f\"\\nRange of Values: ({smallest_name} to {biggest_name})\"\n",
    "\n",
    "    return error_summary + frequency_distribution + range_of_values if error_summary_parts else f\"All {total_values_count} {column} values are valid.\\n{frequency_distribution}{range_of_values}\"'''\n",
    "\n",
    "    non_empty_values = df[column].dropna().replace('', np.nan).dropna()\n",
    "    if not non_empty_values.empty:\n",
    "        smallest_name = non_empty_values.min()\n",
    "        biggest_name = non_empty_values.max()\n",
    "        range_of_values = f\"\\nRange of Values: ({smallest_name} to {biggest_name})\"\n",
    "    else:\n",
    "        range_of_values = \"\\nNo valid values found for range calculation.\"\n",
    "\n",
    "    return error_summary + frequency_distribution + range_of_values if error_summary_parts else f\"All {total_values_count} {column} values are valid.\\n{frequency_distribution}{range_of_values}\"\n",
    "\n",
    "# Test the function\n",
    "df_test = pd.DataFrame({\n",
    "    'name_column': [\n",
    "        'John Doe', 'jane doe', 'Mr. Smith', 'Anne-Marie', '', '  ', '?',\n",
    "        'John3 Doe', 'Emily!', '11', 'Mary Joe MD', 'John Williams II', 'Madonna',\n",
    "        'Jean Paul Gautier, Jr', 'João Paulo Pereira e Souza Filho', 'José Augusto Napoleão Ferreira dos Santos', \n",
    "        'John F. Kennedy, Phd', 'John Newman, PhD', 'ORiley','McDonald', 'van Unen', 'von Brown', None, None, \"D'arcy O'Connor\"\n",
    "    ]\n",
    "})\n",
    "\n",
    "'''df_test = pd.DataFrame({\n",
    "    'name_column': [\n",
    "        'John Doe','Mr. Smith', 'Anne-Marie', 'Madonna',\n",
    "        'Jean Paul Gautier, Jr', 'João Paulo Pereira e Souza Filho', 'José Augusto Napoleão Ferreira dos Santos', \n",
    "        'John F. Kennedy, Phd', 'John Newman, PhD'\n",
    "    ]\n",
    "})'''\n",
    "result = check_name(df_test, 'name_column')\n",
    "print(result)\n",
    "\n",
    "from datetime import datetime\n",
    "print(f\"Last run on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "19 Check Street"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1991,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error(s) found: \n",
      "DQI #1 (Missing Data - Completeness):\n",
      " 4 Blank/Empty/Null/Missing value(s) at index(es): \n",
      "1 NULL values: [(0, 'NULL')]\n",
      "3 Non-NULL values: [(1, 'Empty'), (2, 'Empty'), (3, '?')]\n",
      "\n",
      "Frequency Distribution:\n",
      "         Street  Frequency\n",
      "          Empty          2\n",
      "              ?          1\n",
      "Berguvsvägen  8          1\n",
      "           NULL          1\n",
      "\n",
      "Range of Values: ( to None)\n",
      "Last run on: 2025-04-04 19:28:37\n"
     ]
    }
   ],
   "source": [
    "def check_street(df, column):\n",
    "    \"\"\"\n",
    "    Check if street names in the specified column conform to expected standards and provide a frequency distribution.\n",
    "    \n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): The pandas DataFrame to analyze.\n",
    "    - column (str): The name of the column with street names.\n",
    "    \n",
    "    Returns:\n",
    "    - str: A message indicating the result of the street name checks, including a frequency distribution.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Calculate the total number of values in the column\n",
    "    total_values_count = df[column].size\n",
    "    \n",
    "    error_summary_parts = []\n",
    "    linking_words = {'the', 'and', 'of', 'do', 'da', 'de', 'del', 'e', 'th','rd', 'rue', 'du', 'das', 'des'}  # Set of lowercase linking words\n",
    "    \n",
    "    # First, handle Blank/Empty/Null/Missing values\n",
    "    result_blank = DataQualityIssues.handle_blank_empty_null_nan(df, column)\n",
    "    \n",
    "    # Filtering df based on blank checks\n",
    "    blank_indices = set(result_blank[\"blank_indices\"])\n",
    "    df_filtered = df[~df.index.isin(blank_indices)]\n",
    "    \n",
    "    # Handling special characters and extraneous data\n",
    "    result_street_extraneous = DataQualityIssues.handle_street_extraneous_data(df_filtered, column)\n",
    "    \n",
    "    extraneous_indices = set(result_street_extraneous.get(\"indices\", []))\n",
    "    df_filtered = df_filtered[~df_filtered.index.isin(extraneous_indices)]\n",
    "    result_capitalization_format = DataQualityIssues.handle_capitalization_format(df_filtered, column, linking_words)\n",
    "    \n",
    "    if result_blank['issue']:\n",
    "        error_summary_parts.append(result_blank['dq_issue'] + ':\\n ' + result_blank['error_message'] + '\\n')\n",
    "    \n",
    "    if result_street_extraneous['issue']:\n",
    "        error_summary_parts.append(result_street_extraneous['dq_issue'] + ':\\n ' + result_street_extraneous['error_message'] + '\\n')\n",
    "    \n",
    "    if result_capitalization_format['issue']:\n",
    "        error_summary_parts.append(result_capitalization_format['dq_issue'] + ':\\n ' + result_capitalization_format['error_message']+ '\\n')\n",
    "    \n",
    "    # Frequency distribution calculation\n",
    "    street_counts = {}\n",
    "    valid_streets = []\n",
    "    for street in df[column]:\n",
    "        category = DataQualityIssues.represent_value(street)\n",
    "        if category == street:  # It's a valid street name\n",
    "            street_counts[street] = street_counts.get(street, 0) + 1\n",
    "            valid_streets.append(street)\n",
    "        else:  # It's a special category\n",
    "            street_counts[category] = street_counts.get(category, 0) + 1\n",
    "            \n",
    "    # Creating a frequency table sorted first by frequency and then alphabetically\n",
    "    if street_counts:\n",
    "        frequency_table = pd.DataFrame(street_counts.items(), columns=['Street', 'Frequency'])\n",
    "        frequency_table = frequency_table.sort_values(by=['Frequency', 'Street'], ascending=[False, True]).reset_index(drop=True)\n",
    "\n",
    "        # Select the first 10 and last 10 rows if more than 20 distinct items\n",
    "        if len(frequency_table) > 20:\n",
    "            top_rows = frequency_table.head(10)\n",
    "            bottom_rows = frequency_table.tail(10)\n",
    "            ellipsis_row = pd.DataFrame([(\"...\", \"...\")], columns=['Street', 'Frequency'])\n",
    "            display_table = pd.concat([top_rows, ellipsis_row, bottom_rows], ignore_index=True)\n",
    "            freq_dist_str = f\"Frequency Distribution (showing top and bottom 10 of {len(frequency_table)} streets):\\n{display_table.to_string(index=False)}\\n\"\n",
    "        else:\n",
    "            freq_dist_str = f\"Frequency Distribution:\\n{frequency_table.to_string(index=False)}\\n\"\n",
    "    else:\n",
    "        freq_dist_str = \"\\nNo valid street values found for frequency distribution.\"\n",
    "\n",
    "    # Compile the final result message\n",
    "    error_summary = ''\n",
    "    if error_summary_parts:\n",
    "        error_summary = \"Error(s) found: \\n\" + \"\\n\".join(error_summary_parts) + \"\\n\"\n",
    "\n",
    "    # Get the range of values\n",
    "    sorted_df = df.sort_values(by=column)\n",
    "    smallest_name = sorted_df.iloc[0][column]\n",
    "    biggest_name = sorted_df.iloc[-1][column]\n",
    "    range_str = f\"\\nRange of Values: ({smallest_name} to {biggest_name})\"\n",
    "\n",
    "    if error_summary_parts:\n",
    "        final_message = error_summary + freq_dist_str + range_str\n",
    "    else:\n",
    "        final_message = f\"All {total_values_count} street values are valid.\\n\" + freq_dist_str + range_str\n",
    "\n",
    "    return final_message\n",
    "\n",
    "# Sample data for street checks\n",
    "'''streets = [\n",
    "    '123 Main St', '45 Oxford Road', 'Broadway Ave', '5th Avenue', \n",
    "    'Mt. Everest Street', 'InvalidStreet', '12, Elm Street', '77 Sunset Strip', \n",
    "    '221B Baker Street', 'Elm St.', 'Ocean Drive', 'Park Ave', 'Sesame St', \n",
    "    'Main Street 123', 'Pennsylvania Avenue NW', 'Sunset boulevard', \n",
    "    'Abbey Road', 'Fleet Street', 'Diagon Alley', '15/250 Beaufort St',\n",
    "    None, '', '  ', '?', 'John3 Doe', 'Emily!', '11', 'R. Prof Paulo Roberto Martins, 2', 'Null',\n",
    "    \"27-30 Merchant's Quay\",\"27 30 Merchant's Quay\",\"27-30 Merchants Quay\", \"59 rue de l'Abbaye\", \"59 rue de Abbaye\",\n",
    "    '12, rue des Bouchers','Berguvsvägen  8', 'Jardim das rosas n. 32', 'Jardim das Rosas 32',\n",
    "    'c/ Gobelas, 19-1 Urb. La Florida', 'Gobelas, 19-1 Urb. La Florida','NatWest Center #13-03','Natwest Center 13-03'\n",
    "]'''\n",
    "\n",
    "'''streets = [\n",
    "    '123 Main St', '45 Oxford Road', 'Broadway Ave', '5th Avenue', \n",
    "    'Mt. Everest Street', '12, Elm Street', '77 Sunset Strip', \n",
    "    '221B Baker Street', 'Elm St.', 'Ocean Drive', 'Park Ave', 'Sesame St', \n",
    "    'Main Street 123', 'Pennsylvania Avenue NW', \n",
    "    'Abbey Road', 'Fleet Street', 'Diagon Alley', '15/250 Beaufort St',\n",
    "    'R. Prof Paulo Roberto Martins, 2'\n",
    "]'''\n",
    "streets = [None, '', '  ', '?', 'Berguvsvägen  8']\n",
    "df_streets = pd.DataFrame({'street': streets})\n",
    "result = check_street(df_streets, 'street')\n",
    "print(result)\n",
    "\n",
    "from datetime import datetime\n",
    "print(f\"Last run on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "20 Check City"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1992,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error(s) found: \n",
      "DQI #1 (Missing Data - Completeness):\n",
      " 5 Blank/Empty/Null/Missing value(s) at index(es): \n",
      "2 NULL values: [(22, 'NULL'), (28, 'NULL')]\n",
      "3 Non-NULL values: [(23, 'Empty'), (24, 'Empty'), (25, '?')]\n",
      "\n",
      "DQI #5 (Extraneous Data - Consistency, Uniqueness):\n",
      " 2 Extraneous data value(s) at index(es): [(26, 'Dubai!'), (27, 11)]\n",
      "\n",
      "DQI #15 (Domain Violation - Accuracy):\n",
      " 3 Capitalization/Format issue(s) at index(es): [(5, 'los angeles'), (9, 'new delhi'), (11, 'San francisco')]\n",
      "\n",
      "Frequency Distribution (showing top and bottom 10 of 27 cities):\n",
      "               City Frequency\n",
      "               NULL         2\n",
      "              Tokyo         2\n",
      "                            1\n",
      "                            1\n",
      "                 11         1\n",
      "                  ?         1\n",
      "          Amsterdam         1\n",
      "            Beijing         1\n",
      "             Berlin         1\n",
      "             Boston         1\n",
      "                ...       ...\n",
      "           New York         1\n",
      "              Paris         1\n",
      "      San Francisco         1\n",
      "      San francisco         1\n",
      "          Singapore         1\n",
      "             Sydney         1\n",
      "São José dos Campos         1\n",
      "          São Paulo         1\n",
      "        los angeles         1\n",
      "          new delhi         1\n",
      "\n",
      "Last run on: 2025-04-04 19:28:37\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def check_city(df, column):\n",
    "    \"\"\"\n",
    "    Check if city names in the specified column conform to expected standards and provide a comprehensive frequency distribution.\n",
    "    \"\"\"\n",
    "    total_values_count = df[column].size\n",
    "    city_counts = {}\n",
    "    linking_words = {'the', 'and', 'of', 'do', 'da', 'de', 'del', 'e', 'dos'}\n",
    "    error_summary_parts = []\n",
    "\n",
    "    # Frequency distribution calculation (including all values)\n",
    "    for city in df[column]:\n",
    "        if pd.isna(city):\n",
    "            if city is None:\n",
    "                city_str = 'NULL'\n",
    "            elif isinstance(city, float) and np.isnan(city):\n",
    "                city_str = '[NaN]'\n",
    "            else:\n",
    "                city_str = '[OTHER_NA]'\n",
    "        else:\n",
    "            city_str = str(city)  # Keep everything as is, including empty strings and spaces\n",
    "        city_counts[city_str] = city_counts.get(city_str, 0) + 1\n",
    "\n",
    "    # Error checking\n",
    "    result_blank = DataQualityIssues.handle_blank_empty_null_nan(df, column)\n",
    "    if result_blank['issue']:\n",
    "        error_summary_parts.append(result_blank['dq_issue'] + ':\\n ' + result_blank['error_message'] + '\\n')\n",
    "\n",
    "    df_filtered = df[~df.index.isin(result_blank[\"blank_indices\"])]\n",
    "\n",
    "    result_extraneous = DataQualityIssues.handle_extraneous_data(df_filtered, column)\n",
    "    if result_extraneous['issue']:\n",
    "        error_summary_parts.append(result_extraneous['dq_issue'] + ':\\n ' + result_extraneous['error_message'] + '\\n')\n",
    "\n",
    "    result_capitalization_format = DataQualityIssues.handle_capitalization_format(df_filtered, column, linking_words)\n",
    "    if result_capitalization_format['issue']:\n",
    "        error_summary_parts.append(result_capitalization_format['dq_issue'] + ':\\n ' + result_capitalization_format['error_message'] + '\\n')\n",
    "\n",
    "    # Creating a frequency table sorted by frequency (descending) and then alphabetically\n",
    "    frequency_table = pd.DataFrame(city_counts.items(), columns=['City', 'Frequency'])\n",
    "    frequency_table = frequency_table.sort_values(by=['Frequency', 'City'], ascending=[False, True]).reset_index(drop=True)\n",
    "\n",
    "    # Prepare the display table\n",
    "    if len(frequency_table) > 20:\n",
    "        top_rows = frequency_table.head(10)\n",
    "        bottom_rows = frequency_table.tail(10)\n",
    "        ellipsis_row = pd.DataFrame([(\"...\", \"...\")], columns=['City', 'Frequency'])\n",
    "        display_table = pd.concat([top_rows, ellipsis_row, bottom_rows], ignore_index=True)\n",
    "    else:\n",
    "        display_table = frequency_table\n",
    "\n",
    "    frequency_distribution = f\"\\nFrequency Distribution (showing top and bottom 10 of {len(frequency_table)} cities):\\n{display_table.to_string(index=False)}\\n\"\n",
    "\n",
    "    if not error_summary_parts:\n",
    "        return f\"All {total_values_count} city values are valid.\\n{frequency_distribution}\"\n",
    "\n",
    "    error_summary = \"Error(s) found: \\n\" + \"\\n\".join(error_summary_parts)\n",
    "    return error_summary + frequency_distribution\n",
    "\n",
    "# Sample data for city checks\n",
    "cities = [\n",
    "    'New York', 'London', 'Paris', 'Tokyo', 'Tokyo',\n",
    "    'los angeles', 'Sydney', 'Beijing', 'Cairo', 'new delhi', 'San Francisco', \n",
    "    'San francisco', 'Chicago', 'Boston', 'Berlin', 'Amsterdam', \n",
    "    'Hong Kong', 'Singapore', 'Dubai', 'Moscow', 'São Paulo','São José dos Campos',\n",
    "    None, '', '  ', '?', 'Dubai!',  11, None\n",
    "]\n",
    "\n",
    "'''cities = [\n",
    "    'New York', 'London', 'Paris', 'Tokyo', 'Tokyo',\n",
    "    'Sydney', 'Beijing', 'Cairo', 'San Francisco', \n",
    "    'Chicago', 'Boston', 'Berlin', 'Amsterdam', \n",
    "    'Hong Kong', 'Singapore', 'Dubai', 'Moscow', 'São Paulo','São José dos Campos',\n",
    "    'NY', 'Los Angeles', 'Cairo', 'Brasilia'\n",
    "]'''\n",
    "df_cities = pd.DataFrame({'city': cities})\n",
    "result = check_city(df_cities, 'city')\n",
    "print(result)\n",
    "\n",
    "from datetime import datetime\n",
    "print(f\"Last run on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "21 Check State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1993,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error(s) found: \n",
      "DQI #1 (Missing Data - Completeness):\n",
      " 5 Blank/Empty/Null/Missing value(s) at index(es): \n",
      "2 NULL values: [(22, 'NULL'), (34, 'NULL')]\n",
      "3 Non-NULL values: [(23, 'Empty'), (24, 'Empty'), (25, '?')]\n",
      "\n",
      "DQI #5 (Extraneous Data - Consistency, Uniqueness):\n",
      " 3 Extraneous data value(s) at index(es): [(0, 'CA2'), (26, 'California!'), (27, '11')]\n",
      "\n",
      "DQI #15 (Domain Violation - Accuracy):\n",
      " 4 Capitalization/Format issue(s) at index(es): [(10, 'New york'), (21, 'new south wales'), (29, 'new Jersey'), (30, 'n york')]\n",
      "\n",
      "\n",
      "Frequency Distribution (showing top and bottom 10 of 32 states/provinces):\n",
      "            State Frequency\n",
      "                          2\n",
      "             NULL         2\n",
      "            Texas         2\n",
      "               11         1\n",
      "                ?         1\n",
      "           Alaska         1\n",
      "          Arizona         1\n",
      "          Bavaria         1\n",
      "              CA2         1\n",
      "      California!         1\n",
      "              ...       ...\n",
      "          Ontario         1\n",
      "           Punjab         1\n",
      "       Queensland         1\n",
      "Rio Grande do Sul         1\n",
      "        São Paulo         1\n",
      "         Victoria         1\n",
      "               WA         1\n",
      "           n york         1\n",
      "       new Jersey         1\n",
      "  new south wales         1\n",
      "\n",
      "Last run on: 2025-04-04 19:28:37\n"
     ]
    }
   ],
   "source": [
    "def check_state(df, column):\n",
    "    \"\"\"\n",
    "    Check if state names in the specified column conform to expected standards of capitalization\n",
    "    and provide a frequency distribution, allowing certain lowercase words and abbreviations.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): The pandas DataFrame to analyze.\n",
    "    - column (str): The name of the column with state names.\n",
    "\n",
    "    Returns:\n",
    "    - str: A message indicating the result of the state name checks and their frequency distribution.\n",
    "    \"\"\"\n",
    "    incorrect_indices_and_values = []\n",
    "    state_counts = {}\n",
    "    lowercase_exceptions = {\"e\", \"do\", \"dos\", \"da\", \"das\", \"de\",\"dos\", \"of\", \"y\"}  # Lowercase exceptions\n",
    "    \n",
    "    # Calculate the total number of values in the column\n",
    "    total_values_count = df[column].size\n",
    "\n",
    "    # First, handle Blank/Empty/Null/Missing values\n",
    "    result_blank = DataQualityIssues.handle_blank_empty_null_nan(df, column)\n",
    "\n",
    "    # Filtering df based on blank checks\n",
    "    blank_indices = list(result_blank[\"blank_indices\"])\n",
    "    df_filtered = df[~df.index.isin(blank_indices)]\n",
    "\n",
    "    # Handling special characters and extraneous data\n",
    "    result_extraneous = DataQualityIssues.handle_extraneous_data(df_filtered, column)\n",
    "    result_capitalization_format = DataQualityIssues.handle_capitalization_format(df_filtered, column, lowercase_exceptions)\n",
    "\n",
    "    error_summary_parts = []\n",
    "\n",
    "    if result_blank['issue']:\n",
    "        error_summary_parts.append(result_blank['dq_issue'] + ':\\n ' + result_blank['error_message']+ '\\n')\n",
    "\n",
    "    if result_extraneous['issue']:\n",
    "        error_summary_parts.append(result_extraneous['dq_issue'] + ':\\n ' + result_extraneous['error_message']+ '\\n')\n",
    "\n",
    "    if result_capitalization_format['issue']:\n",
    "        error_summary_parts.append(result_capitalization_format['dq_issue'] + ':\\n ' + result_capitalization_format['error_message']+ '\\n')\n",
    "\n",
    "    # Frequency distribution calculation\n",
    "    for idx, state in df.iterrows():\n",
    "        state_value = state[column]\n",
    "        if pd.isna(state_value):\n",
    "            state_str = 'NULL'\n",
    "        else:\n",
    "            state_str = str(state_value).strip()\n",
    "        \n",
    "        # Counting occurrences of each state\n",
    "        state_counts[state_str] = state_counts.get(state_str, 0) + 1\n",
    "\n",
    "    # Creating a frequency table sorted first by frequency and then alphabetically\n",
    "    if state_counts:\n",
    "        frequency_table = pd.DataFrame(state_counts.items(), columns=['State', 'Frequency'])\n",
    "        frequency_table = frequency_table.sort_values(by=['Frequency', 'State'], ascending=[False, True]).reset_index(drop=True)\n",
    "\n",
    "        # Select the first 10 and last 10 rows if more than 20 distinct items\n",
    "        top_rows = frequency_table.head(10)\n",
    "        bottom_rows = frequency_table.tail(10)\n",
    "        if len(frequency_table) > 20:\n",
    "            ellipsis_row = pd.DataFrame([(\"...\", \"...\")], columns=['State', 'Frequency'])\n",
    "            display_table = pd.concat([top_rows, ellipsis_row, bottom_rows], ignore_index=True)\n",
    "        else:\n",
    "            display_table = frequency_table\n",
    "\n",
    "        result_str = f\"\\nFrequency Distribution (showing top and bottom 10 of {len(frequency_table)} states/provinces):\\n{display_table.to_string(index=False)}\\n\"   \n",
    "\n",
    "        # Compile the final result message\n",
    "        error_summary = ''\n",
    "        if error_summary_parts:\n",
    "            error_summary = \"Error(s) found: \\n\" + \"\\n\".join(error_summary_parts) + \"\\n\"\n",
    "\n",
    "        final_message = error_summary + result_str\n",
    "\n",
    "        # If no errors are found, return a message stating that all values are valid along with frequency distribution\n",
    "        if not error_summary_parts:\n",
    "            final_message = f\"All {total_values_count} state values are valid.\\n{result_str}\"\n",
    "\n",
    "        return final_message\n",
    "\n",
    "# Sample data for state checks\n",
    "states = [\n",
    "     'CA2', 'New York', 'Texas', 'FL', 'Texas',\n",
    "    'Nevada', 'WA', 'Queensland', 'Bavaria', 'Delhi', 'New york',\n",
    "    'Illinois', 'Victoria', 'Ontario', 'Colorado', 'Arizona', \n",
    "    'NSW', 'Gauteng', 'Hawaii', 'Alaska', 'Punjab', 'new south wales',\n",
    "    None, '', '  ', '?', 'California!', '11', 'Rio Grande do Sul', 'new Jersey', 'n york', 'N. Dakota', 'São Paulo','Isle of Wight', None\n",
    "]\n",
    "\n",
    "'''states = [\n",
    "     'CA', 'New York', 'Texas', 'FL', 'Texas',\n",
    "    'Nevada', 'WA', 'Queensland', 'Bavaria', 'Delhi', 'New York',\n",
    "    'Illinois', 'Victoria', 'Ontario', 'Colorado', 'Arizona', \n",
    "    'NSW', 'Gauteng', 'Hawaii', 'Alaska', 'Punjab', \n",
    "    'Rio Grande do Sul', 'N. Dakota', 'São Paulo'\n",
    "]'''\n",
    "\n",
    "df_states = pd.DataFrame({'state': states})\n",
    "result = check_state(df_states, 'state')\n",
    "print(result)\n",
    "\n",
    "from datetime import datetime\n",
    "print(f\"Last run on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "22 Check Country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1994,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error(s) found: \n",
      "DQI #1 (Missing Data - Completeness):\n",
      " 6 Blank/Empty/Null/Missing value(s) at index(es): \n",
      "3 NULL values: [(20, 'NULL'), (31, 'NULL'), (33, 'NULL')]\n",
      "3 Non-NULL values: [(21, 'Empty'), (22, 'Empty'), (23, '?')]\n",
      "\n",
      "DQI #5 (Extraneous Data - Consistency, Uniqueness):\n",
      " 2 Extraneous data value(s) at index(es): [(24, 'Canada!'), (25, '11')]\n",
      "\n",
      "DQI #15 (Domain Violation - Accuracy):\n",
      " 3 Capitalization/Format issue(s) at index(es): [(14, 'puerto rico'), (28, 'guatemala'), (29, 'papua New Guinea')]\n",
      "\n",
      "Frequency Distribution (showing top and bottom 10 of 30 countries):\n",
      "                         Country Frequency\n",
      "                       Australia         2\n",
      "                            NULL         2\n",
      "                              11         1\n",
      "                               ?         1\n",
      "                              BR         1\n",
      "                          Brazil         1\n",
      "                          Canada         1\n",
      "                         Canada!         1\n",
      "                           China         1\n",
      "                           Egypt         1\n",
      "                             ...       ...\n",
      "                             SWE         1\n",
      "Saint Vincent and the Grenadines         1\n",
      "                    South Africa         1\n",
      "    The United States of America         1\n",
      "                 US-Virgin-Isles         1\n",
      "                             USA         1\n",
      "                  United Kingdom         1\n",
      "                       guatemala         1\n",
      "                papua New Guinea         1\n",
      "                     puerto rico         1\n",
      "\n",
      "Last run on: 2025-04-04 19:28:37\n"
     ]
    }
   ],
   "source": [
    "def check_country(df, column):\n",
    "    \"\"\"\n",
    "    Check if country names in the specified column conform to expected standards of capitalization \n",
    "    and provide a frequency distribution.\n",
    "    \"\"\"\n",
    "    incorrect_indices_and_values = []\n",
    "    country_counts = {}\n",
    "    linking_words = {'the', 'and', 'of', 'do', 'da', 'de', 'del', 'e', 'dos','etc'}  # Set of lowercase linking words\n",
    "\n",
    "    # Calculate the total number of values in the column\n",
    "    total_values_count = df[column].size\n",
    "\n",
    "    # First, handle Blank/Empty/Null/Missing values\n",
    "    result_blank = DataQualityIssues.handle_blank_empty_null_nan(df, column)\n",
    "\n",
    "    # Filtering df based on blank checks\n",
    "    blank_indices = list(result_blank[\"blank_indices\"])\n",
    "    df_filtered = df[~df.index.isin(blank_indices)]\n",
    "\n",
    "    # Handling special characters and extraneous data\n",
    "    result_extraneous = DataQualityIssues.handle_extraneous_data(df_filtered, column)\n",
    "\n",
    "    if result_extraneous['issue']:\n",
    "        indices_to_exclude_extraneous = result_extraneous['indices']  # Directly use the indices\n",
    "        df_filtered = df_filtered.drop(index=indices_to_exclude_extraneous)\n",
    "\n",
    "    result_capitalization_format = DataQualityIssues.handle_capitalization_format_country(df_filtered, column, linking_words)\n",
    "\n",
    "    error_summary_parts = []\n",
    "\n",
    "    if result_blank['issue']:\n",
    "        error_summary_parts.append(result_blank['dq_issue'] + ':\\n ' + result_blank['error_message']+ '\\n')\n",
    "\n",
    "    if result_extraneous['issue']:\n",
    "        error_summary_parts.append(result_extraneous['dq_issue'] + ':\\n ' + result_extraneous['error_message']+ '\\n')\n",
    "\n",
    "    if result_capitalization_format['issue']:\n",
    "        error_summary_parts.append(result_capitalization_format['dq_issue'] + ':\\n ' + result_capitalization_format['error_message']+ '\\n')\n",
    "\n",
    "    # Frequency distribution calculation\n",
    "    for idx, country in df.iterrows():\n",
    "        country_value = country[column]\n",
    "        if pd.isna(country_value):\n",
    "            country_str = 'NULL'\n",
    "        else:\n",
    "            country_str = str(country_value).strip()\n",
    "        \n",
    "        # Skip Blank/Empty/Null/Missing values and incorrect values for frequency calculation\n",
    "        if country_str and country_str not in [item[1] for item in incorrect_indices_and_values]:\n",
    "            # Counting occurrences of each country\n",
    "            country_counts[country_str] = country_counts.get(country_str, 0) + 1\n",
    "\n",
    "    # Creating a frequency table sorted first by frequency and then alphabetically\n",
    "    if country_counts:\n",
    "        frequency_table = pd.DataFrame(country_counts.items(), columns=['Country', 'Frequency'])\n",
    "        frequency_table = frequency_table.sort_values(by=['Frequency', 'Country'], ascending=[False, True]).reset_index(drop=True)\n",
    "\n",
    "        # Select the first 10 and last 10 rows if more than 20 distinct items\n",
    "        top_rows = frequency_table.head(10)\n",
    "        bottom_rows = frequency_table.tail(10)\n",
    "        if len(frequency_table) > 20:\n",
    "            ellipsis_row = pd.DataFrame([(\"...\", \"...\")], columns=['Country', 'Frequency'])\n",
    "            display_table = pd.concat([top_rows, ellipsis_row, bottom_rows], ignore_index=True)\n",
    "            result_str = f\"Frequency Distribution (showing top and bottom 10 of {len(frequency_table)} countries):\\n{display_table.to_string(index=False)}\\n\"\n",
    "        else:\n",
    "            result_str = f\"Frequency Distribution:\\n{frequency_table.to_string(index=False)}\\n\"\n",
    "    else:\n",
    "        result_str = \"All country values are valid\"\n",
    "\n",
    "    # Compile the final result message\n",
    "    error_summary = ''\n",
    "    if error_summary_parts:\n",
    "        error_summary = \"Error(s) found: \\n\" + \"\\n\".join(error_summary_parts) + \"\\n\"\n",
    "\n",
    "    final_message = error_summary + result_str\n",
    "\n",
    "    # If no errors are found, return a message stating that all values are valid along with frequency distribution\n",
    "    if not error_summary_parts:\n",
    "        final_message = f\"All {total_values_count} country values are valid.\\n{result_str}\"\n",
    "\n",
    "    return final_message\n",
    "\n",
    "\n",
    "# Test function\n",
    "countries = [\n",
    "    'USA', 'United Kingdom', 'France', 'Japan', 'Australia', \n",
    "    'India', 'Australia', 'China', 'Egypt', 'Canada', \n",
    "    'Germany', 'Brazil', 'South Africa', 'Russia', 'puerto rico', \n",
    "    'ITA', 'SPA', 'BR', 'SWE', 'Papua New Guinea',\n",
    "    None, '', '  ', '?', 'Canada!', '11', 'The United States of America', 'Outlying-US (Guam-USVI-etc)',\n",
    "    'guatemala', 'papua New Guinea', 'Saint Vincent and the Grenadines', 'Null', 'US-Virgin-Isles', None\n",
    "]\n",
    "\n",
    "'''\"AnalysedColumns 2202.xlsx\"countries = [\n",
    "    'USA', 'United Kingdom', 'France', 'Japan', 'Australia', \n",
    "    'India', 'Australia', 'China', 'Egypt', 'Canada', \n",
    "    'Germany', 'Brazil', 'South Africa', 'Russia', 'Puerto Rico', \n",
    "    'ITA', 'SPA', 'BR', 'SWE', 'Papua New Guinea',\n",
    "    'Canada', 'The United States of America',\n",
    "    'Guatemala', 'Papua New Guinea', 'Saint Vincent and the Grenadines', 'US-Virgin-Isles','?'\n",
    "]'''\n",
    "\n",
    "df_countries = pd.DataFrame({'country': countries})\n",
    "result = check_country(df_countries, 'country')\n",
    "print(result)\n",
    "\n",
    "from datetime import datetime\n",
    "print(f\"Last run on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "23 Check Postal Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1995,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error(s) found: \n",
      "DQI #1 (Missing Data - Completeness):\n",
      " 4 Blank/Empty/Null/Missing value(s) at index(es): \n",
      "1 NULL values: [(21, 'NULL')]\n",
      "3 Non-NULL values: [(22, 'Empty'), (23, 'Empty'), (24, '?')]\n",
      "\n",
      "DQI #15 (Domain Violation - Accuracy):\n",
      " 1 Short length alphanumeric value(s) at index(es): [(26, '11')]\n",
      "\n",
      "DQI #17 (Wrong Data Type - Consistency):\n",
      " 1 Non-alphanumeric value(s) at index(es): [(25, '1000!')]\n",
      "\n",
      "Last run on: 2025-04-04 19:28:37\n"
     ]
    }
   ],
   "source": [
    "def check_postal_code(df: pd.DataFrame, column: str) -> dict:\n",
    "    \"\"\"\n",
    "    Check if postal code entries in the specified column are valid.\n",
    "    \"\"\"\n",
    "    # Calculate the total number of values in the column\n",
    "    total_values_count = df[column].size\n",
    "\n",
    "    # First, handle Blank/Empty/Null/Missing values\n",
    "    result_blank = DataQualityIssues.handle_blank_empty_null_nan(df, column)\n",
    "\n",
    "    # Filtering df based on blank checks\n",
    "    blank_indices = list(result_blank[\"blank_indices\"])\n",
    "    df_filtered = df[~df.index.isin(blank_indices)]\n",
    "    \n",
    "    result_non_alphanumeric = DataQualityIssues.handle_non_alphanumeric_values(df_filtered, column)\n",
    "    result_short_length = DataQualityIssues.handle_short_length_values(df_filtered, column, 4)\n",
    "\n",
    "    error_summary_parts = []\n",
    "\n",
    "    if result_blank['issue']:\n",
    "        error_summary_parts.append(result_blank['dq_issue'] + ':\\n ' + result_blank['error_message']+ '\\n')\n",
    "\n",
    "    if result_short_length['issue']:\n",
    "        error_summary_parts.append(result_short_length['dq_issue'] + ':\\n ' + result_short_length['error_message']+ '\\n')\n",
    "        \n",
    "    if result_non_alphanumeric['issue']:\n",
    "        error_summary_parts.append(result_non_alphanumeric['dq_issue'] + ':\\n ' + result_non_alphanumeric['error_message']+ '\\n')\n",
    " \n",
    "    error_summary = \"Error(s) found: \\n\" + \"\\n\".join(error_summary_parts)  \n",
    "    # Sort the DataFrame by the name column\n",
    "    sorted_df = df.sort_values(by=column)\n",
    "\n",
    "    # Get the first and last name after sorting\n",
    "    smallest_name = sorted_df.iloc[0][column]\n",
    "    biggest_name = sorted_df.iloc[-1][column]\n",
    " \n",
    "    return error_summary if error_summary_parts else f\"All {total_values_count} postal codes values are valid in the range ({smallest_name} to {biggest_name}).\\n\"\n",
    "\n",
    "# Sample data for postal code checks, including edge cases\n",
    "postal_codes = [\n",
    "    '10001', 'SW1A 1AA', '75008', '100-0001', \n",
    "    '110001', '2000', '100000', '11511', 'M4W 1A8', \n",
    "    '10115', '01311', '2001', '101000', '06500', \n",
    "    '00184', '28013', '1012 WX', '111 20', '0101', \n",
    "    '71676-110', '6000', None, '', '  ', '?', '1000!', '11'\n",
    "]\n",
    "\n",
    "'''postal_codes = [\n",
    "    '10001', 'SW1A 1AA', '75008', '100-0001', \n",
    "    '110001', '2000', '100000', '11511', 'M4W 1A8', \n",
    "    '10115', '01311', '2001', '101000', '06500', \n",
    "    '00184', '28013', '1012 WX', '111 20', '0101', \n",
    "    '71676-110', '6000'\n",
    "]'''\n",
    "\n",
    "df_postal_codes = pd.DataFrame({'postal_code': postal_codes})\n",
    "\n",
    "# Test the function\n",
    "result = check_postal_code(df_postal_codes, 'postal_code')\n",
    "print(result)\n",
    "\n",
    "from datetime import datetime\n",
    "print(f\"Last run on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "24 Check Phone Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1996,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error(s) found: \n",
      "DQI #1 (Missing Data - Completeness):\n",
      " 4 Blank/Empty/Null/Missing value(s) at index(es): \n",
      "1 NULL values: [(17, 'NULL')]\n",
      "3 Non-NULL values: [(18, 'Empty'), (19, 'Empty'), (20, '?')]\n",
      "\n",
      "DQI #15 (Domain Violation - Accuracy):\n",
      " 5 Incorrect telephone number format issue(s) at index(es): [(3, 'InvalidNumber'), (21, 'John Doe'), (22, '0405 833 952!'), (23, 11), (24, 'A191')]\n",
      "Last run on: 2025-04-04 19:28:37\n"
     ]
    }
   ],
   "source": [
    "def check_phone_numbers(df, column):\n",
    "    \"\"\"\n",
    "    Check if phone number entries in the specified column are valid.\n",
    "    \"\"\"\n",
    "    # Calculate the total number of values in the column\n",
    "    total_values_count = df[column].size\n",
    "\n",
    "    # First, handle Blank/Empty/Null/Missing values\n",
    "    result_blank = DataQualityIssues.handle_blank_empty_null_nan(df, column)\n",
    "\n",
    "    # Filtering df based on blank checks\n",
    "    blank_indices = list(result_blank[\"blank_indices\"])\n",
    "    df_filtered = df[~df.index.isin(blank_indices)]\n",
    "\n",
    "    result_format = DataQualityIssues.handle_phone_number_format(df_filtered, column)\n",
    "\n",
    "    error_summary_parts = []\n",
    "\n",
    "    if result_blank['issue']:\n",
    "        error_summary_parts.append(result_blank['dq_issue'] + ':\\n ' + result_blank['error_message']+ '\\n')\n",
    "\n",
    "    if result_format['issue']:\n",
    "        error_summary_parts.append(result_format['dq_issue'] + ':\\n ' + result_format['error_message'])\n",
    "      \n",
    "    error_summary = \"Error(s) found: \\n\" + \"\\n\".join(error_summary_parts)\n",
    "    \n",
    "    # Convert all entries to strings for sorting\n",
    "    df[column] = df[column].astype(str)\n",
    "    sorted_df = df.sort_values(by=column)\n",
    "\n",
    "    # Get the first and last name after sorting\n",
    "    smallest_name = sorted_df.iloc[0][column]\n",
    "    biggest_name = sorted_df.iloc[-1][column]\n",
    " \n",
    "    return error_summary if error_summary_parts else f\"All {total_values_count} telephone numbers are valid in the range ({smallest_name} to {biggest_name}).\\n\"\n",
    "    \n",
    "# Test data\n",
    "df_test = pd.DataFrame({\n",
    "    'phone_numbers': [\n",
    "        '123-456-7890', '(123) 456-7890', '+1 123 456 7890', 'InvalidNumber', \n",
    "        '+55 21 11 3415 1515', '04148991268624', '+55 48 3224-4209', '+55 48 91268-624', \n",
    "        '000', '+61137425', '+1 414-690-7935', '04121993720444', '01188335944', \n",
    "        '4144494331', '+55 31 3414-2179', '+61 405 833 952', '0405 833 952',\n",
    "        None, '', '  ', '?', 'John Doe', '0405 833 952!', 11, 'A191'\n",
    "    ]\n",
    "})\n",
    "\n",
    "'''df_test = pd.DataFrame({\n",
    "    'phone_numbers': [\n",
    "        '123-456-7890', '(123) 456-7890', '+1 123 456 7890', \n",
    "        '+55 21 11 3415 1515', '04148991268624', '+55 48 3224-4209', '+55 48 91268-624', \n",
    "        '000', '+61137425', '+1 414-690-7935', '04121993720444', '01188335944', \n",
    "        '4144494331', '+55 31 3414-2179', '+61 405 833 952', '0405 833 952',\n",
    "        '0405 833 952'\n",
    "    ]\n",
    "})'''\n",
    "\n",
    "result = check_phone_numbers(df_test, 'phone_numbers')\n",
    "print(result)\n",
    "\n",
    "from datetime import datetime\n",
    "print(f\"Last run on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "25 Check IP format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1997,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error(s) found: \n",
      "DQI #1 (Missing Data - Completeness):\n",
      " 4 Blank/Empty/Null/Missing value(s) at index(es): \n",
      "1 NULL values: [(8, 'NULL')]\n",
      "3 Non-NULL values: [(9, 'Empty'), (10, 'Empty'), (11, '?')]\n",
      "\n",
      "DQI #15 (Domain Violation - Accuracy):\n",
      " 7 Invalid IP format issue(s) at index(es): [(3, '1.1'), (5, '::1'), (6, '2001:db8::1234:5678'), (7, 'fe80::1ff:fe23:4567:890a'), (12, '0.0.0.0.0!'), (13, '11'), (14, 'incorrect:ipv6:address')]\n",
      "\n",
      "Last run on: 2025-04-04 19:28:37\n"
     ]
    }
   ],
   "source": [
    "def check_ip_format(df, column):\n",
    "\n",
    "    # Calculate the total number of values in the column\n",
    "    total_values_count = df[column].size\n",
    "\n",
    "    # Handle Blank/Empty/Null/Missing values first\n",
    "    result_blank = DataQualityIssues.handle_blank_empty_null_nan(df, column)\n",
    "    # Filtering df based on blank checks\n",
    "    blank_indices = list(result_blank[\"blank_indices\"])\n",
    "    df_filtered = df[~df.index.isin(blank_indices)]\n",
    "\n",
    "    # Then check IP format\n",
    "    result_ip_format = DataQualityIssues.handle_ip_format(df_filtered, column)\n",
    "\n",
    "    error_summary_parts = []\n",
    "\n",
    "    if result_blank['issue']:\n",
    "        error_summary_parts.append(result_blank['dq_issue'] + ':\\n ' + result_blank['error_message'] + '\\n')\n",
    "\n",
    "    if result_ip_format['issue']:\n",
    "        error_summary_parts.append(result_ip_format['dq_issue'] + ':\\n ' + result_ip_format['error_message'] + '\\n')\n",
    "\n",
    "    return \"Error(s) found: \\n\" + \"\\n\".join(error_summary_parts) if error_summary_parts else f\"All {total_values_count} IP values are valid.\"\n",
    "\n",
    "# Example usage\n",
    "df_test = pd.DataFrame({\n",
    "    'ip_column': [\n",
    "        '192.168.1.1', '256.256.256.256', '127.0.0.1', '1.1', \n",
    "        '2001:0db8:85a3:0000:0000:8a2e:0370:7334', '::1', \n",
    "        '2001:db8::1234:5678', 'fe80::1ff:fe23:4567:890a', \n",
    "        None, '', '  ', '?', '0.0.0.0.0!', '11', 'incorrect:ipv6:address'\n",
    "    ]\n",
    "})\n",
    "\n",
    "'''df_test = pd.DataFrame({\n",
    "    'ip_column': [\n",
    "        '192.168.1.1', '256.256.256.256', '127.0.0.1'\n",
    "    ]\n",
    "})'''\n",
    "\n",
    "result = check_ip_format(df_test, 'ip_column')\n",
    "print(result)\n",
    "\n",
    "from datetime import datetime\n",
    "print(f\"Last run on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "26 Check URL format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1998,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error(s) found: \n",
      "DQI #1 (Missing Data - Completeness):\n",
      " 4 Blank/Empty/Null/Missing value(s) at index(es): \n",
      "2 NULL values: [(17, 'NULL'), (18, 'NULL')]\n",
      "2 Non-NULL values: [(15, 'Empty'), (16, 'Empty')]\n",
      "\n",
      "DQI #15 (Domain Violation - Accuracy):\n",
      " 11 Invalid URL format issue(s) at index(es): [(5, 'ftp://example.com'), (6, 'http://exa_mple.com'), (8, 'https://'), (9, 'http://'), (10, 'http://exam!ple.com'), (11, 'https://www.example..com'), (12, 'http:// example .com'), (13, 'justsometext'), (14, '12345'), (20, 'https:--www.uol.com.br'), (22, 'https:///www.uol.com.br')]\n",
      "\n",
      "Frequency Distribution (showing top and bottom 10 of 21 entries):\n",
      "                                                           URL Frequency\n",
      "                                                         Empty         2\n",
      "                                                          NULL         2\n",
      "                                                         12345         1\n",
      "                                             ftp://example.com         1\n",
      "                                                       http://         1\n",
      "                                          http:// example .com         1\n",
      "                                            http://192.168.1.1         1\n",
      "                                        http://999.999.999.999         1\n",
      "                                           http://exa_mple.com         1\n",
      "                                           http://exam!ple.com         1\n",
      "                                                           ...       ...\n",
      "                                         http://localhost/test         1\n",
      "                                        https:--www.uol.com.br         1\n",
      "                                                      https://         1\n",
      "                                       https:///www.uol.com.br         1\n",
      "https://chat.openai.com/c/9c317ba2-cefe-44b9-b9f4-7ef818744434         1\n",
      "                                      https://www.example..com         1\n",
      "                                       https://www.example.com         1\n",
      "                 https://www.example.com:8080/path/to/resource         1\n",
      "                                        https://www.uol.com.br         1\n",
      "                                                  justsometext         1\n",
      "\n",
      "Last run on: 2025-04-04 19:28:37\n"
     ]
    }
   ],
   "source": [
    "def check_url_format(df, column):\n",
    "    # Calculate the total number of values in the column\n",
    "    total_values_count = df[column].size\n",
    "    \n",
    "    # Handle Blank/Empty/Null/Missing values first\n",
    "    result_blank = DataQualityIssues.handle_blank_empty_null_nan(df, column)\n",
    "    # Filtering df based on blank checks\n",
    "    blank_indices = list(result_blank[\"blank_indices\"])\n",
    "    df_filtered = df[~df.index.isin(blank_indices)]\n",
    "\n",
    "    # Then check URL format\n",
    "    result_url_format = DataQualityIssues.handle_url_format(df_filtered, column)\n",
    "\n",
    "    error_summary_parts = []\n",
    "\n",
    "    if result_blank['issue']:\n",
    "        error_summary_parts.append(result_blank['dq_issue'] + ':\\n ' + result_blank['error_message'] + '\\n')\n",
    "\n",
    "    if result_url_format['issue']:\n",
    "        error_summary_parts.append(result_url_format['dq_issue'] + ':\\n ' + result_url_format['error_message'] + '\\n')\n",
    "\n",
    "    # Frequency distribution calculation\n",
    "    url_counts = {}\n",
    "    for url in df[column]:\n",
    "        category = DataQualityIssues.represent_value(url)\n",
    "        if category == url:  # It's a valid URL\n",
    "            url_counts[url] = url_counts.get(url, 0) + 1\n",
    "        else:  # It's a special category\n",
    "            url_counts[category] = url_counts.get(category, 0) + 1\n",
    "\n",
    "    # Creating a frequency table\n",
    "    if url_counts:\n",
    "        frequency_table = pd.DataFrame(url_counts.items(), columns=['URL', 'Frequency'])\n",
    "        frequency_table = frequency_table.sort_values(by=['Frequency', 'URL'], ascending=[False, True]).reset_index(drop=True)\n",
    "\n",
    "        if len(frequency_table) > 20:\n",
    "            top_rows = frequency_table.head(10)\n",
    "            bottom_rows = frequency_table.tail(10)\n",
    "            ellipsis_row = pd.DataFrame([(\"...\", \"...\")], columns=['URL', 'Frequency'])\n",
    "            display_table = pd.concat([top_rows, ellipsis_row, bottom_rows], ignore_index=True)\n",
    "            freq_dist_str = f\"\\nFrequency Distribution (showing top and bottom 10 of {len(frequency_table)} entries):\\n{display_table.to_string(index=False)}\\n\"\n",
    "        else:\n",
    "            freq_dist_str = f\"\\nFrequency Distribution:\\n{frequency_table.to_string(index=False)}\\n\"\n",
    "    else:\n",
    "        freq_dist_str = \"\\nNo URL values found for frequency distribution.\"\n",
    "\n",
    "    # Compile the final result message\n",
    "    error_summary = \"Error(s) found: \\n\" + \"\\n\".join(error_summary_parts) if error_summary_parts else \"\"\n",
    "\n",
    "    final_message = (error_summary + freq_dist_str) if error_summary_parts else \\\n",
    "                    f\"All {total_values_count} URL values are valid.\\n\" + freq_dist_str\n",
    "\n",
    "    return final_message\n",
    "test_urls = [\n",
    "    'https://www.example.com', 'http://example.org', 'http://192.168.1.1', 'http://localhost/test',\n",
    "    'https://www.example.com:8080/path/to/resource', 'ftp://example.com', 'http://exa_mple.com',\n",
    "    'http://999.999.999.999', 'https://', 'http://', 'http://exam!ple.com', 'https://www.example..com',\n",
    "    'http:// example .com', 'justsometext', '12345', '', '  ', None, 'null',\n",
    "    'https://chat.openai.com/c/9c317ba2-cefe-44b9-b9f4-7ef818744434',\n",
    "    'https:--www.uol.com.br', 'https://www.uol.com.br', 'https:///www.uol.com.br'\n",
    "]\n",
    "\n",
    "'''test_urls = [\n",
    "    'https://www.example.com', 'http://example.org', 'http://192.168.1.1', 'http://localhost/test',\n",
    "    'https://www.example.com:8080/path/to/resource', 'https://www.uol.com.br'\n",
    "]'''\n",
    "\n",
    "# Example usage\n",
    "df_test_urls = pd.DataFrame({'url_column': test_urls})\n",
    "result = check_url_format(df_test_urls, 'url_column')\n",
    "print(result)\n",
    "\n",
    "from datetime import datetime\n",
    "print(f\"Last run on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "27 Check Email Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1999,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error(s) found: \n",
      "DQI #1 (Missing Data - Completeness):\n",
      " 5 Blank/Empty/Null/Missing value(s) at index(es): \n",
      "2 NULL values: [(28, 'NULL'), (29, 'NULL')]\n",
      "3 Non-NULL values: [(26, 'Empty'), (27, 'Empty'), (30, '%')]\n",
      "\n",
      "DQI #15 (Domain Violation - Accuracy):\n",
      " 24 Invalid email format issue(s) at index(es): [(0, 'example.com'), (1, 'userexample.com'), (2, 'name.domain.com'), (3, 'user@.com'), (4, 'name@'), (5, 'user name@example.com'), (6, 'user@ exam ple.com'), (7, 'user @example.com'), (9, 'name#domain.com'), (11, 'user@@example.com'), ('...', '...'), (16, 'name@domain.'), (17, 'user..name@example.com'), (18, 'user@domain..com'), (19, 'user@-example.com'), (20, 'user@domain--name.com'), (21, 'user@[192.168.0.1]'), (22, 'name@[123.123.123.123]'), (23, 'user[name]@example.com'), (24, 'name[123]@domain.com'), (25, 'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa@example.com')] (displaying only the first and last 10 items)\n",
      "\n",
      "Last run on: 2025-04-04 19:28:37\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def check_email_format(df, column):\n",
    "    # Calculate the total number of values in the column\n",
    "    total_values_count = df[column].size\n",
    "\n",
    "    # Handle Blank/Empty/Null/Missing values first\n",
    "    result_blank = DataQualityIssues.handle_blank_empty_null_nan(df, column)\n",
    "\n",
    "    # Filtering df based on blank checks\n",
    "    blank_indices = list(result_blank[\"blank_indices\"])\n",
    "    df_filtered = df[~df.index.isin(blank_indices)]\n",
    "\n",
    "    # Then check email format\n",
    "    result_email_format = DataQualityIssues.handle_email_format(df_filtered, column)\n",
    "\n",
    "    error_summary_parts = []\n",
    "\n",
    "    if result_blank['issue']:\n",
    "        error_summary_parts.append(result_blank['dq_issue'] + ':\\n ' + result_blank['error_message'] + '\\n')\n",
    "\n",
    "    if result_email_format['issue']:\n",
    "        error_summary_parts.append(result_email_format['dq_issue'] + ':\\n ' + result_email_format['error_message'] + '\\n')\n",
    "\n",
    "    return \"Error(s) found: \\n\" + \"\\n\".join(error_summary_parts) if error_summary_parts else f\"All {total_values_count} email values are valid.\"\n",
    "\n",
    "# Example usage\n",
    "\n",
    "df_test = pd.DataFrame({\n",
    "    'email_column': [\n",
    "        'example.com', 'userexample.com', 'name.domain.com',  # Missing @ Symbol\n",
    "        'user@.com', 'name@',  # Missing Domain\n",
    "        'user name@example.com', 'user@ exam ple.com', 'user @example.com',  # Spaces in Email Address\n",
    "        'user!@example.com', 'name#domain.com', 'user*name@example.com',  # Special Characters\n",
    "        'user@@example.com', 'name@domain@domain.com',  # Multiple @ Symbols\n",
    "        '@example.com', '@domain.com',  # Missing Username\n",
    "        'user@example.c', 'name@domain.',  # Domain Extension Too Short or Missing\n",
    "        'user..name@example.com', 'user@domain..com',  # Consecutive Dots\n",
    "        'user@-example.com', 'user@domain--name.com',  # Dashes in Domain\n",
    "        'user@[192.168.0.1]', 'name@[123.123.123.123]',  # IP Address in Domain\n",
    "        'user[name]@example.com', 'name[123]@domain.com',  # Brackets in Local Part\n",
    "        'a'*255 + '@example.com',  # Too Long Email Address\n",
    "        '', '  ', None, 'null', '%'\n",
    "    ]\n",
    "})\n",
    "\n",
    "'''df_test = pd.DataFrame({\n",
    "    'email_column': [\n",
    "        'marcelo.valentimsilva@postgrad.curtin.edu.au', 'marcelovalentimsilva@gmail.com', 'marcelo_valentim@uol.com.br'\n",
    "    ]\n",
    "})'''\n",
    "\n",
    "result = check_email_format(df_test, 'email_column')\n",
    "print(result)\n",
    "\n",
    "from datetime import datetime\n",
    "print(f\"Last run on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "28 Check Binary Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2000,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error(s) found: \n",
      "DQI #1 (Missing Data - Completeness):\n",
      " 6 Blank/Empty/Null/Missing value(s) at index(es): \n",
      "1 NULL values: [(13, 'NULL')]\n",
      "5 Non-NULL values: [(14, 'Empty'), (15, '?'), (16, 'Empty'), (21, 'Empty'), (22, 'Empty')]\n",
      "\n",
      "DQI #15 (Domain Violation - Accuracy):\n",
      " 5 Non-binary value(s) at index(es): [(8, 'Invalid'), (9, '2'), (12, 3), (18, 0.1), (19, '-2')]\n",
      "\n",
      "Frequency Distribution (showing top and bottom 10 of 19 binary values):\n",
      "  Value  Frequency\n",
      "      0          2\n",
      "[EMPTY]          2\n",
      "[SPACE]          2\n",
      "     no          2\n",
      "     -2          1\n",
      "    0.1          1\n",
      "      1          1\n",
      "      2          1\n",
      "      3          1\n",
      "      ?          1\n",
      "  False          1\n",
      "Invalid          1\n",
      "   NULL          1\n",
      "      T          1\n",
      "      Y          1\n",
      "    YES          1\n",
      "    Yes          1\n",
      "   true          1\n",
      "      y          1\n",
      "\n",
      "Last run on: 2025-04-04 19:28:37\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "\n",
    "def check_binary_values(df, column):\n",
    "    \"\"\"\n",
    "    Check if the values in the specified column conform to binary values and provide a comprehensive frequency distribution.\n",
    "    \"\"\"\n",
    "    total_values_count = df[column].size\n",
    "    binary_counts = OrderedDict()\n",
    "    error_summary_parts = []\n",
    "\n",
    "    # Frequency distribution calculation (including all values)\n",
    "    for value in df[column]:\n",
    "        if pd.isna(value):\n",
    "            if value is None:\n",
    "                value_str = 'NULL'\n",
    "            elif isinstance(value, float) and np.isnan(value):\n",
    "                value_str = '[NaN]'\n",
    "            else:\n",
    "                value_str = '[OTHER_NA]'\n",
    "        elif value == '':\n",
    "            value_str = '[EMPTY]'\n",
    "        elif isinstance(value, str) and value.isspace():\n",
    "            value_str = '[SPACE]'\n",
    "        elif isinstance(value, (int, float)):\n",
    "            value_str = str(value)  # Preserve numeric values as strings\n",
    "        else:\n",
    "            value_str = str(value)  # Preserve original case for strings\n",
    "        binary_counts[value_str] = binary_counts.get(value_str, 0) + 1\n",
    "\n",
    "    # Error checking\n",
    "    result_blank = DataQualityIssues.handle_blank_empty_null_nan(df, column)\n",
    "    if result_blank['issue']:\n",
    "        error_summary_parts.append(result_blank['dq_issue'] + ':\\n ' + result_blank['error_message'] + '\\n')\n",
    "\n",
    "    df_filtered = df[~df.index.isin(result_blank[\"blank_indices\"])]\n",
    "\n",
    "    result_binary_format = DataQualityIssues.handle_binary_values(df_filtered, column)\n",
    "    if result_binary_format['issue']:\n",
    "        error_summary_parts.append(result_binary_format['dq_issue'] + ':\\n ' + result_binary_format['error_message'] + '\\n')\n",
    "\n",
    "    # Creating a frequency table sorted by frequency (descending) and then alphabetically\n",
    "    frequency_table = pd.DataFrame({'Value': list(binary_counts.keys()), 'Frequency': list(binary_counts.values())})\n",
    "    frequency_table = frequency_table.sort_values(by=['Frequency', 'Value'], ascending=[False, True]).reset_index(drop=True)\n",
    "\n",
    "    # Prepare the display table\n",
    "    total_categories = len(frequency_table)\n",
    "    if total_categories > 20:\n",
    "        top_rows = frequency_table.head(10)\n",
    "        bottom_rows = frequency_table.tail(10)\n",
    "        ellipsis_row = pd.DataFrame([(\"...\", \"...\")], columns=['Value', 'Frequency'])\n",
    "        display_table = pd.concat([top_rows, ellipsis_row, bottom_rows], ignore_index=True)\n",
    "    else:\n",
    "        display_table = frequency_table\n",
    "\n",
    "    frequency_distribution = f\"\\nFrequency Distribution (showing top and bottom 10 of {total_categories} binary values):\\n{display_table.to_string(index=False)}\\n\"\n",
    "\n",
    "    if not error_summary_parts:\n",
    "        return f\"All {total_values_count} binary values are valid.\\n{frequency_distribution}\"\n",
    "\n",
    "    error_summary = \"Error(s) found: \\n\" + \"\\n\".join(error_summary_parts)\n",
    "    return error_summary + frequency_distribution\n",
    "\n",
    "# Example usage\n",
    "df_test = pd.DataFrame({\n",
    "    'binary_column': ['1', '0', 'Yes', 'YES', 'y', 'no', 'true', 'False', 'Invalid', '2', 'Y', 0, 3, None, '', '?', '   ', \"T\", 0.1, '-2', 'no', '','  ',]\n",
    "})\n",
    "\n",
    "'''df_test = pd.DataFrame({\n",
    "    'binary_column': ['1', '0', 'Yes', 'YES', 'y', 'no', 'true', 'False', 'Y', 0, \"T\", 'no']\n",
    "})'''\n",
    "\n",
    "result = check_binary_values(df_test, 'binary_column')\n",
    "print(result)\n",
    "\n",
    "from datetime import datetime\n",
    "print(f\"Last run on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "29 Parse Analysis Result and Generate Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2001,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last run on: 2025-04-04 19:28:37\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import ast\n",
    "\n",
    "# Define a dictionary to map function names to check numbers\n",
    "check_numbers = {\n",
    "    \"check_numerical_ge_zero\": 1,\n",
    "    \"check_numerical\": 2,\n",
    "    \"check_numerical_between\": 3,\n",
    "    \"check_id_attributes\": 4,\n",
    "    \"check_string\": 5,\n",
    "    \"check_categorical\": 6,\n",
    "    \"check_month\": 7,\n",
    "    \"check_weekday\": 8,\n",
    "    \"check_date\": 9,\n",
    "    \"check_datetime\": 10,\n",
    "    \"check_time\": 11,\n",
    "    \"check_model_name\": 11.5,\n",
    "    \"check_name\": 12,\n",
    "    \"check_street\": 13,\n",
    "    \"check_city\": 14,\n",
    "    \"check_state\": 15,\n",
    "    \"check_country\": 16,\n",
    "    \"check_postal_code\": 17,\n",
    "    \"check_phone_numbers\": 18,\n",
    "    \"check_ip_format\": 19,\n",
    "    \"check_url_format\": 20,\n",
    "    \"check_email_format\": 21,\n",
    "    \"check_binary_values\": 22\n",
    "}\n",
    "\n",
    "def parse_analysis_result(column, result, format_type, dataset_name, total_rows):\n",
    "    summary_lines = []\n",
    "    \n",
    "    numerical_range_checks = ('year','age','percentage', 'ph', 'latitude', 'longitude', 'normalized','week','day',\n",
    "                              'hour', 'tannins', 'saltness','heartrate','acidity','alkalinity','bloodpressure')\n",
    "\n",
    "    if isinstance(result, dict):\n",
    "        for key, value in result.items():\n",
    "            dqi_matches = re.findall(r'DQI #(\\d+) \\((.*?)\\):', value)\n",
    "\n",
    "            if dqi_matches:  # Only process if DQIs are found\n",
    "                for dqi_num, dq_issue_desc in dqi_matches:\n",
    "                    # Separate DQ Issue Description from Data Quality Dimension\n",
    "                    dq_parts = dq_issue_desc.split(' - ')\n",
    "                    dq_issue_desc = dq_parts[0].strip()\n",
    "                    data_quality_dim = dq_parts[1].strip() \n",
    "\n",
    "                    # Use regex with DOTALL flag to capture multiline content                 \n",
    "                    issue_count_match = re.search(rf'DQI #{dqi_num}.*?(\\d+)\\s+(.*?)\\s+(?:at index|value\\(s\\))', value, re.DOTALL)\n",
    "                    if issue_count_match:\n",
    "                        issue_count = int(issue_count_match.group(1))\n",
    "                        error_desc = issue_count_match.group(2)\n",
    "                    else:\n",
    "                        issue_count = 0\n",
    "                        error_desc = \"See full analysis for details\"\n",
    "\n",
    "                    # Calculate error percentage\n",
    "                    error_percentage = (issue_count / total_rows) * 100 if total_rows > 0 else \"N/A\"\n",
    "\n",
    "                    if dqi_num == '1':\n",
    "                        # Extract NULL values explicitly (handles optional \"All\" prefix)\n",
    "                        null_values_match = re.search(r'(?:All )?NULL values:\\s*(\\[[^\\]]*\\])', value, re.DOTALL)\n",
    "\n",
    "                        # Extract Non-NULL values explicitly\n",
    "                        non_null_values_match = re.search(r'Non-NULL values:\\s*(\\[[^\\]]*\\])', value, re.DOTALL)\n",
    "\n",
    "                        unique_values = set()\n",
    "\n",
    "                        try:\n",
    "                            if null_values_match:\n",
    "                                null_data = ast.literal_eval(null_values_match.group(1))\n",
    "                                for _, val in null_data:\n",
    "                                    val_cleaned = val.strip(\"'\\\"\") if isinstance(val, str) else val\n",
    "                                    if val_cleaned and val_cleaned != '...':\n",
    "                                        unique_values.add(f'\"{val_cleaned}\"')\n",
    "\n",
    "                            if non_null_values_match:\n",
    "                                non_null_data = ast.literal_eval(non_null_values_match.group(1))\n",
    "                                for _, val in non_null_data:\n",
    "                                    val_cleaned = val.strip(\"'\\\"\") if isinstance(val, str) else val\n",
    "                                    if val_cleaned and val_cleaned != '...':\n",
    "                                        unique_values.add(f'\"{val_cleaned}\"')\n",
    "\n",
    "                            # If still no values, explicitly handle the scenario\n",
    "                            if not unique_values:\n",
    "                                data_issues = \"No specific values extracted\"\n",
    "                            else:\n",
    "                                unique_values_sorted = sorted(unique_values)\n",
    "                                data_issues = ', '.join(unique_values_sorted[:3])\n",
    "                                if len(unique_values_sorted) > 3:\n",
    "                                    data_issues += \", ...\"\n",
    "                        except Exception as e:\n",
    "                            data_issues = f\"Error parsing issue data: {str(e)}\"\n",
    "\n",
    "                    # Special handling for categorical data\n",
    "                    elif dqi_num == '10':\n",
    "                        categorical_match = re.search(r'Categorical format with (\\d+) unique value\\(s\\):', value)\n",
    "                        if categorical_match:\n",
    "                            issue_count = int(categorical_match.group(1))\n",
    "                            error_desc = \"Too many categories\"\n",
    "                        data_issues = f\"{issue_count} categories\"\n",
    "                        error_percentage = \"N/A\"\n",
    "                        issue_count = 'N/A'\n",
    "\n",
    "                    elif dqi_num == '9':  # Duplicates case\n",
    "                     \n",
    "                        # Try different patterns to extract duplicate values\n",
    "                        patterns = [\n",
    "                            r'\\((\\d+),\\s*\\'([a-fA-F0-9]{32})\\'\\)',  # Hash pattern\n",
    "                            r'\\[(\\d+),\\s*(\\d+(?:\\.\\d+)?)\\]',  # Original pattern\n",
    "                            r'(\\d+),\\s*(\\d+(?:\\.\\d+)?)',      # Without square brackets\n",
    "                            r\"'(\\d+(?:\\.\\d+)?)'\",             # Values in single quotes\n",
    "                            r'\"(\\d+(?:\\.\\d+)?)\"',             # Values in double quotes\n",
    "                            r'\\b(\\d+(?:\\.\\d+)?)\\b'            # Any numeric value\n",
    "                        ]\n",
    "                        \n",
    "                        duplicate_values = []\n",
    "                        for pattern in patterns:\n",
    "                            duplicate_values = re.findall(pattern, value)\n",
    "                        \n",
    "                        if duplicate_values:\n",
    "                            # If we found values with index, use the second item (the actual value)\n",
    "                            if isinstance(duplicate_values[0], tuple) and len(duplicate_values[0]) > 1:\n",
    "                                value_counts = Counter(v[1] for v in duplicate_values)\n",
    "                            else:\n",
    "                                value_counts = Counter(v for v in duplicate_values)\n",
    "                            \n",
    "                            # Convert to int if possible, otherwise keep as float                          \n",
    "                            def convert_to_int_if_possible(v):\n",
    "                                try:\n",
    "                                    return int(float(v))\n",
    "                                except ValueError:\n",
    "                                    try:\n",
    "                                        return float(v)\n",
    "                                    except ValueError:\n",
    "                                        return v  # Return the original string if it can't be converted\n",
    "                            \n",
    "                            \n",
    "                            # Get all unique values, convert to int if possible, and sort\n",
    "                            unique_values = sorted(set(convert_to_int_if_possible(v) for v in value_counts.keys()))\n",
    "                            \n",
    "                            # Format the output\n",
    "                            if len(unique_values) <= 3:\n",
    "                                data_issues = ', '.join(str(v) for v in unique_values)\n",
    "                            else:\n",
    "                                data_issues = ', '.join(str(v) for v in unique_values[:3]) + '...'\n",
    "                        else:\n",
    "                            data_issues = \"Unable to extract duplicate values\"\n",
    "\n",
    "                    elif dqi_num == '19':  # Uniqueness violation case\n",
    "                     \n",
    "                        # Try different patterns to extract violation values\n",
    "                        patterns = [\n",
    "                            r'\\((\\d+),\\s*\\'([a-fA-F0-9]{32})\\'\\)',  # Hash pattern\n",
    "                            r'\\[(\\d+),\\s*(\\d+(?:\\.\\d+)?)\\]',  # Original pattern\n",
    "                            r'(\\d+),\\s*(\\d+(?:\\.\\d+)?)',      # Without square brackets\n",
    "                            r\"'(\\d+(?:\\.\\d+)?)'\",             # Values in single quotes\n",
    "                            r'\"(\\d+(?:\\.\\d+)?)\"',             # Values in double quotes\n",
    "                            r'\\b(\\d+(?:\\.\\d+)?)\\b'            # Any numeric value\n",
    "                        ]\n",
    "                        \n",
    "                        violation_values = []\n",
    "                        for pattern in patterns:\n",
    "                            violation_values = re.findall(pattern, value)\n",
    "\n",
    "                        \n",
    "                        if violation_values:\n",
    "                            # If we found values with index, use the second item (the actual value)\n",
    "                            if isinstance(violation_values[0], tuple) and len(violation_values[0]) > 1:\n",
    "                                values = [v[1] for v in violation_values]\n",
    "                            else:\n",
    "                                values = violation_values\n",
    "                                                      \n",
    "                            # Convert to int if possible, otherwise keep as float\n",
    "                            def convert_to_int_if_possible(v):\n",
    "                                try:\n",
    "                                    return int(float(v))\n",
    "                                except ValueError:\n",
    "                                    try:\n",
    "                                        return float(v)\n",
    "                                    except ValueError:\n",
    "                                        return v  # Return the original string if it can't be converted\n",
    "                            \n",
    "                            # Get all unique values, convert to int if possible, and sort\n",
    "                            unique_values = sorted(set(convert_to_int_if_possible(v) for v in values))\n",
    "                            \n",
    "                            # Format the output\n",
    "                            if len(unique_values) <= 3:\n",
    "                                data_issues = ', '.join(str(v) for v in unique_values)\n",
    "                            else:\n",
    "                                data_issues = ', '.join(str(v) for v in unique_values[:3]) + '...'\n",
    "\n",
    "                    elif dqi_num == '15' and 'name' in format_type.lower():\n",
    "                        # Special handling for DQI #15 related to names\n",
    "                        data_issues_match = re.search(rf'DQI #{dqi_num}.*?at index\\(es\\): \\[(.*?)\\]', value, re.DOTALL)\n",
    "                        if data_issues_match:\n",
    "                            data_issues = data_issues_match.group(1)\n",
    "                            # Remove indices and other unnecessary characters, but keep apostrophes\n",
    "                            data_issues = re.sub(r'\\(\\d+,\\s*|\\)', '', data_issues)\n",
    "                            # Split by comma, strip whitespace, and remove outer quotes, but keep apostrophes within words\n",
    "                            unique_issues = [re.sub(r'^[\"\\']|[\"\\']$', '', issue.strip()) for issue in data_issues.split(',') if issue.strip()]\n",
    "                            # Remove duplicates while preserving order\n",
    "                            unique_issues = list(dict.fromkeys(unique_issues))\n",
    "                            \n",
    "                            # Format the output\n",
    "                            if len(unique_issues) <= 3:\n",
    "                                data_issues = ', '.join(unique_issues)\n",
    "                            else:\n",
    "                                data_issues = ', '.join(unique_issues[:3]) + '...'\n",
    "                        else:\n",
    "                            data_issues = \"N/A\"\n",
    "\n",
    "                    elif dqi_num == '6':  # Outdated Temporal Data\n",
    "                       \n",
    "                        issue_data = re.findall(r'(\\d{4})-(\\d{2})-(\\d{2})', value)\n",
    "                        unique_values = set()\n",
    "\n",
    "                        for year, month, day in issue_data:\n",
    "                            date_value = f'{int(day):02d}-{int(month):02d}-{int(year):04d}'\n",
    "                            unique_values.add(date_value)\n",
    "\n",
    "                        if len(unique_values) > 3:\n",
    "                            data_issues = ', '.join(list(unique_values)[:3]) + \", ...\"\n",
    "                        else:\n",
    "                            data_issues = ', '.join(unique_values)\n",
    "\n",
    "                        error_desc = \"Date value(s) not in [1800-2100] period\"\n",
    "                        # Ensure the data is passed correctly to the next step\n",
    "                        add_quotes_to_items(data_issues)\n",
    "                    else:\n",
    "                        data_issues_match = re.search(rf'DQI #{dqi_num}.*?(?:at index\\(es\\): \\[(.*?)\\]|Sample values: \\[(.*?)\\])', value, re.DOTALL)\n",
    "                        if data_issues_match:\n",
    "                            data_issues = data_issues_match.group(1) or data_issues_match.group(2)\n",
    "                            # Remove indices, quotes, parentheses, ellipsis, and leading/trailing commas\n",
    "                            data_issues = re.sub(r'\\(\\d+,\\s*|\\)|\\(|\\'|\\\"|\\.\\.\\.|^,\\s*|,\\s*$', '', data_issues)\n",
    "                            \n",
    "                            # Split, clean, and get unique values\n",
    "                            unique_issues = list(set(issue.strip() for issue in data_issues.split(',') if issue.strip()))\n",
    "                            \n",
    "                            # Sort the unique issues to ensure consistent output\n",
    "                            unique_issues.sort()\n",
    "                            \n",
    "                            # Format the output\n",
    "                            if len(unique_issues) <= 3:\n",
    "                                data_issues = ', '.join(unique_issues)\n",
    "                            else:\n",
    "                                data_issues = ', '.join(unique_issues[:3]) + '...'\n",
    "                        else:\n",
    "                            data_issues = \"N/A\"\n",
    "\n",
    "                    # Determine the correct function name based on format_type\n",
    "                    if format_type.lower() == 'categorical':\n",
    "                        function_name = \"check_categorical\"\n",
    "                    elif format_type.lower() == 'idcolumn':\n",
    "                        function_name = \"check_id_attributes\"\n",
    "                    elif format_type.lower() == 'numerical>=0':\n",
    "                        function_name = \"check_numerical_ge_zero\"\n",
    "                    elif format_type.lower() == 'modelname':\n",
    "                        function_name = \"check_model_name\"\n",
    "                    elif format_type.lower() == 'postalcode':\n",
    "                        function_name = \"check_postal_code\"\n",
    "                    elif format_type.lower() == 'phone':\n",
    "                        function_name = \"check_phone_numbers\"\n",
    "                    elif format_type.lower() == 'ipformat':\n",
    "                        function_name = \"check_ip_format\"\n",
    "                    elif format_type.lower() == 'urlformat':\n",
    "                        function_name = \"check_url_format\"\n",
    "                    elif format_type.lower() == 'e-mailformat':\n",
    "                        function_name = \"check_email_format\"\n",
    "                    elif format_type.lower() == 'binary':\n",
    "                        function_name = \"check_binary_values\"\n",
    "                    elif format_type.lower() in numerical_range_checks:\n",
    "                        function_name = \"check_numerical_between\"\n",
    "                    else:\n",
    "                        function_name = f\"check_{format_type.lower().replace(' ', '_')}\"\n",
    "                    \n",
    "                    check_number = check_numbers.get(function_name, \"N/A\")\n",
    "                     \n",
    "                    if dqi_num == '10': \n",
    "                        data_issues_final = data_issues\n",
    "                    else: \n",
    "                        data_issues_final = add_quotes_to_items(data_issues)\n",
    "\n",
    "                    summary_line = {\n",
    "                        \"DQI#\": dqi_num,\n",
    "                        \"DQ Issue Description\": dq_issue_desc,\n",
    "                        \"Data Quality Dimension\": data_quality_dim,\n",
    "                        \"Function Name\": function_name,\n",
    "                        \"Check#\": int(check_number) if str(check_number).isdigit() else check_number,\n",
    "                        \"Format Being Analyzed\": format_type,\n",
    "                        \"Error Count\": issue_count,\n",
    "                        \"Error Percentage\": f\"{error_percentage:.2f}%\" if isinstance(error_percentage, float) else error_percentage,\n",
    "                        \"Error Explanation\": error_desc,\n",
    "                        \"Data Issues\": data_issues_final,\n",
    "                        \"Dataset\": dataset_name,\n",
    "                        \"Columns - Attributes\": column\n",
    "                    }\n",
    "                  \n",
    "                    summary_lines.append(summary_line)       \n",
    "    return summary_lines \n",
    "\n",
    "def generate_summary(all_results_ordered, analysed_columns_df, desired_dataset_index, dataset_name, total_rows):\n",
    "    all_summary_lines = []\n",
    "    \n",
    "    for column, analysis_result in all_results_ordered.items():\n",
    "       \n",
    "        matching_rows = analysed_columns_df[\n",
    "            (analysed_columns_df['index'] == desired_dataset_index) & \n",
    "            (analysed_columns_df['Column'] == column)\n",
    "        ]\n",
    "               \n",
    "        format_type = matching_rows['FinalFormat'].iloc[0]\n",
    "\n",
    "        if isinstance(analysis_result, dict):\n",
    "            for key, value in analysis_result.items():\n",
    "                    summary_lines = parse_analysis_result(column, {key: value}, format_type, dataset_name, total_rows)\n",
    "                    all_summary_lines.extend(summary_lines)\n",
    "        elif isinstance(analysis_result, str):\n",
    "            summary_lines = parse_analysis_result(column, {\"General\": analysis_result}, format_type, dataset_name, total_rows)\n",
    "            all_summary_lines.extend(summary_lines)\n",
    "        else:\n",
    "            summary_lines = parse_analysis_result(column, {\"Unexpected\": str(analysis_result)}, format_type, dataset_name, total_rows)\n",
    "            all_summary_lines.extend(summary_lines)\n",
    "    \n",
    "    return all_summary_lines\n",
    "\n",
    "def add_quotes_to_items(text):\n",
    "    # Convert strings like \"datetime.date(1001, 1, 1)\" to \"01-01-1001\"\n",
    "    date_pattern = re.compile(r\"datetime\\.date\\((\\d+),\\s*(\\d+),\\s*(\\d+)\\)\")\n",
    "    \n",
    "    def replace_date(match):\n",
    "        year, month, day = match.groups()\n",
    "        return f'{int(day):02d}-{int(month):02d}-{int(year):04d}'\n",
    "    \n",
    "    formatted_text = date_pattern.sub(replace_date, text)\n",
    "    # Split the text by comma, preserving empty items and whitespace\n",
    "    items = re.split(r'(,\\s*)', formatted_text)\n",
    "    \n",
    "    quoted_items = []\n",
    "    for i, item in enumerate(items):\n",
    "        if i % 2 == 0:  # This is an actual item, not a separator\n",
    "            stripped_item = item.strip()\n",
    "            if stripped_item == '':\n",
    "                quoted_items.append('\"\"')  # Empty string becomes two quotes\n",
    "            elif stripped_item.lower() == 'null':\n",
    "                quoted_items.append('\"NULL\"')\n",
    "            elif stripped_item.isspace():\n",
    "                quoted_items.append(f'\"{item}\"')  # Preserve original whitespace\n",
    "            elif not (stripped_item.startswith('\"') and stripped_item.endswith('\"')):\n",
    "                quoted_items.append(f'\"{stripped_item}\"')\n",
    "            else:\n",
    "                quoted_items.append(stripped_item)  # Already quoted item\n",
    "        else:  # This is a separator (comma and possible whitespace)\n",
    "            quoted_items.append(item)\n",
    "\n",
    "    result = ''.join(quoted_items)\n",
    "    return result\n",
    "\n",
    "def extract_and_categorize_dqi1_values(text: str) -> str:\n",
    "    # Extract all values\n",
    "    values = re.findall(r\"'([^']*)'\", text)\n",
    "    print(values)\n",
    "    # Categorize and represent values\n",
    "    categorized_values = [DataQualityIssues.represent_value(v) for v in values]\n",
    "    print('categorized_values', categorized_values)\n",
    "    # Remove duplicates and 'Valid' values\n",
    "    unique_issues = [v for v in set(categorized_values) if v != 'Valid' and v != '...']\n",
    "    print('UNIQUE ISSUES ', unique_issues)\n",
    "    if unique_issues:\n",
    "        # Sort the unique issues\n",
    "        sorted_issues = sorted(unique_issues)\n",
    "        \n",
    "        # Determine if we need to add '...'\n",
    "        if len(sorted_issues) > 5  in categorized_values:\n",
    "            display_issues = sorted_issues[:5] + ['...']\n",
    "        else:\n",
    "            display_issues = sorted_issues\n",
    "        \n",
    "        data_issues = ', '.join(f'\"{v}\"' for v in display_issues)\n",
    "    \n",
    "    return data_issues\n",
    "\n",
    "def extract_duplicate_values(text: str) -> str:\n",
    "    value_match = re.findall(r'\\[(\\d+),\\s*\\'([^\\']+)\\'\\]', text)\n",
    "    if value_match:\n",
    "        duplicate_values = [v[1] for v in value_match]\n",
    "        data_issues = ', '.join(f'\"{v}\"' for v in set(duplicate_values)[:5])\n",
    "        if len(set(duplicate_values)) > 5:\n",
    "            data_issues += ', ...'\n",
    "    else:\n",
    "        data_issues = \"Duplicate or uniqueness violation found\"\n",
    "    return data_issues\n",
    "\n",
    "def append_to_excel(df, filename, headers_iq_score, total_rows, total_columns, total_dqi_count, dataset_name):\n",
    "    from openpyxl import load_workbook, Workbook\n",
    "    from openpyxl.styles import PatternFill, Font\n",
    "    from openpyxl.utils.dataframe import dataframe_to_rows\n",
    "    from datetime import datetime\n",
    "    import os\n",
    "\n",
    "    # Prepare Excel workbook and sheet\n",
    "    if os.path.exists(filename):\n",
    "        book = load_workbook(filename)\n",
    "        sheet = book.active\n",
    "    else:\n",
    "        book = Workbook()\n",
    "        sheet = book.active\n",
    "        sheet.title = \"Summary\"\n",
    "\n",
    "    # Dataset summary values\n",
    "    timestamp = datetime.now().strftime(\"%d-%m-%Y %H:%M\")\n",
    "    data_items = total_rows * total_columns\n",
    "\n",
    "    # Ensure headers_iq_score is a float\n",
    "    try:\n",
    "        headers_iq_score = float(headers_iq_score)\n",
    "    except (ValueError, TypeError):\n",
    "        headers_iq_score = 0.0\n",
    "\n",
    "    # Remove the HeadersIQ summary row from the DataFrame if present\n",
    "    if not df.empty and \"Columns - Attributes\" in df.columns:\n",
    "        df = df[df[\"Columns - Attributes\"] != \"HeadersIQ\"]\n",
    "\n",
    "    # Calculate total error count from cleaned df\n",
    "    if not df.empty and \"Error Count\" in df.columns:\n",
    "        try:\n",
    "            df.loc[:, \"Error Count\"] = pd.to_numeric(df[\"Error Count\"], errors='coerce').fillna(0)\n",
    "            total_error_count = int(df[\"Error Count\"].sum())\n",
    "        except Exception:\n",
    "            total_error_count = 0\n",
    "    else:\n",
    "        total_error_count = 0\n",
    "\n",
    "    yellow_fill = PatternFill(start_color='FFFACD', end_color='FFFACD', fill_type='solid')\n",
    "    bold_font = Font(bold=True)\n",
    "\n",
    "    # Ensure the insertion happens after the last row with content\n",
    "    last_row = sheet.max_row + 2\n",
    "\n",
    "    # HeadersIQ summary block (two lines: titles and values), with 6 blank columns before\n",
    "    summary_titles = [\"\", \"\", \"\", \"\", \"\", \"\", \"Total Rows\", \"Columns\", \"Total Data Items\", \"Total Errors\", \"Dataset\", \"HeadersIQ\", \"Timestamp\"]\n",
    "    summary_values = [\"\", \"\", \"\", \"\", \"\", \"\", total_rows, total_columns, data_items,\n",
    "                      total_error_count, dataset_name, headers_iq_score, timestamp]\n",
    "\n",
    "    for col_idx, title in enumerate(summary_titles, start=1):\n",
    "        cell = sheet.cell(row=last_row, column=col_idx, value=title)\n",
    "        cell.font = bold_font\n",
    "\n",
    "    for col_idx, value in enumerate(summary_values, start=1):\n",
    "        cell = sheet.cell(row=last_row + 1, column=col_idx, value=value)\n",
    "        cell.font = bold_font\n",
    "        if col_idx == 12 and headers_iq_score < 100:\n",
    "            cell.fill = yellow_fill\n",
    "\n",
    "    # Append DQI rows only if HeadersIQ < 100 and df has data\n",
    "    if headers_iq_score < 100 and not df.empty:\n",
    "        sheet.append([])  # Blank row before DQI table\n",
    "\n",
    "        # Append DQI header\n",
    "        dqi_header = list(df.columns)\n",
    "        sheet.append(dqi_header)\n",
    "        for cell in sheet[sheet.max_row]:\n",
    "            cell.font = bold_font\n",
    "\n",
    "        for row in dataframe_to_rows(df, index=False, header=False):\n",
    "            sheet.append(row)\n",
    "\n",
    "    # Save workbook\n",
    "    book.save(filename)\n",
    "    print(f\"Data appended to {filename}\")\n",
    "\n",
    "\n",
    "print(f\"Last run on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "30 Analyse Data Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2007,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of Analysis on: 2025-04-04 19:34:48\n",
      "Analysis result for y - has the client subscribed a term deposit?:\n",
      "{'Binary format': 'All 41188 binary values are valid.\\n\\nFrequency Distribution (showing top and bottom 10 of 2 binary values):\\nValue  Frequency\\n   no      36548\\n  yes       4640\\n'}\n",
      "age:\n",
      "  Age format: All 41188 values are numerical and valid in the range [0, 130].\n",
      "Actual range of values: (17 : 98)\n",
      "\n",
      "job:\n",
      "  Categorical format: Error(s) found: \n",
      "DQI #4 (Ambiguous Data - Accuracy, Consistency):\n",
      " 330 Unacceptable value(s) at index(es): [(29, 'unknown'), (35, 'unknown'), (73, 'unknown'), (91, 'unknown'), (144, 'unknown'), (299, 'unknown'), (303, 'unknown'), (343, 'unknown'), (388, 'unknown'), (428, 'unknown'), ('...', '...'), (40058, 'unknown'), (40208, 'unknown'), (40240, 'unknown'), (40244, 'unknown'), (40289, 'unknown'), (40370, 'unknown'), (40428, 'unknown'), (40656, 'unknown'), (41005, 'unknown'), (41108, 'unknown')] (displaying only the first and last 10 items)\n",
      "\n",
      "Categorical format with 12 unique value(s):\n",
      "     Category  Frequency\n",
      "       admin.      10422\n",
      "  blue-collar       9254\n",
      "   technician       6743\n",
      "     services       3969\n",
      "   management       2924\n",
      "      retired       1720\n",
      " entrepreneur       1456\n",
      "self-employed       1421\n",
      "    housemaid       1060\n",
      "   unemployed       1014\n",
      "      student        875\n",
      "      unknown        330\n",
      "\n",
      "marital:\n",
      "  Categorical format: Error(s) found: \n",
      "DQI #4 (Ambiguous Data - Accuracy, Consistency):\n",
      " 80 Unacceptable value(s) at index(es): [(40, 'unknown'), (390, 'unknown'), (413, 'unknown'), (1493, 'unknown'), (1608, 'unknown'), (3167, 'unknown'), (3478, 'unknown'), (4196, 'unknown'), (4344, 'unknown'), (4454, 'unknown'), ('...', '...'), (36380, 'unknown'), (36434, 'unknown'), (37258, 'unknown'), (37387, 'unknown'), (37425, 'unknown'), (38550, 'unknown'), (38579, 'unknown'), (39322, 'unknown'), (39323, 'unknown'), (40857, 'unknown')] (displaying only the first and last 10 items)\n",
      "\n",
      "Categorical format with 4 unique value(s):\n",
      "Category  Frequency\n",
      " married      24928\n",
      "  single      11568\n",
      "divorced       4612\n",
      " unknown         80\n",
      "\n",
      "education:\n",
      "  Categorical format: Error(s) found: \n",
      "DQI #4 (Ambiguous Data - Accuracy, Consistency):\n",
      " 1731 Unacceptable value(s) at index(es): [(7, 'unknown'), (10, 'unknown'), (26, 'unknown'), (30, 'unknown'), (31, 'unknown'), (32, 'unknown'), (67, 'unknown'), (73, 'unknown'), (78, 'unknown'), (91, 'unknown'), ('...', '...'), (41108, 'unknown'), (41109, 'unknown'), (41110, 'unknown'), (41113, 'unknown'), (41114, 'unknown'), (41118, 'unknown'), (41120, 'unknown'), (41122, 'unknown'), (41135, 'unknown'), (41175, 'unknown')] (displaying only the first and last 10 items)\n",
      "\n",
      "Categorical format with 8 unique value(s):\n",
      "           Category  Frequency\n",
      "  university.degree      12168\n",
      "        high.school       9515\n",
      "           basic.9y       6045\n",
      "professional.course       5243\n",
      "           basic.4y       4176\n",
      "           basic.6y       2292\n",
      "            unknown       1731\n",
      "         illiterate         18\n",
      "\n",
      "default (categorical):\n",
      "  Categorical format: Error(s) found: \n",
      "DQI #4 (Ambiguous Data - Accuracy, Consistency):\n",
      " 8597 Unacceptable value(s) at index(es): [(1, 'unknown'), (5, 'unknown'), (7, 'unknown'), (10, 'unknown'), (15, 'unknown'), (17, 'unknown'), (19, 'unknown'), (21, 'unknown'), (27, 'unknown'), (28, 'unknown'), ('...', '...'), (40673, 'unknown'), (40702, 'unknown'), (40714, 'unknown'), (40718, 'unknown'), (40727, 'unknown'), (40886, 'unknown'), (40940, 'unknown'), (40941, 'unknown'), (40969, 'unknown'), (40986, 'unknown')] (displaying only the first and last 10 items)\n",
      "\n",
      "Categorical format with 3 unique value(s):\n",
      "Category  Frequency\n",
      "      no      32588\n",
      " unknown       8597\n",
      "     yes          3\n",
      "\n",
      "housing (categorical):\n",
      "  Categorical format: Error(s) found: \n",
      "DQI #4 (Ambiguous Data - Accuracy, Consistency):\n",
      " 990 Unacceptable value(s) at index(es): [(29, 'unknown'), (81, 'unknown'), (261, 'unknown'), (385, 'unknown'), (401, 'unknown'), (470, 'unknown'), (485, 'unknown'), (494, 'unknown'), (495, 'unknown'), (501, 'unknown'), ('...', '...'), (40792, 'unknown'), (40832, 'unknown'), (40875, 'unknown'), (40895, 'unknown'), (40941, 'unknown'), (40990, 'unknown'), (41014, 'unknown'), (41015, 'unknown'), (41029, 'unknown'), (41115, 'unknown')] (displaying only the first and last 10 items)\n",
      "\n",
      "Categorical format with 3 unique value(s):\n",
      "Category  Frequency\n",
      "     yes      21576\n",
      "      no      18622\n",
      " unknown        990\n",
      "\n",
      "loan (categorical):\n",
      "  Categorical format: Error(s) found: \n",
      "DQI #4 (Ambiguous Data - Accuracy, Consistency):\n",
      " 990 Unacceptable value(s) at index(es): [(29, 'unknown'), (81, 'unknown'), (261, 'unknown'), (385, 'unknown'), (401, 'unknown'), (470, 'unknown'), (485, 'unknown'), (494, 'unknown'), (495, 'unknown'), (501, 'unknown'), ('...', '...'), (40792, 'unknown'), (40832, 'unknown'), (40875, 'unknown'), (40895, 'unknown'), (40941, 'unknown'), (40990, 'unknown'), (41014, 'unknown'), (41015, 'unknown'), (41029, 'unknown'), (41115, 'unknown')] (displaying only the first and last 10 items)\n",
      "\n",
      "Categorical format with 3 unique value(s):\n",
      "Category  Frequency\n",
      "      no      33950\n",
      "     yes       6248\n",
      " unknown        990\n",
      "\n",
      "contact (categorical):\n",
      "  Categorical format: All 41188 values are correctly categorical.\n",
      "\n",
      "Categorical format with 2 unique value(s):\n",
      " Category  Frequency\n",
      " cellular      26144\n",
      "telephone      15044\n",
      "\n",
      "month:\n",
      "  Month format: All 41188 month values are valid.\n",
      "\n",
      "Frequency Distribution:\n",
      "    Month  Frequency\n",
      "      May      13769\n",
      "     July       7174\n",
      "   August       6178\n",
      "     June       5318\n",
      " November       4101\n",
      "    April       2632\n",
      "  October        718\n",
      "September        570\n",
      "    March        546\n",
      " December        182\n",
      "\n",
      "day_of_week (day of week):\n",
      "  Weekday format: All 41188 weekday values are valid.\n",
      "\n",
      "Frequency Distribution:\n",
      "  Weekday  Frequency\n",
      " Thursday       8623\n",
      "   Monday       8514\n",
      "Wednesday       8134\n",
      "  Tuesday       8090\n",
      "   Friday       7827\n",
      "\n",
      "duration:\n",
      "  Numerical >=0 format: All 41188 values are numerical and greater or equal to 0 in the range (0:4918).\n",
      "\n",
      "campaign:\n",
      "  Categorical format: All 41188 values are correctly categorical.\n",
      "\n",
      "Categorical format with 42 unique value(s):\n",
      "Category Frequency\n",
      "       1     17642\n",
      "       2     10570\n",
      "       3      5341\n",
      "       4      2651\n",
      "       5      1599\n",
      "       6       979\n",
      "       7       629\n",
      "       8       400\n",
      "       9       283\n",
      "      10       225\n",
      "     ...       ...\n",
      "      32         4\n",
      "      33         4\n",
      "      34         3\n",
      "      40         2\n",
      "      42         2\n",
      "      43         2\n",
      "      37         1\n",
      "      39         1\n",
      "      41         1\n",
      "      56         1\n",
      "\n",
      "pdays (numeric):\n",
      "  Numerical format: All 41188 values are numerical in the range (0:999).\n",
      "\n",
      "previous (numeric):\n",
      "  Numerical format: All 41188 values are numerical in the range (0:7).\n",
      "\n",
      "poutcome (categorical):\n",
      "  Categorical format: Error(s) found: \n",
      "DQI #4 (Ambiguous Data - Accuracy, Consistency):\n",
      " 35563 Unacceptable value(s) at index(es): [(0, 'nonexistent'), (1, 'nonexistent'), (2, 'nonexistent'), (3, 'nonexistent'), (4, 'nonexistent'), (5, 'nonexistent'), (6, 'nonexistent'), (7, 'nonexistent'), (8, 'nonexistent'), (9, 'nonexistent'), ('...', '...'), (41172, 'nonexistent'), (41176, 'nonexistent'), (41177, 'nonexistent'), (41179, 'nonexistent'), (41180, 'nonexistent'), (41181, 'nonexistent'), (41183, 'nonexistent'), (41184, 'nonexistent'), (41185, 'nonexistent'), (41186, 'nonexistent')] (displaying only the first and last 10 items)\n",
      "\n",
      "Categorical format with 3 unique value(s):\n",
      "   Category  Frequency\n",
      "nonexistent      35563\n",
      "    failure       4252\n",
      "    success       1373\n",
      "\n",
      "emp.var.rate (rate):\n",
      "  Numerical format: All 41188 values are numerical in the range (-3.4:1.4).\n",
      "\n",
      "cons.price.idx (price):\n",
      "  Numerical format: All 41188 values are numerical in the range (92.201:94.767).\n",
      "\n",
      "cons.conf.idx (numeric):\n",
      "  Numerical format: All 41188 values are numerical in the range (-50.8:-26.9).\n",
      "\n",
      "euribor3m (numeric):\n",
      "  Numerical format: All 41188 values are numerical in the range (0.634:5.045).\n",
      "\n",
      "nr.employed (number):\n",
      "  Numerical format: All 41188 values are numerical in the range (4963.6:5228.1).\n",
      "\n",
      "y - has the client subscribed a term deposit? (binary):\n",
      "  Binary format: All 41188 binary values are valid.\n",
      "\n",
      "Frequency Distribution (showing top and bottom 10 of 2 binary values):\n",
      "Value  Frequency\n",
      "   no      36548\n",
      "  yes       4640\n",
      "\n",
      "\n",
      "Data appended to SummaryofDQIssuesDiscovered.xlsx\n",
      "Analysis completed and summary appended. Last run on: 2025-04-04 19:34:51\n",
      "Last run on: 2025-04-04 19:34:51\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def calculate_headers_iq_from_summary(summary_lines, total_columns):\n",
    "    dq_issues_per_column = {}\n",
    "\n",
    "    for line in summary_lines:\n",
    "        col = line['Columns - Attributes']\n",
    "        dqi_number = line['DQI#']\n",
    "        percentage_str = str(line['Error Percentage']).replace('%','').strip()\n",
    "\n",
    "        if percentage_str.upper() == \"N/A\":\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            percentage = float(percentage_str)\n",
    "        except ValueError:\n",
    "            continue\n",
    "\n",
    "        if col not in dq_issues_per_column:\n",
    "            dq_issues_per_column[col] = {}\n",
    "\n",
    "        dq_issues_per_column[col][dqi_number] = percentage\n",
    "\n",
    "    # Reuse existing HeadersIQ logic\n",
    "    return calculate_headers_iq(dq_issues_per_column, total_columns)\n",
    "\n",
    "def calculate_headers_iq(dq_issues, total_columns):\n",
    "    if total_columns == 0:\n",
    "        return 100.0\n",
    "\n",
    "    total_iq = 0\n",
    "    for col in range(total_columns):\n",
    "        column_name = list(dq_issues.keys())[col] if col < len(dq_issues) else None\n",
    "        issues = dq_issues.get(column_name, {})\n",
    "        \n",
    "        column_iq = 100\n",
    "        unique_issues = set(issues.keys())\n",
    "        if '9' in unique_issues and '10' in unique_issues:\n",
    "            unique_issues.remove('10')\n",
    "        penalty = sum(min(issues[dqi], 100) for dqi in unique_issues)\n",
    "        column_iq -= min(penalty, 100)\n",
    "        total_iq += column_iq\n",
    "\n",
    "    headers_iq = total_iq / total_columns\n",
    "    return round(headers_iq, 2)\n",
    "\n",
    "\n",
    "def analyse_data_quality(df, analysed_columns_df, desired_dataset_index):\n",
    "\n",
    "    print(f\"Start of Analysis on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    all_results_ordered = {}\n",
    "\n",
    "    # Define a range for valid years and weeks\n",
    "    min_valid_year = 1800\n",
    "    max_valid_year = 2100\n",
    "        \n",
    "    categorical_threshold = 100\n",
    "\n",
    "    # Initialize primary_key and foreign_key_columns\n",
    "    primary_key = None\n",
    "    foreign_key_columns = []\n",
    "    primary_key_columns = []\n",
    "\n",
    "    # Get PK and FK information for this dataset if available\n",
    "    if all_datasets_info is not None:\n",
    "        dataset_info = all_datasets_info[all_datasets_info['index'] == desired_dataset_index]\n",
    "        if not dataset_info.empty:\n",
    "            primary_key = dataset_info.iloc[0]['primary_key']\n",
    "            if isinstance(primary_key, str):\n",
    "                primary_key_columns = [col.strip() for col in primary_key.split(',')]\n",
    "            else:\n",
    "                primary_key_columns = []\n",
    "            foreign_keys = dataset_info.iloc[0]['foreign_keys']\n",
    "            print(f\"Primary key columns: {primary_key_columns}\")\n",
    "            print(f\"foreign_keys: {foreign_keys}\")\n",
    "            if pd.notna(foreign_keys) and isinstance(foreign_keys, str):\n",
    "                foreign_key_columns = [fk.split(' -> ')[0] for fk in foreign_keys.split(', ')]\n",
    "                print(f\"foreign_key_columns: {foreign_key_columns}\")\n",
    "            else:\n",
    "                print(\"No valid foreign keys found.\")\n",
    "\n",
    "    if (DB_or_dataset == \"Dataset\"):\n",
    "        desired_dataset_index = int(desired_dataset_index)\n",
    "    \n",
    "    column_order = analysed_columns_df[analysed_columns_df['index'] == desired_dataset_index].sort_values('ID')['Column'].tolist()\n",
    "\n",
    "    # Check if there's already a composite primary key column\n",
    "    composite_pk_name = next((col for col in df.columns if '|' in col), None)\n",
    "\n",
    "    if composite_pk_name:\n",
    "        print(f\"Found existing composite primary key: {composite_pk_name}\")\n",
    "        all_results_ordered[composite_pk_name] = {\n",
    "            \"Composite Primary Key format\": check_id_attributes(df, composite_pk_name, is_foreign_key=False)\n",
    "        }\n",
    "        print(f\"Analyzing composite primary key: {composite_pk_name}\")\n",
    "        \n",
    "        # Remove individual primary key columns from the analysis but not from the DataFrame\n",
    "        individual_pk_columns = composite_pk_name.split('|')\n",
    "        column_order = [col for col in column_order if col not in individual_pk_columns]\n",
    "        print(f\"Removed individual primary key columns from analysis: {individual_pk_columns}\")\n",
    "    elif len(primary_key_columns) > 1:\n",
    "        composite_pk_name = '|'.join(primary_key_columns)\n",
    "        df[composite_pk_name] = df[primary_key_columns].astype(str).agg('|'.join, axis=1)\n",
    "        all_results_ordered[composite_pk_name] = {\n",
    "            \"Composite Primary Key format\": check_id_attributes(df, composite_pk_name, is_foreign_key=False)\n",
    "        }\n",
    "        print(f\"Created and analyzing composite primary key: {composite_pk_name}\")\n",
    "        \n",
    "        # Remove individual primary key columns from the analysis but not from the DataFrame\n",
    "        column_order = [col for col in column_order if col not in primary_key_columns]\n",
    "        print(f\"Removed individual primary key columns from analysis: {primary_key_columns}\")\n",
    "    keyword = \"categorical\"  # Keyword to search for in the \"FinalFormat\"\n",
    "\n",
    "    # Find columns marked as \"categorical\" for the specified dataset index\n",
    "    categorical_columns = analysed_columns_df[\n",
    "        (analysed_columns_df['index'] == desired_dataset_index) & \n",
    "        (analysed_columns_df['FinalFormat'].str.contains(keyword, case=False, na=False))\n",
    "    ]['Column'].tolist()\n",
    "\n",
    "    def handle_blob_column(df, column_name):\n",
    "        def process_blob(value):\n",
    "            if pd.isna(value):\n",
    "                return 'NULL'\n",
    "            elif isinstance(value, bytes):\n",
    "                return 'BLOB'\n",
    "            else:\n",
    "                return str(value)\n",
    "\n",
    "        df[column_name] = df[column_name].apply(process_blob)\n",
    "        return df\n",
    "\n",
    "    # Identify and process BLOB columns\n",
    "    for column in df.columns:\n",
    "        if df[column].dtype == 'object':\n",
    "            # Check if any non-null value in the column is bytes\n",
    "            if df[column].dropna().apply(lambda x: isinstance(x, bytes)).any():\n",
    "                print(f\"Identified BLOB column: {column}\")\n",
    "                df = handle_blob_column(df, column)\n",
    "                \n",
    "    # Iterate through the columns in the custom order and analyze them\n",
    "    for column in column_order:\n",
    "        # Ensure that the column exists in the DataFrame\n",
    "        if column in dataset_df.columns:\n",
    "            if column in categorical_columns:\n",
    "                all_results_ordered[column] = {\n",
    "                            \"Categorical format\": check_categorical(dataset_df, column, categorical_threshold)\n",
    "                        }\n",
    "            else:\n",
    "                # Check the format specified in the \"AnalysedColumns\" sheet\n",
    "                format_in_sheet = analysed_columns_df[\n",
    "                    (analysed_columns_df['index'] == desired_dataset_index) &\n",
    "                    (analysed_columns_df['Column'] == column)\n",
    "                ]['FinalFormat'].iloc[0]\n",
    "                \n",
    "                if pd.isna(format_in_sheet):\n",
    "                    all_results_ordered[column] = \"Target word not found, and Format not determined\"\n",
    "                else:\n",
    "                    if \"IDcolumn\" in format_in_sheet:\n",
    "                        is_foreign_key = column in foreign_key_columns\n",
    "                        print(f\"Column: {column}, is_foreign_key: {is_foreign_key}\")\n",
    "                        id_check_result = check_id_attributes(dataset_df, column, is_foreign_key)\n",
    "                        if column == primary_key:\n",
    "                            all_results_ordered[column] = {\n",
    "                                \"Primary Key format\": id_check_result,\n",
    "                            }\n",
    "                        elif is_foreign_key:\n",
    "                            referenced_table = next((fk.split(' -> ')[1] for fk in foreign_keys.split(', ') if fk.startswith(column)), None)\n",
    "                            all_results_ordered[column] = {\n",
    "                                \"Foreign Key format\": id_check_result,\n",
    "                                \"References\": referenced_table\n",
    "                            }\n",
    "                        else:\n",
    "                            all_results_ordered[column] = {\n",
    "                                \"ID column format\": id_check_result,\n",
    "                            }\n",
    "                    elif \"numerical>=0\" in format_in_sheet:\n",
    "                        all_results_ordered[column] = {\n",
    "                            \"Numerical >=0 format\": check_numerical_ge_zero(dataset_df, column),\n",
    "                        }\n",
    "                    elif \"percentage\" in format_in_sheet:\n",
    "                        all_results_ordered[column] = {\n",
    "                            \"Percentage format\": check_numerical_between(dataset_df, column, 0, 100),\n",
    "                        }\n",
    "                    elif \"age\" in format_in_sheet:\n",
    "                        all_results_ordered[column] = {\n",
    "                            \"Age format\": check_numerical_between(dataset_df, column, 0, 130),\n",
    "                        }\n",
    "                    elif \"numerical between 0 and 360\" in format_in_sheet:\n",
    "                        all_results_ordered[column] = {\n",
    "                            \"Numerical (between 0 and 360) format\": check_numerical_between(dataset_df, column, 0, 360),\n",
    "                        }\n",
    "                    elif \"numerical between 0 and 60\" in format_in_sheet:\n",
    "                        all_results_ordered[column] = {\n",
    "                            \"Numerical (between 0 and 60) format\": check_numerical_between(dataset_df, column, 0, 60),\n",
    "                        }\n",
    "                    elif \"numerical\" in format_in_sheet:\n",
    "                        all_results_ordered[column] = {\n",
    "                            \"Numerical format\": check_numerical(dataset_df, column),\n",
    "                        }\n",
    "                    elif \"string\" in format_in_sheet:\n",
    "                        all_results_ordered[column] = {\n",
    "                            \"String format\": check_string(dataset_df, column),\n",
    "                        }\n",
    "                    elif \"datetime\" in format_in_sheet:\n",
    "                        all_results_ordered[column] = {\n",
    "                            \"Datetime format\": check_datetime(dataset_df, column),\n",
    "                        }\n",
    "                    elif \"date\" in format_in_sheet:\n",
    "                        all_results_ordered[column] = {\n",
    "                            \"Date format\": check_date(dataset_df, column),\n",
    "                        }\n",
    "                    elif \"time\" in format_in_sheet:\n",
    "                        all_results_ordered[column] = {\n",
    "                            \"Time format\": check_time(dataset_df, column),\n",
    "                        }\n",
    "                    elif \"month\" in format_in_sheet:\n",
    "                        all_results_ordered[column] = {\n",
    "                            \"Month format\": check_month(dataset_df, column),\n",
    "                        }\n",
    "                    elif \"year\" in format_in_sheet:\n",
    "                        all_results_ordered[column] = {\n",
    "                            \"Year format\": check_numerical_between(dataset_df, column, min_valid_year, max_valid_year),\n",
    "                        }\n",
    "                    elif \"weekday\" in format_in_sheet:\n",
    "                        all_results_ordered[column] = {\n",
    "                            \"Weekday format\": check_weekday(dataset_df, column),\n",
    "                        } \n",
    "                    elif \"week\" in format_in_sheet:\n",
    "                        all_results_ordered[column] = {\n",
    "                            \"Week format\": check_numerical_between(dataset_df, column, 1, 53),\n",
    "                        }\n",
    "                    elif \"day\" in format_in_sheet:\n",
    "                        all_results_ordered[column] = {\n",
    "                            \"Day format\" : check_numerical_between(dataset_df, column, 1, 366),\n",
    "                        }\n",
    "                    elif \"hour\" in format_in_sheet:\n",
    "                        all_results_ordered[column] = {\n",
    "                            \"Hour format\" : check_numerical_between(dataset_df, column, 0, 24),\n",
    "                        }\n",
    "                    elif \"modelname\" in format_in_sheet:\n",
    "                        all_results_ordered[column] = {\n",
    "                            \"Model Name format\": check_model_name(dataset_df, column),\n",
    "                        }\n",
    "                    elif \"name\" in format_in_sheet:\n",
    "                        all_results_ordered[column] = {\n",
    "                            \"Name format\": check_name(dataset_df, column),\n",
    "                        }\n",
    "                    elif \"street\" in format_in_sheet:\n",
    "                        all_results_ordered[column] = {\n",
    "                            \"Street format\": check_street(dataset_df, column),\n",
    "                        }\n",
    "                    elif \"city\" in format_in_sheet:\n",
    "                        all_results_ordered[column] = {\n",
    "                            \"City format\": check_city(dataset_df, column),\n",
    "                        }\n",
    "                    elif \"state\" in format_in_sheet:\n",
    "                        all_results_ordered[column] = {\n",
    "                            \"State format\": check_state(dataset_df, column),\n",
    "                        }\n",
    "                    elif \"country\" in format_in_sheet:\n",
    "                        all_results_ordered[column] = {\n",
    "                            \"Country format\": check_country(dataset_df, column),\n",
    "                        }\n",
    "                    elif \"postalcode\" in format_in_sheet:\n",
    "                        all_results_ordered[column] = {\n",
    "                            \"Postal Code format\": check_postal_code(dataset_df, column),\n",
    "                        }  \n",
    "                    elif \"phone\" in format_in_sheet:\n",
    "                        all_results_ordered[column] = {\n",
    "                            \"Phone format\": check_phone_numbers(dataset_df, column),\n",
    "                        }\n",
    "                    elif \"ph\" in format_in_sheet:\n",
    "                        all_results_ordered[column] = {\n",
    "                            \"pH format\": check_numerical_between(dataset_df, column, 0, 14),\n",
    "                        }    \n",
    "                    elif \"acidity\" in format_in_sheet:\n",
    "                        all_results_ordered[column] = {\n",
    "                            \"Acidity format\": check_numerical_between(dataset_df, column, 0, 7),\n",
    "                        }                                                 \n",
    "                    elif \"alkalinity\" in format_in_sheet:\n",
    "                        all_results_ordered[column] = {\n",
    "                            \"Alkalinity format\": check_numerical_between(dataset_df, column, 7, 14),\n",
    "                        }  \n",
    "                    elif \"tannins\" in format_in_sheet:\n",
    "                        all_results_ordered[column] = {\n",
    "                            \"Tannins format\": check_numerical_between(dataset_df, column, 0, 100),\n",
    "                        }                         \n",
    "                    elif \"saltness\" in format_in_sheet:\n",
    "                        all_results_ordered[column] = {\n",
    "                            \"Saltiness format\": check_numerical_between(dataset_df, column, 0, 40),\n",
    "                        }\n",
    "                    elif \"heartrate\" in format_in_sheet:\n",
    "                        all_results_ordered[column] = {\n",
    "                            \"Heart Rate format\": check_numerical_between(dataset_df, column, 40, 200),\n",
    "                        } \n",
    "                    elif \"bloodpressure\" in format_in_sheet:\n",
    "                        all_results_ordered[column] = {\n",
    "                            \"Blood Pressure format\": check_numerical_between(dataset_df, column, 0, 250),\n",
    "                        }                        \n",
    "                    elif \"latitude\" in format_in_sheet:\n",
    "                        all_results_ordered[column] = {\n",
    "                            \"Latitude format\": check_numerical_between(dataset_df, column, -90, 90),\n",
    "                        }\n",
    "                    elif \"longitude\" in format_in_sheet:\n",
    "                        all_results_ordered[column] = {\n",
    "                            \"Longitude format\": check_numerical_between(dataset_df, column, -180, 180),\n",
    "                        }\n",
    "                    elif \"normalized\" in format_in_sheet:\n",
    "                        all_results_ordered[column] = {\n",
    "                            \"Normalized format\": check_numerical_between(dataset_df, column, 0, 1),\n",
    "                        }\n",
    "                    elif \"IPformat\" in format_in_sheet:\n",
    "                        all_results_ordered[column] = {\n",
    "                            \"IP Address format\": check_ip_format(df, column),\n",
    "                        }\n",
    "                    elif \"URLformat\" in format_in_sheet:\n",
    "                        url_result = check_url_format(df, column)\n",
    "                        all_results_ordered[column] = {\n",
    "                            \"URL format\": url_result\n",
    "                        }\n",
    "                    elif \"E-mailformat\" in format_in_sheet:\n",
    "                        all_results_ordered[column] = {\n",
    "                            \"Email format\": check_email_format(df, column),\n",
    "                        }\n",
    "                    elif \"binary\" in format_in_sheet:\n",
    "                        all_results_ordered[column] = {\n",
    "                            \"Binary format\": check_binary_values(df, column),\n",
    "                        }\n",
    "                        # After each analysis, print the result\n",
    "                        print(f\"Analysis result for {column}:\")\n",
    "                        print(all_results_ordered[column])\n",
    "                    else:\n",
    "                        print(f\"Column '{column}' not found in the dataset.\")\n",
    "                        all_results_ordered[column] = {\n",
    "                            \"Error\": f\"Column '{column}' not found in the dataset.\"\n",
    "                        }\n",
    "\n",
    "    # Loop to print the results in the desired format\n",
    "    for column, analysis_result in all_results_ordered.items():\n",
    "        # Fetch the 'SourceKeyword' for this column from 'analysed_columns_df'\n",
    "        if '|' in column:  # This is a composite primary key\n",
    "            print(f\"{column} (Composite Primary Key):\")\n",
    "        else:\n",
    "            try:\n",
    "                SourceKeyword_value = analysed_columns_df.loc[(analysed_columns_df['index'] == desired_dataset_index) & (analysed_columns_df['Column'] == column), 'SourceKeyword'].iloc[0]\n",
    "                # Check if 'SourceKeyword' is not NaN before converting to lower case\n",
    "                if pd.notna(SourceKeyword_value) and SourceKeyword_value.lower() != column.lower():\n",
    "                    print(f\"{column} ({SourceKeyword_value}):\")\n",
    "                else:\n",
    "                    print(f\"{column}:\")\n",
    "            except IndexError:\n",
    "                print(f\"{column}:\")\n",
    "        \n",
    "        if isinstance(analysis_result, pd.DataFrame):\n",
    "            print(analysis_result.to_string(index=False))\n",
    "        elif isinstance(analysis_result, dict):\n",
    "            for key, value in analysis_result.items():\n",
    "                print(f\"  {key}: {value}\")\n",
    "        elif isinstance(analysis_result, str):  # Added condition to handle string results\n",
    "            print(f\"  {analysis_result}\")\n",
    "        else:\n",
    "            print(\"No results available.\")\n",
    "        \n",
    "        print()  # Add an empty line for separation\n",
    "    \n",
    "    # Return the results as well\n",
    "    return all_results_ordered\n",
    "\n",
    "def generate_pk_fk_summary(all_results_ordered, primary_key, foreign_key_columns):\n",
    "    summary = \"Primary and Foreign Key Analysis:\\n\\n\"\n",
    "    \n",
    "    if primary_key in all_results_ordered:\n",
    "        pk_result = all_results_ordered[primary_key].get(\"Primary Key format\")\n",
    "        if isinstance(pk_result, dict) and pk_result.get(\"issue\"):\n",
    "            summary += f\"Primary Key '{primary_key}' has issues: {pk_result['error_message']}\\n\"\n",
    "        else:\n",
    "            summary += f\"Primary Key '{primary_key}' is valid.\\n\"\n",
    "    \n",
    "    for column in foreign_key_columns:\n",
    "        if column in all_results_ordered:\n",
    "            fk_result = all_results_ordered[column].get(\"Foreign Key format\")\n",
    "            referenced_table = all_results_ordered[column].get(\"References\")\n",
    "            if isinstance(fk_result, dict) and fk_result.get(\"issue\"):\n",
    "                summary += f\"Foreign Key '{column}' (referencing {referenced_table}) has issues: {fk_result['error_message']}\\n\"\n",
    "            else:\n",
    "                summary += f\"Foreign Key '{column}' (referencing {referenced_table}) is valid.\\n\"\n",
    "    \n",
    "    return summary\n",
    "\n",
    "# Main execution\n",
    "\n",
    "# Load AllDatasetsInfo if available\n",
    "try:\n",
    "    all_datasets_info = pd.read_excel(\"AllDatasetsInfo.xlsx\")      \n",
    "       \n",
    "except FileNotFoundError:\n",
    "    all_datasets_info = None\n",
    "\n",
    "if all_datasets_info is not None:\n",
    "    dataset_info = all_datasets_info[all_datasets_info['index'] == desired_dataset_index]\n",
    "    if not dataset_info.empty:\n",
    "        primary_key = dataset_info.iloc[0]['primary_key']\n",
    "        foreign_keys = dataset_info.iloc[0]['foreign_keys']\n",
    "        print(f\"Dataset info for index {desired_dataset_index}:\")\n",
    "        print(dataset_info)\n",
    "        print(f\"Primary key: {primary_key}\")\n",
    "        print(f\"Foreign keys: {foreign_keys}\")\n",
    "        if pd.notna(foreign_keys) and isinstance(foreign_keys, str):\n",
    "            foreign_key_columns = [fk.split(' -> ')[0] for fk in foreign_keys.split(', ')]\n",
    "            print(\"Foreign key columns:\", foreign_key_columns)\n",
    "        else:\n",
    "            print(f\"Invalid foreign keys value: {foreign_keys}\")\n",
    "            foreign_key_columns = []\n",
    "    \n",
    "dataset_name = desired_dataset_index  \n",
    "results = analyse_data_quality(dataset_df, analysed_columns_df, desired_dataset_index)\n",
    "\n",
    "# Get the total number of rows in the dataset\n",
    "total_rows = len(dataset_df)\n",
    "\n",
    "# get all parsed results\n",
    "summary_lines = generate_summary(results, analysed_columns_df, desired_dataset_index, dataset_name, total_rows)\n",
    "\n",
    "# compute and inject HeadersIQ\n",
    "total_columns = analysed_columns_df[analysed_columns_df['index'] == desired_dataset_index].shape[0]\n",
    "headers_iq_score = calculate_headers_iq_from_summary(summary_lines, total_columns)\n",
    "\n",
    "# Count DQIs (one per summary line that has a DQI#)\n",
    "#total_dqi_count = sum(1 for line in summary_lines if str(line.get(\"DQI#\", \"\")).isdigit())\n",
    "total_dqi_count = len([line for line in summary_lines if line['DQI#']])\n",
    "# Obter timestamp\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "# Linha de títulos personalizados (apenas para o resumo HeadersIQ)\n",
    "header_summary_titles = {\n",
    "    'DQI#': '',\n",
    "    'DQ Issue Description': '',\n",
    "    'Data Quality Dimension': '',\n",
    "    'Function Name': '',\n",
    "    'Check#': '',\n",
    "    'Format Being Analyzed': '',\n",
    "    'Error Count': 'Total Rows',\n",
    "    'Error Percentage': 'Columns',\n",
    "    'Error Explanation': 'Total Data Items',\n",
    "    'Data Issues': 'DQIs Found',\n",
    "    'Dataset': '',\n",
    "    'Columns - Attributes': 'HeadersIQ',\n",
    "    'Timestamp': ''\n",
    "}\n",
    "\n",
    "headers_iq_summary = {\n",
    "    'DQI#': '',\n",
    "    'DQ Issue Description': '',\n",
    "    'Data Quality Dimension': '',\n",
    "    'Function Name': '',\n",
    "    'Check#': '',\n",
    "    'Format Being Analyzed': '',\n",
    "    'Error Count': total_rows,\n",
    "    'Error Percentage': total_columns,\n",
    "    'Error Explanation': total_rows * total_columns,\n",
    "    'Data Issues': total_dqi_count,\n",
    "    'Dataset': dataset_name,\n",
    "    'Columns - Attributes': 'HeadersIQ',\n",
    "    'Timestamp': timestamp\n",
    "}\n",
    "\n",
    "summary_lines.insert(0, headers_iq_summary)\n",
    "summary_lines.insert(0, header_summary_titles)\n",
    "\n",
    "\n",
    "# build your DataFrame and write to Excel\n",
    "if summary_lines:\n",
    "    summary_df = pd.DataFrame(summary_lines)\n",
    "    # Add the Timestamp column here\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    summary_df['Timestamp'] = timestamp\n",
    "    \n",
    "    # Ensure Timestamp is the last column (this is now safe to do)\n",
    "    columns = summary_df.columns.tolist()\n",
    "    columns.remove('Timestamp')\n",
    "    columns.append('Timestamp')\n",
    "    summary_df = summary_df[columns]\n",
    "\n",
    "    excel_filename = \"SummaryofDQIssuesDiscovered.xlsx\"\n",
    "    append_to_excel(summary_df, excel_filename, headers_iq_score, total_rows, total_columns, total_dqi_count, dataset_name)\n",
    "    print(f\"Analysis completed and summary appended. Last run on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "else:\n",
    "    print(\"No summary lines generated. Nothing to append to Excel.\")\n",
    "\n",
    "from datetime import datetime\n",
    "print(f\"Last run on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EXIT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2003,
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\290099c\\AppData\\Local\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3585: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "raise SystemExit"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
