{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "580f536d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'viznet'...\n",
      "Updating files:   4% (500/12147)\n",
      "Updating files:   5% (608/12147)\n",
      "Updating files:   6% (729/12147)\n",
      "Updating files:   7% (851/12147)\n",
      "Updating files:   8% (972/12147)\n",
      "Updating files:   9% (1094/12147)\n",
      "Updating files:   9% (1150/12147)\n",
      "Updating files:  10% (1215/12147)\n",
      "Updating files:  11% (1337/12147)\n",
      "Updating files:  12% (1458/12147)\n",
      "Updating files:  13% (1580/12147)\n",
      "Updating files:  14% (1701/12147)\n",
      "Updating files:  15% (1823/12147)\n",
      "Updating files:  16% (1944/12147)\n",
      "Updating files:  17% (2065/12147)\n",
      "Updating files:  18% (2187/12147)\n",
      "Updating files:  18% (2249/12147)\n",
      "Updating files:  19% (2308/12147)\n",
      "Updating files:  20% (2430/12147)\n",
      "Updating files:  21% (2551/12147)\n",
      "error: unable to create file experiment/data/abamama-abemama-abemama-atoll-abemama-island-apamama-atoll-apamama-island-dundas-hopper-island-roger-simpson-island-simpson-island-kiribati-earth-geody-8__5.json: Filename too long\n",
      "Updating files:  22% (2673/12147)\n",
      "error: unable to create file experiment/data/alexis-ajinca-player-profile-new-orleans-pelicans-news-rumors-nba-stats-d-league-stats-international-stats-events-stats-game-logs-bests-awards-realgm-25__2.json: Filename too long\n",
      "Updating files:  23% (2794/12147)\n",
      "Updating files:  24% (2916/12147)\n",
      "error: unable to create file experiment/data/april-may-june-july-2013-regularly-scheduled-flights-with-more-than-50-delayed-arrivals-of-more-than-30-minutes-bureau-of-transportation-statistics-0__7.json: Filename too long\n",
      "Updating files:  25% (3037/12147)\n",
      "Updating files:  26% (3159/12147)\n",
      "Updating files:  27% (3280/12147)\n",
      "Updating files:  28% (3402/12147)\n",
      "Updating files:  28% (3405/12147)\n",
      "Updating files:  29% (3523/12147)\n",
      "Updating files:  30% (3645/12147)\n",
      "Updating files:  31% (3766/12147)\n",
      "Updating files:  32% (3888/12147)\n",
      "Updating files:  33% (4009/12147)\n",
      "Updating files:  34% (4130/12147)\n",
      "Updating files:  35% (4252/12147)\n",
      "Updating files:  36% (4373/12147)\n",
      "Updating files:  37% (4495/12147)\n",
      "Updating files:  37% (4605/12147)\n",
      "Updating files:  38% (4616/12147)\n",
      "Updating files:  39% (4738/12147)\n",
      "Updating files:  40% (4859/12147)\n",
      "Updating files:  41% (4981/12147)\n",
      "error: unable to create file experiment/data/cricket-records-new-zealand-tour-of-england-and-scotland-apr-jul-2008-records-first-class-matches-most-sixes-in-an-innings-espn-cricinfo-0__7.json: Filename too long\n",
      "error: unable to create file experiment/data/cricket-records-new-zealand-v-sri-lanka-in-india-records-one-day-internationals-most-runs-from-fours-and-sixes-in-an-innings-espn-cricinfo-6__0.json: Filename too long\n",
      "Updating files:  42% (5102/12147)\n",
      "error: unable to create file experiment/data/cricket-records-records-england-marylebone-cricket-club-tour-of-australia-nov-1903-mar-1904-first-class-matches-best-strike-rates-in-an-innings-espn-cricinfo-0__3.json: Filename too long\n",
      "error: unable to create file experiment/data/cricket-records-records-icc-cricket-world-cup-qualifier-icc-trophy-one-day-internationals-most-runs-from-fours-and-sixes-in-an-innings-espn-cricinfo-0__7.json: Filename too long\n",
      "error: unable to create file experiment/data/cricket-records-records-south-africa-tour-of-england-and-ireland-jun-sep-2003-first-class-matches-most-runs-from-fours-and-sixes-in-an-innings-espn-cricinfo-0__2.json: Filename too long\n",
      "error: unable to create file experiment/data/cricket-records-shere-bangla-national-stadium-mirpur-dhaka-ireland-records-one-day-internationals-best-economy-rates-in-an-innings-espn-cricinfo-3__3.json: Filename too long\n",
      "error: unable to create file experiment/data/cricket-records-st-george-s-park-port-elizabeth-south-africa-records-one-day-internationals-best-economy-rates-in-an-innings-espn-cricinfo-3__1.json: Filename too long\n",
      "Updating files:  43% (5224/12147)\n",
      "Updating files:  44% (5345/12147)\n",
      "error: unable to create file experiment/data/daily-fantasy-basketball-strategy-the-fake-basketball-fantasy-basketball-floor-time-increases-decreases-2-23-daily-fantasy-basketball-strategy-the-fake-basketball-1__5.json: Filename too long\n",
      "Updating files:  45% (5467/12147)\n",
      "Updating files:  46% (5588/12147)\n",
      "Updating files:  46% (5688/12147)\n",
      "Updating files:  47% (5710/12147)\n",
      "Updating files:  48% (5831/12147)\n",
      "Updating files:  49% (5953/12147)\n",
      "Updating files:  50% (6074/12147)\n",
      "Updating files:  51% (6195/12147)\n",
      "Updating files:  52% (6317/12147)\n",
      "Updating files:  53% (6438/12147)\n",
      "Updating files:  54% (6560/12147)\n",
      "error: unable to create file experiment/data/final-pakistan-international-airlines-v-zarai-taraqiati-bank-limited-at-karachi-dec-20-23-2011-cricket-fall-of-wickets-and-partnerships-espn-cricinfo-3__0.json: Filename too long\n",
      "Updating files:  55% (6681/12147)\n",
      "Updating files:  56% (6803/12147)\n",
      "Updating files:  56% (6860/12147)\n",
      "error: unable to create file experiment/data/group-a-burgher-recreation-club-v-saracens-sports-club-at-colombo-burgher-feb-8-10-2013-cricket-fall-of-wickets-and-partnerships-espn-cricinfo-3__2.json: Filename too long\n",
      "error: unable to create file experiment/data/group-a-karnataka-state-cricket-association-xi-v-bangladesh-a-at-alur-aug-6-9-2012-cricket-fall-of-wickets-and-partnerships-espn-cricinfo-7__5.json: Filename too long\n",
      "error: unable to create file experiment/data/group-b-colombo-cricket-club-v-nondescripts-cricket-club-at-colombo-ccc-mar-1-3-2013-cricket-fall-of-wickets-and-partnerships-espn-cricinfo-3__2.json: Filename too long\n",
      "Updating files:  57% (6924/12147)\n",
      "error: unable to create file experiment/data/group-b-mumbai-cricket-association-xi-v-hyderabad-cricket-association-xi-at-mysore-aug-1-3-2012-cricket-fall-of-wickets-and-partnerships-espn-cricinfo-7__3.json: Filename too long\n",
      "Updating files:  58% (7046/12147)\n",
      "Updating files:  59% (7167/12147)\n",
      "Updating files:  60% (7289/12147)\n",
      "error: unable to create file experiment/data/jerome-dyson-player-profile-new-orleans-pelicans-news-rumors-nba-stats-ncaa-stats-d-league-stats-international-stats-events-stats-game-logs-bests-awards-realgm-31__8.json: Filename too long\n",
      "Updating files:  61% (7410/12147)\n",
      "Updating files:  62% (7532/12147)\n",
      "error: unable to create file experiment/data/keith-langford-player-profile-san-antonio-spurs-news-rumors-nba-stats-ncaa-stats-d-league-stats-international-stats-events-stats-game-logs-bests-awards-realgm-22__0.json: Filename too long\n",
      "error: unable to create file experiment/data/kevin-pauwels-jim-aernouts-and-sanne-cant-shine-in-lille-updated-full-results-photos-cyclocross-magazine-cyclocross-news-races-bikes-photos-videos-1__0.json: Filename too long\n",
      "Updating files:  63% (7653/12147)\n",
      "Updating files:  63% (7758/12147)\n",
      "Updating files:  64% (7775/12147)\n",
      "Updating files:  65% (7896/12147)\n",
      "error: unable to create file experiment/data/loaddata-9mm-luger-9mm-parabellum-alliant-reloaders-guide-2004-charge-and-load-information-data-for-the-9mm-luger-9mm-parabellum-alliant-reloaders-guide-2004-5__1.json: Filename too long\n",
      "error: unable to create file experiment/data/long-beach-state-official-athletic-site-long-beach-state-official-athletic-site-long-beach-state-official-athletic-site-men-s-volleyball-1__4.json: Filename too long\n",
      "Updating files:  66% (8018/12147)\n",
      "Updating files:  67% (8139/12147)\n",
      "Updating files:  68% (8260/12147)\n",
      "Updating files:  69% (8382/12147)\n",
      "Updating files:  70% (8503/12147)\n",
      "Updating files:  70% (8547/12147)\n",
      "Updating files:  71% (8625/12147)\n",
      "error: unable to create file experiment/data/north-division-marylebone-cricket-club-young-cricketers-v-lancashire-2nd-xi-at-shenley-sep-3-5-2013-cricket-fall-of-wickets-and-partnerships-espn-cricinfo-3__0.json: Filename too long\n",
      "Updating files:  72% (8746/12147)\n",
      "error: unable to create file experiment/data/nys-marks-a-milestone-with-win-in-gieten-van-den-brand-wins-the-day-updated-full-results-photos-cyclocross-magazine-cyclocross-news-races-bikes-photos-videos-0__0.json: Filename too long\n",
      "Updating files:  73% (8868/12147)\n",
      "error: unable to create file experiment/data/pape-sy-player-profile-atlanta-hawks-news-rumors-nba-stats-d-league-stats-international-stats-events-stats-game-logs-bests-awards-realgm-24__1.json: Filename too long\n",
      "Updating files:  74% (8989/12147)\n",
      "Updating files:  75% (9111/12147)\n",
      "Updating files:  76% (9232/12147)\n",
      "Updating files:  77% (9354/12147)\n",
      "Updating files:  77% (9359/12147)\n",
      "Updating files:  78% (9475/12147)\n",
      "Updating files:  79% (9597/12147)\n",
      "Updating files:  80% (9718/12147)\n",
      "Updating files:  81% (9840/12147)\n",
      "Updating files:  82% (9961/12147)\n",
      "Updating files:  83% (10083/12147)\n",
      "Updating files:  83% (10131/12147)\n",
      "Updating files:  84% (10204/12147)\n",
      "Updating files:  85% (10325/12147)\n",
      "Updating files:  86% (10447/12147)\n",
      "Updating files:  87% (10568/12147)\n",
      "Updating files:  88% (10690/12147)\n",
      "Updating files:  89% (10811/12147)\n",
      "error: unable to create file experiment/data/table-3-global-spread-of-vancomycin-resistant-enterococcus-faecium-from-distinct-nosocomial-genetic-complex-volume-11-number-6-june-2005-emerging-infectious-disease-journal-cdc-1__0.json: Filename too long\n",
      "Updating files:  89% (10855/12147)\n",
      "Updating files:  90% (10933/12147)\n",
      "Updating files:  91% (11054/12147)\n",
      "Updating files:  92% (11176/12147)\n",
      "Updating files:  93% (11297/12147)\n",
      "Updating files:  94% (11419/12147)\n",
      "error: unable to create file experiment/data/usgs-open-file-report-2006-1008-high-resolution-geologic-mapping-of-the-inner-continental-shelf-boston-harbor-and-approaches-massachusetts-appendix-2-5__2.json: Filename too long\n",
      "Updating files:  95% (11540/12147)\n",
      "Updating files:  95% (11642/12147)\n",
      "Updating files:  96% (11662/12147)\n",
      "Updating files:  97% (11783/12147)\n",
      "Updating files:  98% (11905/12147)\n",
      "Updating files:  99% (12026/12147)\n",
      "Updating files: 100% (12147/12147)\n",
      "Updating files: 100% (12147/12147), done.\n",
      "fatal: unable to checkout working tree\n",
      "warning: Clone succeeded, but checkout failed.\n",
      "You can inspect what was checked out with 'git status'\n",
      "and retry with 'git restore --source=HEAD :/'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!git clone --depth 1 https://github.com/mitmedialab/viznet.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72d5df6",
   "metadata": {},
   "source": [
    "SATO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "439bf1a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Done! 80000 datasets listed in datasets_viznet.csv\n",
      "Last run on: 2025-06-04 13:05:24\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# 1. Define base folders\n",
    "# -------------------------------------------------------------\n",
    "base_path = Path(\"C:/Users/290099c/Documents/viznet-columns/sato/table_data\")\n",
    "viznet_dirs = [\n",
    "    base_path / \"viznet_tables\" / \"webtables1\" / f\"K{i}\" for i in range(5)\n",
    "] + [\n",
    "    base_path / \"viznet_tables\" / \"webtables2\" / f\"K{i}\" for i in range(5)\n",
    "]\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# 2. Collect all dataset filenames\n",
    "# -------------------------------------------------------------\n",
    "viznet_files = []\n",
    "for folder in viznet_dirs:\n",
    "    viznet_files += list(folder.glob(\"*\"))\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# 3. Create dataset records\n",
    "# -------------------------------------------------------------\n",
    "dataset_rows = []\n",
    "for i, file in enumerate(sorted(viznet_files), start=1):\n",
    "    name = file.name\n",
    "    dataset_index = f\"SATO_{i:06d}\"\n",
    "    dataset_rows.append({\n",
    "        \"index\": dataset_index,\n",
    "        \"name\": name,\n",
    "        \"area\": \"Sato-Viznet\",\n",
    "        \"url\": \"\"\n",
    "    })\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# 4. Save to CSV\n",
    "# -------------------------------------------------------------\n",
    "df_datasets = pd.DataFrame(dataset_rows)\n",
    "df_datasets.to_csv(\"datasets_viznet.csv\", index=False)\n",
    "\n",
    "from datetime import datetime\n",
    "print(f\"✅ Done! {len(df_datasets)} datasets listed in datasets_viznet.csv\")\n",
    "print(f\"Last run on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c746556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Processed 1000 / 80000 datasets\n",
      "✅ Processed 3000 / 80000 datasets\n",
      "✅ Processed 4000 / 80000 datasets\n",
      "✅ Processed 5000 / 80000 datasets\n",
      "✅ Processed 6000 / 80000 datasets\n",
      "✅ Processed 7000 / 80000 datasets\n",
      "✅ Processed 8000 / 80000 datasets\n",
      "✅ Processed 9000 / 80000 datasets\n",
      "✅ Processed 10000 / 80000 datasets\n",
      "✅ Processed 11000 / 80000 datasets\n",
      "✅ Processed 12000 / 80000 datasets\n",
      "✅ Processed 13000 / 80000 datasets\n",
      "✅ Processed 14000 / 80000 datasets\n",
      "✅ Processed 15000 / 80000 datasets\n",
      "✅ Processed 16000 / 80000 datasets\n",
      "✅ Processed 17000 / 80000 datasets\n",
      "✅ Processed 18000 / 80000 datasets\n",
      "✅ Processed 19000 / 80000 datasets\n",
      "✅ Processed 20000 / 80000 datasets\n",
      "✅ Processed 21000 / 80000 datasets\n",
      "✅ Processed 22000 / 80000 datasets\n",
      "✅ Processed 23000 / 80000 datasets\n",
      "✅ Processed 24000 / 80000 datasets\n",
      "✅ Processed 25000 / 80000 datasets\n",
      "✅ Processed 26000 / 80000 datasets\n",
      "✅ Processed 27000 / 80000 datasets\n",
      "✅ Processed 28000 / 80000 datasets\n",
      "✅ Processed 29000 / 80000 datasets\n",
      "✅ Processed 30000 / 80000 datasets\n",
      "✅ Processed 31000 / 80000 datasets\n",
      "✅ Processed 32000 / 80000 datasets\n",
      "✅ Processed 33000 / 80000 datasets\n",
      "✅ Processed 35000 / 80000 datasets\n",
      "✅ Processed 36000 / 80000 datasets\n",
      "✅ Processed 37000 / 80000 datasets\n",
      "✅ Processed 38000 / 80000 datasets\n",
      "✅ Processed 39000 / 80000 datasets\n",
      "✅ Processed 40000 / 80000 datasets\n",
      "✅ Processed 41000 / 80000 datasets\n",
      "✅ Processed 42000 / 80000 datasets\n",
      "✅ Processed 43000 / 80000 datasets\n",
      "✅ Processed 44000 / 80000 datasets\n",
      "✅ Processed 45000 / 80000 datasets\n",
      "✅ Processed 46000 / 80000 datasets\n",
      "✅ Processed 47000 / 80000 datasets\n",
      "✅ Processed 48000 / 80000 datasets\n",
      "✅ Processed 49000 / 80000 datasets\n",
      "✅ Processed 50000 / 80000 datasets\n",
      "✅ Processed 51000 / 80000 datasets\n",
      "✅ Processed 52000 / 80000 datasets\n",
      "✅ Processed 53000 / 80000 datasets\n",
      "✅ Processed 54000 / 80000 datasets\n",
      "✅ Processed 55000 / 80000 datasets\n",
      "✅ Processed 56000 / 80000 datasets\n",
      "✅ Processed 57000 / 80000 datasets\n",
      "✅ Processed 58000 / 80000 datasets\n",
      "✅ Processed 59000 / 80000 datasets\n",
      "✅ Processed 60000 / 80000 datasets\n",
      "✅ Processed 61000 / 80000 datasets\n",
      "✅ Processed 62000 / 80000 datasets\n",
      "✅ Processed 63000 / 80000 datasets\n",
      "✅ Processed 64000 / 80000 datasets\n",
      "✅ Processed 65000 / 80000 datasets\n",
      "✅ Processed 66000 / 80000 datasets\n",
      "✅ Processed 67000 / 80000 datasets\n",
      "✅ Processed 68000 / 80000 datasets\n",
      "✅ Processed 69000 / 80000 datasets\n",
      "✅ Processed 70000 / 80000 datasets\n",
      "✅ Processed 71000 / 80000 datasets\n",
      "✅ Processed 72000 / 80000 datasets\n",
      "✅ Processed 73000 / 80000 datasets\n",
      "✅ Processed 74000 / 80000 datasets\n",
      "✅ Processed 75000 / 80000 datasets\n",
      "✅ Processed 76000 / 80000 datasets\n",
      "✅ Processed 77000 / 80000 datasets\n",
      "✅ Processed 78000 / 80000 datasets\n",
      "✅ Processed 79000 / 80000 datasets\n",
      "✅ Processed 80000 / 80000 datasets\n",
      "Last run on: 2025-06-04 16:39:25\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import unicodedata\n",
    "import csv\n",
    "from datetime import datetime\n",
    "\n",
    "# Your working directory\n",
    "base_path = Path(\"C:/Users/290099c/Documents/viznet-columns/sato/table_data\")\n",
    "viznet_dirs = [base_path / \"viznet_tables\" / \"webtables1\" / f\"K{i}\" for i in range(5)] + \\\n",
    "              [base_path / \"viznet_tables\" / \"webtables2\" / f\"K{i}\" for i in range(5)]\n",
    "sato_dirs = [base_path / \"sato_tables\" / \"all\" / f\"K{i}\" for i in range(5)]\n",
    "\n",
    "# Output file\n",
    "output_file = base_path / \"columns_sato_only.csv\"\n",
    "\n",
    "# Gather all files\n",
    "viznet_files = [f for folder in viznet_dirs for f in folder.glob(\"*\")]\n",
    "sato_files = {f.name: f for folder in sato_dirs for f in folder.glob(\"*\")}\n",
    "\n",
    "# Helper to normalize column names\n",
    "def normalize(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = unicodedata.normalize('NFKD', text)\n",
    "    return \"\".join([c for c in text if not unicodedata.combining(c)]).strip().lower()\n",
    "\n",
    "# Write matching results\n",
    "with open(output_file, \"w\", newline='', encoding=\"utf-8\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"index\", \"name\", \"area\", \"Original Column\", \"ID\"])\n",
    "\n",
    "    for i, file in enumerate(sorted(viznet_files), start=1):\n",
    "        file_name = file.name\n",
    "        dataset_index = f\"SATO_{i:06d}\"\n",
    "\n",
    "        if file_name not in sato_files:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            df_viz = pd.read_csv(file, dtype=str, nrows=1)\n",
    "            df_sato = pd.read_csv(sato_files[file_name], dtype=str, nrows=1)\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "        viz_columns = [normalize(c) for c in df_viz.columns]\n",
    "        sato_columns = [normalize(c) for c in df_sato.columns]\n",
    "        original_columns = list(df_viz.columns)\n",
    "\n",
    "        for idx, (col_raw, col_norm) in enumerate(zip(original_columns, viz_columns), start=1):\n",
    "            if col_norm in sato_columns:\n",
    "                writer.writerow([dataset_index, file_name, \"Sato-Viznet\", f\"{idx}.   {col_raw}\", idx])\n",
    "\n",
    "        if i % 1000 == 0 or i == len(viznet_files):\n",
    "            print(f\"✅ Processed {i} / {len(viznet_files)} datasets\")\n",
    "\n",
    "print(f\"Last run on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff40e8eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Processed 1000 datasets...\n",
      "✅ Processed 2000 datasets...\n",
      "✅ Processed 3000 datasets...\n",
      "✅ Processed 4000 datasets...\n",
      "✅ Processed 5000 datasets...\n",
      "✅ Processed 6000 datasets...\n",
      "✅ Processed 7000 datasets...\n",
      "✅ Processed 8000 datasets...\n",
      "✅ Processed 9000 datasets...\n",
      "✅ Processed 10000 datasets...\n",
      "✅ Processed 11000 datasets...\n",
      "✅ Processed 12000 datasets...\n",
      "✅ Processed 13000 datasets...\n",
      "✅ Processed 14000 datasets...\n",
      "✅ Processed 15000 datasets...\n",
      "✅ Processed 16000 datasets...\n",
      "✅ Processed 17000 datasets...\n",
      "✅ Processed 18000 datasets...\n",
      "✅ Processed 19000 datasets...\n",
      "✅ Processed 20000 datasets...\n",
      "✅ Processed 21000 datasets...\n",
      "✅ Processed 22000 datasets...\n",
      "✅ Processed 23000 datasets...\n",
      "✅ Processed 24000 datasets...\n",
      "✅ Processed 25000 datasets...\n",
      "✅ Processed 26000 datasets...\n",
      "✅ Processed 27000 datasets...\n",
      "✅ Processed 28000 datasets...\n",
      "✅ Processed 29000 datasets...\n",
      "✅ Processed 30000 datasets...\n",
      "✅ Processed 31000 datasets...\n",
      "✅ Processed 32000 datasets...\n",
      "✅ Processed 33000 datasets...\n",
      "✅ Processed 34000 datasets...\n",
      "✅ Processed 35000 datasets...\n",
      "✅ Processed 36000 datasets...\n",
      "✅ Processed 37000 datasets...\n",
      "✅ Processed 38000 datasets...\n",
      "✅ Processed 39000 datasets...\n",
      "✅ Processed 40000 datasets...\n",
      "✅ Processed 41000 datasets...\n",
      "✅ Processed 42000 datasets...\n",
      "✅ Processed 43000 datasets...\n",
      "✅ Processed 44000 datasets...\n",
      "✅ Processed 45000 datasets...\n",
      "✅ Processed 46000 datasets...\n",
      "✅ Processed 47000 datasets...\n",
      "✅ Processed 48000 datasets...\n",
      "✅ Processed 49000 datasets...\n",
      "✅ Processed 50000 datasets...\n",
      "✅ Processed 51000 datasets...\n",
      "✅ Processed 52000 datasets...\n",
      "✅ Processed 53000 datasets...\n",
      "✅ Processed 54000 datasets...\n",
      "✅ Processed 55000 datasets...\n",
      "✅ Processed 56000 datasets...\n",
      "✅ Processed 57000 datasets...\n",
      "✅ Processed 58000 datasets...\n",
      "✅ Processed 59000 datasets...\n",
      "✅ Processed 60000 datasets...\n",
      "✅ Processed 61000 datasets...\n",
      "✅ Processed 62000 datasets...\n",
      "✅ Processed 63000 datasets...\n",
      "✅ Processed 64000 datasets...\n",
      "✅ Processed 65000 datasets...\n",
      "✅ Processed 66000 datasets...\n",
      "✅ Processed 67000 datasets...\n",
      "✅ Processed 68000 datasets...\n",
      "✅ Processed 69000 datasets...\n",
      "✅ Processed 70000 datasets...\n",
      "✅ Processed 71000 datasets...\n",
      "✅ Processed 72000 datasets...\n",
      "✅ Processed 73000 datasets...\n",
      "✅ Processed 74000 datasets...\n",
      "✅ Processed 75000 datasets...\n",
      "✅ Processed 76000 datasets...\n",
      "✅ Processed 77000 datasets...\n",
      "✅ Processed 78000 datasets...\n",
      "\n",
      "✅ Saved 115298 annotated columns to columns_sato_only.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import unicodedata\n",
    "\n",
    "# Base paths\n",
    "base_path = Path(\"C:/Users/290099c/Documents/viznet-columns/sato/table_data\")\n",
    "viznet_dirs = [\n",
    "    base_path / \"viznet_tables\" / \"webtables1\" / f\"K{i}\" for i in range(5)\n",
    "] + [\n",
    "    base_path / \"viznet_tables\" / \"webtables2\" / f\"K{i}\" for i in range(5)\n",
    "]\n",
    "sato_dirs = [base_path / \"sato_tables\" / \"all\" / f\"K{i}\" for i in range(5)]\n",
    "\n",
    "# Get all valid sato file names\n",
    "sato_files = {}\n",
    "for folder in sato_dirs:\n",
    "    if folder.exists():\n",
    "        for f in folder.glob(\"*\"):\n",
    "            if f.is_file():\n",
    "                sato_files[f.name] = f\n",
    "\n",
    "# Normalisation helper\n",
    "def normalize(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = unicodedata.normalize('NFKD', text)\n",
    "    text = \"\".join([c for c in text if not unicodedata.combining(c)])\n",
    "    return text.strip().lower()\n",
    "\n",
    "# Process VizNet files\n",
    "rows = []\n",
    "count = 0\n",
    "\n",
    "for viznet_dir in viznet_dirs:\n",
    "    if not viznet_dir.exists():\n",
    "        continue\n",
    "    for file in sorted(viznet_dir.glob(\"*\")):\n",
    "        if not file.is_file():\n",
    "            continue\n",
    "\n",
    "        file_name = file.name\n",
    "        if file_name not in sato_files:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            df_viz = pd.read_csv(file, dtype=str, nrows=1)\n",
    "            viz_columns = [str(c) for c in df_viz.columns]\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Could not read {file_name}: {e}\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            df_sato = pd.read_csv(sato_files[file_name], dtype=str, nrows=1)\n",
    "            sato_columns = [normalize(c) for c in df_sato.columns]\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Could not read Sato file for {file_name}: {e}\")\n",
    "            continue\n",
    "\n",
    "        dataset_index = f\"SATO_{count+1:06d}\"\n",
    "        for i, col in enumerate(viz_columns, start=1):\n",
    "            norm_col = normalize(col)\n",
    "            if norm_col in sato_columns:\n",
    "                rows.append({\n",
    "                    \"index\": dataset_index,\n",
    "                    \"name\": file_name,\n",
    "                    \"area\": \"Sato-Viznet\",\n",
    "                    \"Original Column\": f\"{i}.   {col}\",\n",
    "                    \"ID\": i\n",
    "                })\n",
    "        count += 1\n",
    "        if count % 1000 == 0:\n",
    "            print(f\"✅ Processed {count} datasets...\")\n",
    "\n",
    "# Save result\n",
    "df = pd.DataFrame(rows)\n",
    "df.to_csv(\"columns_sato_only.csv\", index=False)\n",
    "print(f\"\\n✅ Saved {len(df)} annotated columns to columns_sato_only.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6195450c",
   "metadata": {},
   "source": [
    "Garante que os index sao corretos em columns viznet all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5efaaf15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Atualização concluída! Arquivo salvo como columns_viznet_all_updated.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1. Carrega ambos como Excel\n",
    "datasets = pd.read_excel('datasets_viznet.xlsx')           # deve ter colunas 'name' e 'index'\n",
    "columns  = pd.read_excel('columns_viznet_all.xlsx')        # deve ter colunas 'name' e 'index'\n",
    "\n",
    "# 2. Remove espaços extras (opcional, mas recomendado)\n",
    "#datasets['name'] = datasets['name'].astype(str).str.strip()\n",
    "#columns ['name'] = columns ['name'].astype(str).str.strip()\n",
    "\n",
    "# 3. Cria um dicionário name → index\n",
    "map_index = dict(zip(datasets['name'], datasets['index']))\n",
    "\n",
    "# 4. Atualiza a coluna 'index' em columns a partir desse mapeamento\n",
    "columns['index'] = columns['name'].map(map_index)\n",
    "\n",
    "# 5. (Opcional) Lista quais names não bateram\n",
    "missing = columns.loc[columns['index'].isna(), 'name'].unique()\n",
    "if len(missing):\n",
    "    print(\"Names sem correspondência em datasets_viznet.xlsx:\")\n",
    "    for nm in missing:\n",
    "        print(\"  -\", nm)\n",
    "\n",
    "# 6. Salva o resultado num novo Excel\n",
    "columns.to_excel('columns_viznet_all_updated.xlsx', index=False)\n",
    "print(\"Atualização concluída! Arquivo salvo como columns_viznet_all_updated.xlsx\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056eac72",
   "metadata": {},
   "source": [
    "Renamed without updated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0352c772",
   "metadata": {},
   "source": [
    "Checking if all datasets have columns associated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "52fc6eec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶ Total VizNet datasets: 80000\n",
      "▶ Datasets found at least once in columns_viznet_all.csv: 79985\n",
      "▶ Datasets missing in columns_viznet_all.csv: 15\n",
      "\n",
      "The following dataset IDs did NOT appear in columns_viznet_all.csv:\n",
      "['0_1438042986022.41;warc;CC-MAIN-20150728002306-00166-ip-10-236-191-2.ec2.internal.json.gz_367-Fiji RL questions PNG', '0_1438042986022.41;warc;CC-MAIN-20150728002306-00218-ip-10-236-191-2.ec2.internal.json.gz_454-O', '0_1438042986022.41;warc;CC-MAIN-20150728002306-00265-ip-10-236-191-2.ec2.internal.json.gz_362-HoMePaGe', \"0_1438042986022.41;warc;CC-MAIN-20150728002306-00102-ip-10-236-191-2.ec2.internal.json.gz_6-Umphrey's McGee \", '0_1438042986022.41;warc;CC-MAIN-20150728002306-00265-ip-10-236-191-2.ec2.internal.json.gz_363-HoMePaGe', '0_1438042986022.41;warc;CC-MAIN-20150728002306-00297-ip-10-236-191-2.ec2.internal.json.gz_626-Fa', \"0_1438042986022.41;warc;CC-MAIN-20150728002306-00102-ip-10-236-191-2.ec2.internal.json.gz_7-Umphrey's McGee \", '0_1438042986022.41;warc;CC-MAIN-20150728002306-00204-ip-10-236-191-2.ec2.internal.json.gz_389-Tuesday', '0_1438042986022.41;warc;CC-MAIN-20150728002306-00294-ip-10-236-191-2.ec2.internal.json.gz_279-Penrith confirm Gower', '0_1438042987171.38;warc;CC-MAIN-20150728002307-00147-ip-10-236-191-2.ec2.internal.json.gz_332-Union', '0_1438042987171.38;warc;CC-MAIN-20150728002307-00274-ip-10-236-191-2.ec2.internal.json.gz_308-C\\uf03a', '0_1438042987171.38;warc;CC-MAIN-20150728002307-00197-ip-10-236-191-2.ec2.internal.json.gz_411-Tuesday', '0_1438042987171.38;warc;CC-MAIN-20150728002307-00244-ip-10-236-191-2.ec2.internal.json.gz_276-This week on ', '0_1438042987171.38;warc;CC-MAIN-20150728002307-00331-ip-10-236-191-2.ec2.internal.json.gz_319-Griffin', '0_1438042987171.38;warc;CC-MAIN-20150728002307-00154-ip-10-236-191-2.ec2.internal.json.gz_2477-Joyrin Sultana - Panda']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1. Load the two CSVs\n",
    "#    - Adjust the column names below if yours differ.\n",
    "df_datasets = pd.read_csv(\"datasets_viznet.csv\")         # 80 000 rows, one per dataset\n",
    "df_columns  = pd.read_csv(\"columns_viznet_all.csv\")      # 575 013 rows, one per column\n",
    "\n",
    "# If the dataset‐ID column is called something else, replace 'name' below.\n",
    "# For example, if your CSV uses 'dataset_id', change both to 'dataset_id'.\n",
    "dataset_ids = df_datasets[\"name\"].astype(str).unique()\n",
    "column_ids  = df_columns[\"name\"].astype(str).unique()\n",
    "\n",
    "# 2. Check presence\n",
    "in_columns = pd.Series(dataset_ids).isin(column_ids)\n",
    "\n",
    "# 3. Report any missing dataset IDs\n",
    "missing_ids = pd.Series(dataset_ids)[~in_columns]\n",
    "print(f\"▶ Total VizNet datasets: {len(dataset_ids)}\")\n",
    "print(f\"▶ Datasets found at least once in columns_viznet_all.csv: {in_columns.sum()}\")\n",
    "print(f\"▶ Datasets missing in columns_viznet_all.csv: {len(missing_ids)}\")\n",
    "\n",
    "if len(missing_ids) > 0:\n",
    "    print(\"\\nThe following dataset IDs did NOT appear in columns_viznet_all.csv:\")\n",
    "    print(missing_ids.tolist())\n",
    "else:\n",
    "    print(\"\\nAll VizNet dataset IDs appear at least once in columns_viznet_all.csv.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ad44b6",
   "metadata": {},
   "source": [
    "Freq distro for SATO columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448e7b38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 column names:\n",
      "    Column Name  Frequency\n",
      "0          name      15827\n",
      "1   description      11144\n",
      "2          team      10348\n",
      "3           age       7171\n",
      "4          type       6526\n",
      "5      location       5340\n",
      "6          year       5293\n",
      "7          city       5281\n",
      "8          rank       4856\n",
      "9        status       4704\n",
      "10        state       4357\n",
      "11     category       3988\n",
      "12         code       2697\n",
      "13         club       2182\n",
      "14       artist       2136\n",
      "15       result       2086\n",
      "16      country       1714\n",
      "17       weight       1592\n",
      "18     position       1558\n",
      "19      company       1518\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import unicodedata\n",
    "\n",
    "# Load the file\n",
    "df = pd.read_csv(\"columns_sato_only.csv\")\n",
    "\n",
    "# Clean the column name from \"1.   Country\" → \"Country\"\n",
    "df[\"Cleaned Column\"] = df[\"Original Column\"].astype(str).str.extract(r\"^\\s*\\d+\\.\\s*(.*)$\", expand=False)\n",
    "\n",
    "# Normalise (remove accents and make lowercase)\n",
    "def normalize(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    text = unicodedata.normalize(\"NFKD\", str(text))\n",
    "    return \"\".join(c for c in text if not unicodedata.combining(c)).strip().lower()\n",
    "\n",
    "df[\"Normalized Column\"] = df[\"Cleaned Column\"].apply(normalize)\n",
    "\n",
    "# Compute frequency\n",
    "frequency = df[\"Normalized Column\"].value_counts().reset_index()\n",
    "frequency.columns = [\"Column Name\", \"Frequency\"]\n",
    "\n",
    "# Save to CSV (optional)\n",
    "frequency.to_csv(\"column_name_frequency_sato.csv\", index=False)\n",
    "\n",
    "# Show top 20 (optional)\n",
    "print(\"Top 20 column names:\")\n",
    "print(frequency.head(20))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
