{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It started on: 2025-06-09 11:30:53\n",
      "v\n",
      "Using Viznet Filtered Datasets\n",
      "         index  name  area  url\n",
      "0  SATO_000001   NaN   NaN  NaN\n",
      "1  SATO_000002   NaN   NaN  NaN\n",
      "2  SATO_000003   NaN   NaN  NaN\n",
      "3  SATO_000004   NaN   NaN  NaN\n",
      "4  SATO_000005   NaN   NaN  NaN\n",
      "Original df shape: (74909, 5)\n",
      "AFTER CLEAN   dataset_index  name         area        Original Column ID  \\\n",
      "0   SATO_000001   NaN  Sato-Viznet                1.   ID  1   \n",
      "1   SATO_000001   NaN  Sato-Viznet           2.   Country  2   \n",
      "2   SATO_000001   NaN  Sato-Viznet        3.   Unnamed: 2  3   \n",
      "3   SATO_000001   NaN  Sato-Viznet  4.   Complete-o-meter  4   \n",
      "4   SATO_000001   NaN  Sato-Viznet           5.   Courses  5   \n",
      "\n",
      "             Column Description  \n",
      "0                ID         NaN  \n",
      "1           Country         NaN  \n",
      "2           Unnamed           2  \n",
      "3  Complete-o-meter         NaN  \n",
      "4           Courses         NaN  \n",
      "      dataset_index  name         area        Original Column ID  \\\n",
      "0       SATO_000001   NaN  Sato-Viznet                1.   ID  1   \n",
      "1       SATO_000001   NaN  Sato-Viznet           2.   Country  2   \n",
      "2       SATO_000001   NaN  Sato-Viznet        3.   Unnamed: 2  3   \n",
      "3       SATO_000001   NaN  Sato-Viznet  4.   Complete-o-meter  4   \n",
      "4       SATO_000001   NaN  Sato-Viznet           5.   Courses  5   \n",
      "...             ...   ...          ...                    ... ..   \n",
      "74904   SATO_079969   NaN  Sato-Viznet              2.   Name  2   \n",
      "74905   SATO_079969   NaN  Sato-Viznet          3.   Friendly  3   \n",
      "74906   SATO_079969   NaN  Sato-Viznet           4.   Honored  4   \n",
      "74907   SATO_079969   NaN  Sato-Viznet           5.   Revered  5   \n",
      "74908   SATO_079969   NaN  Sato-Viznet           6.   Exalted  6   \n",
      "\n",
      "                 Column Description     CleanedColumn  \n",
      "0                    ID         NaN                id  \n",
      "1               Country         NaN           country  \n",
      "2               Unnamed           2           unnamed  \n",
      "3      Complete-o-meter         NaN  complete o meter  \n",
      "4               Courses         NaN           courses  \n",
      "...                 ...         ...               ...  \n",
      "74904              Name         NaN              name  \n",
      "74905          Friendly         NaN          friendly  \n",
      "74906           Honored         NaN           honored  \n",
      "74907           Revered         NaN           revered  \n",
      "74908           Exalted         NaN           exalted  \n",
      "\n",
      "[74909 rows x 8 columns]\n",
      "✅ Processed 1000 rows...\n",
      "✅ Processed 2000 rows...\n",
      "✅ Processed 3000 rows...\n",
      "✅ Processed 4000 rows...\n",
      "✅ Processed 5000 rows...\n",
      "✅ Processed 6000 rows...\n",
      "✅ Processed 7000 rows...\n",
      "✅ Processed 8000 rows...\n",
      "✅ Processed 9000 rows...\n",
      "✅ Processed 10000 rows...\n",
      "✅ Processed 11000 rows...\n",
      "✅ Processed 12000 rows...\n",
      "✅ Processed 13000 rows...\n",
      "✅ Processed 14000 rows...\n",
      "✅ Processed 15000 rows...\n",
      "✅ Processed 16000 rows...\n",
      "✅ Processed 17000 rows...\n",
      "✅ Processed 18000 rows...\n",
      "✅ Processed 19000 rows...\n",
      "✅ Processed 20000 rows...\n",
      "✅ Processed 21000 rows...\n",
      "✅ Processed 22000 rows...\n",
      "✅ Processed 23000 rows...\n",
      "✅ Processed 24000 rows...\n",
      "✅ Processed 25000 rows...\n",
      "✅ Processed 26000 rows...\n",
      "✅ Processed 27000 rows...\n",
      "✅ Processed 28000 rows...\n",
      "✅ Processed 29000 rows...\n",
      "✅ Processed 30000 rows...\n",
      "✅ Processed 31000 rows...\n",
      "✅ Processed 32000 rows...\n",
      "✅ Processed 33000 rows...\n",
      "✅ Processed 34000 rows...\n",
      "✅ Processed 35000 rows...\n",
      "✅ Processed 36000 rows...\n",
      "✅ Processed 37000 rows...\n",
      "✅ Processed 38000 rows...\n",
      "✅ Processed 39000 rows...\n",
      "✅ Processed 40000 rows...\n",
      "✅ Processed 41000 rows...\n",
      "✅ Processed 42000 rows...\n",
      "✅ Processed 43000 rows...\n",
      "✅ Processed 44000 rows...\n",
      "✅ Processed 45000 rows...\n",
      "✅ Processed 46000 rows...\n",
      "✅ Processed 47000 rows...\n",
      "✅ Processed 48000 rows...\n",
      "✅ Processed 49000 rows...\n",
      "✅ Processed 50000 rows...\n",
      "✅ Processed 51000 rows...\n",
      "✅ Processed 52000 rows...\n",
      "✅ Processed 53000 rows...\n",
      "✅ Processed 54000 rows...\n",
      "✅ Processed 55000 rows...\n",
      "✅ Processed 56000 rows...\n",
      "✅ Processed 57000 rows...\n",
      "✅ Processed 58000 rows...\n",
      "✅ Processed 59000 rows...\n",
      "✅ Processed 60000 rows...\n",
      "✅ Processed 61000 rows...\n",
      "✅ Processed 62000 rows...\n",
      "✅ Processed 63000 rows...\n",
      "✅ Processed 64000 rows...\n",
      "✅ Processed 65000 rows...\n",
      "✅ Processed 66000 rows...\n",
      "✅ Processed 67000 rows...\n",
      "✅ Processed 68000 rows...\n",
      "✅ Processed 69000 rows...\n",
      "✅ Processed 70000 rows...\n",
      "✅ Processed 71000 rows...\n",
      "✅ Processed 72000 rows...\n",
      "✅ Processed 73000 rows...\n",
      "✅ Processed 74000 rows...\n",
      "✅ Processed 1000 rows...\n",
      "✅ Processed 2000 rows...\n",
      "✅ Processed 3000 rows...\n",
      "✅ Processed 4000 rows...\n",
      "✅ Processed 5000 rows...\n",
      "✅ Processed 6000 rows...\n",
      "✅ Processed 7000 rows...\n",
      "✅ Processed 8000 rows...\n",
      "✅ Processed 9000 rows...\n",
      "✅ Processed 10000 rows...\n",
      "✅ Processed 11000 rows...\n",
      "✅ Processed 12000 rows...\n",
      "✅ Processed 13000 rows...\n",
      "✅ Processed 14000 rows...\n",
      "✅ Processed 15000 rows...\n",
      "✅ Processed 16000 rows...\n",
      "✅ Processed 17000 rows...\n",
      "✅ Processed 18000 rows...\n",
      "✅ Processed 19000 rows...\n",
      "✅ Processed 20000 rows...\n",
      "✅ Processed 21000 rows...\n",
      "✅ Processed 22000 rows...\n",
      "✅ Processed 23000 rows...\n",
      "✅ Processed 24000 rows...\n",
      "✅ Processed 25000 rows...\n",
      "✅ Processed 26000 rows...\n",
      "✅ Processed 27000 rows...\n",
      "✅ Processed 28000 rows...\n",
      "✅ Processed 29000 rows...\n",
      "✅ Processed 30000 rows...\n",
      "✅ Processed 31000 rows...\n",
      "✅ Processed 32000 rows...\n",
      "✅ Processed 33000 rows...\n",
      "✅ Processed 34000 rows...\n",
      "✅ Processed 35000 rows...\n",
      "✅ Processed 36000 rows...\n",
      "✅ Processed 37000 rows...\n",
      "✅ Processed 38000 rows...\n",
      "✅ Processed 39000 rows...\n",
      "✅ Processed 40000 rows...\n",
      "✅ Processed 41000 rows...\n",
      "✅ Processed 42000 rows...\n",
      "✅ Processed 43000 rows...\n",
      "✅ Processed 44000 rows...\n",
      "✅ Processed 45000 rows...\n",
      "✅ Processed 46000 rows...\n",
      "✅ Processed 47000 rows...\n",
      "✅ Processed 48000 rows...\n",
      "✅ Processed 49000 rows...\n",
      "✅ Processed 50000 rows...\n",
      "✅ Processed 51000 rows...\n",
      "✅ Processed 52000 rows...\n",
      "✅ Processed 53000 rows...\n",
      "✅ Processed 54000 rows...\n",
      "✅ Processed 55000 rows...\n",
      "✅ Processed 56000 rows...\n",
      "✅ Processed 57000 rows...\n",
      "✅ Processed 58000 rows...\n",
      "✅ Processed 59000 rows...\n",
      "✅ Processed 60000 rows...\n",
      "✅ Processed 61000 rows...\n",
      "✅ Processed 62000 rows...\n",
      "✅ Processed 63000 rows...\n",
      "✅ Processed 64000 rows...\n",
      "✅ Processed 65000 rows...\n",
      "✅ Processed 66000 rows...\n",
      "✅ Processed 67000 rows...\n",
      "✅ Processed 68000 rows...\n",
      "✅ Processed 69000 rows...\n",
      "✅ Processed 70000 rows...\n",
      "✅ Processed 71000 rows...\n",
      "✅ Processed 72000 rows...\n",
      "✅ Processed 73000 rows...\n",
      "✅ Processed 74000 rows...\n",
      "Class distribution after SMOTE:\n",
      "IDcolumn         17360\n",
      "country          17360\n",
      "heartrate        17360\n",
      "phone            17360\n",
      "E-mailformat     17360\n",
      "state            17360\n",
      "latitude         17360\n",
      "longitude        17360\n",
      "postalcode       17360\n",
      "money            17360\n",
      "month            17360\n",
      "datetime         17360\n",
      "binary           17360\n",
      "weekday          17360\n",
      "ph               17360\n",
      "hour             17360\n",
      "angle            17360\n",
      "IPformat         17360\n",
      "URLformat        17360\n",
      "week             17360\n",
      "bloodpressure    17360\n",
      "string           17360\n",
      "nan              17360\n",
      "categorical      17360\n",
      "name             17360\n",
      "numerical>=0     17360\n",
      "date             17360\n",
      "numerical        17360\n",
      "street           17360\n",
      "city             17360\n",
      "percentage       17360\n",
      "year             17360\n",
      "age              17360\n",
      "day              17360\n",
      "time             17360\n",
      "modelname        17360\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Training RandomForestClassifier...\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "Best parameters for RandomForestClassifier: {'max_depth': None, 'min_samples_split': 5, 'n_estimators': 200}\n",
      "Classification Report for RandomForestClassifier:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      " E-mailformat       1.00      1.00      1.00      3472\n",
      "     IDcolumn       1.00      1.00      1.00      3472\n",
      "     IPformat       1.00      1.00      1.00      3472\n",
      "    URLformat       0.99      1.00      1.00      3472\n",
      "          age       1.00      1.00      1.00      3472\n",
      "        angle       1.00      1.00      1.00      3472\n",
      "       binary       1.00      0.98      0.99      3472\n",
      "bloodpressure       1.00      1.00      1.00      3472\n",
      "  categorical       0.99      0.96      0.97      3472\n",
      "         city       0.99      1.00      0.99      3472\n",
      "      country       1.00      1.00      1.00      3472\n",
      "         date       1.00      0.95      0.98      3472\n",
      "     datetime       0.98      1.00      0.99      3472\n",
      "          day       1.00      1.00      1.00      3472\n",
      "    heartrate       1.00      1.00      1.00      3472\n",
      "         hour       1.00      1.00      1.00      3472\n",
      "     latitude       1.00      1.00      1.00      3472\n",
      "    longitude       1.00      1.00      1.00      3472\n",
      "    modelname       1.00      1.00      1.00      3472\n",
      "        money       1.00      0.99      0.99      3472\n",
      "        month       1.00      1.00      1.00      3472\n",
      "         name       1.00      0.98      0.99      3472\n",
      "          nan       0.53      1.00      0.69      3472\n",
      "    numerical       0.99      0.96      0.97      3472\n",
      " numerical>=0       0.99      0.95      0.97      3472\n",
      "   percentage       0.95      0.36      0.52      3472\n",
      "           ph       1.00      1.00      1.00      3472\n",
      "        phone       1.00      1.00      1.00      3472\n",
      "   postalcode       1.00      0.99      1.00      3472\n",
      "        state       1.00      1.00      1.00      3472\n",
      "       street       1.00      1.00      1.00      3472\n",
      "       string       0.99      0.94      0.96      3472\n",
      "         time       1.00      0.96      0.98      3472\n",
      "         week       1.00      1.00      1.00      3472\n",
      "      weekday       1.00      1.00      1.00      3472\n",
      "         year       1.00      1.00      1.00      3472\n",
      "\n",
      "     accuracy                           0.97    124992\n",
      "    macro avg       0.98      0.97      0.97    124992\n",
      " weighted avg       0.98      0.97      0.97    124992\n",
      "\n",
      "DataFrame for RandomForestClassifier saved to AnalysedColumnsV_with_Predictions_RandomForestClassifier.xlsx\n",
      "\n",
      "Training LogisticRegression...\n",
      "Fitting 3 folds for each of 2 candidates, totalling 6 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\290099c\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of f AND g EVALUATIONS EXCEEDS LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for LogisticRegression: {'C': 0.1, 'max_iter': 20000, 'solver': 'lbfgs', 'warm_start': True}\n",
      "Classification Report for LogisticRegression:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      " E-mailformat       1.00      1.00      1.00      3472\n",
      "     IDcolumn       1.00      1.00      1.00      3472\n",
      "     IPformat       1.00      0.99      0.99      3472\n",
      "    URLformat       1.00      0.98      0.99      3472\n",
      "          age       1.00      1.00      1.00      3472\n",
      "        angle       1.00      1.00      1.00      3472\n",
      "       binary       1.00      0.97      0.98      3472\n",
      "bloodpressure       1.00      1.00      1.00      3472\n",
      "  categorical       0.98      0.95      0.97      3472\n",
      "         city       0.99      1.00      0.99      3472\n",
      "      country       1.00      1.00      1.00      3472\n",
      "         date       1.00      0.94      0.97      3472\n",
      "     datetime       0.98      1.00      0.99      3472\n",
      "          day       1.00      1.00      1.00      3472\n",
      "    heartrate       1.00      1.00      1.00      3472\n",
      "         hour       1.00      1.00      1.00      3472\n",
      "     latitude       1.00      1.00      1.00      3472\n",
      "    longitude       1.00      1.00      1.00      3472\n",
      "    modelname       1.00      1.00      1.00      3472\n",
      "        money       1.00      0.99      0.99      3472\n",
      "        month       1.00      1.00      1.00      3472\n",
      "         name       0.99      0.97      0.98      3472\n",
      "          nan       0.53      1.00      0.69      3472\n",
      "    numerical       0.96      0.95      0.95      3472\n",
      " numerical>=0       0.97      0.95      0.96      3472\n",
      "   percentage       0.92      0.31      0.46      3472\n",
      "           ph       1.00      1.00      1.00      3472\n",
      "        phone       1.00      1.00      1.00      3472\n",
      "   postalcode       0.98      1.00      0.99      3472\n",
      "        state       1.00      1.00      1.00      3472\n",
      "       street       1.00      1.00      1.00      3472\n",
      "       string       0.95      0.94      0.94      3472\n",
      "         time       1.00      0.96      0.98      3472\n",
      "         week       1.00      1.00      1.00      3472\n",
      "      weekday       1.00      1.00      1.00      3472\n",
      "         year       1.00      1.00      1.00      3472\n",
      "\n",
      "     accuracy                           0.97    124992\n",
      "    macro avg       0.98      0.97      0.97    124992\n",
      " weighted avg       0.98      0.97      0.97    124992\n",
      "\n",
      "DataFrame for LogisticRegression saved to AnalysedColumnsV_with_Predictions_LogisticRegression.xlsx\n",
      "\n",
      "Training GradientBoostingClassifier...\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Best parameters for GradientBoostingClassifier: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 100}\n",
      "Classification Report for GradientBoostingClassifier:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      " E-mailformat       1.00      1.00      1.00      3472\n",
      "     IDcolumn       1.00      1.00      1.00      3472\n",
      "     IPformat       1.00      1.00      1.00      3472\n",
      "    URLformat       0.99      1.00      1.00      3472\n",
      "          age       1.00      1.00      1.00      3472\n",
      "        angle       1.00      1.00      1.00      3472\n",
      "       binary       1.00      0.97      0.98      3472\n",
      "bloodpressure       1.00      1.00      1.00      3472\n",
      "  categorical       1.00      0.82      0.90      3472\n",
      "         city       0.99      0.99      0.99      3472\n",
      "      country       1.00      1.00      1.00      3472\n",
      "         date       1.00      0.95      0.97      3472\n",
      "     datetime       0.98      1.00      0.99      3472\n",
      "          day       1.00      1.00      1.00      3472\n",
      "    heartrate       1.00      1.00      1.00      3472\n",
      "         hour       1.00      1.00      1.00      3472\n",
      "     latitude       1.00      1.00      1.00      3472\n",
      "    longitude       1.00      1.00      1.00      3472\n",
      "    modelname       1.00      1.00      1.00      3472\n",
      "        money       1.00      0.99      0.99      3472\n",
      "        month       1.00      1.00      1.00      3472\n",
      "         name       1.00      0.96      0.98      3472\n",
      "          nan       0.43      1.00      0.60      3472\n",
      "    numerical       0.98      0.88      0.93      3472\n",
      " numerical>=0       0.99      0.88      0.93      3472\n",
      "   percentage       0.88      0.34      0.49      3472\n",
      "           ph       1.00      1.00      1.00      3472\n",
      "        phone       1.00      1.00      1.00      3472\n",
      "   postalcode       0.99      1.00      0.99      3472\n",
      "        state       1.00      1.00      1.00      3472\n",
      "       street       1.00      1.00      1.00      3472\n",
      "       string       1.00      0.79      0.88      3472\n",
      "         time       1.00      0.96      0.98      3472\n",
      "         week       1.00      1.00      1.00      3472\n",
      "      weekday       1.00      1.00      1.00      3472\n",
      "         year       1.00      1.00      1.00      3472\n",
      "\n",
      "     accuracy                           0.96    124992\n",
      "    macro avg       0.98      0.96      0.96    124992\n",
      " weighted avg       0.98      0.96      0.96    124992\n",
      "\n",
      "DataFrame for GradientBoostingClassifier saved to AnalysedColumnsV_with_Predictions_GradientBoostingClassifier.xlsx\n",
      "\n",
      "Training KNeighborsClassifier...\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\290099c\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1102: UserWarning: One or more of the test scores are non-finite: [nan nan nan nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for KNeighborsClassifier: {'n_neighbors': 3, 'weights': 'uniform'}\n",
      "Classification Report for KNeighborsClassifier:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      " E-mailformat       1.00      0.99      1.00      3472\n",
      "     IDcolumn       1.00      1.00      1.00      3472\n",
      "     IPformat       0.98      0.99      0.99      3472\n",
      "    URLformat       0.99      1.00      0.99      3472\n",
      "          age       1.00      1.00      1.00      3472\n",
      "        angle       1.00      1.00      1.00      3472\n",
      "       binary       0.99      0.97      0.98      3472\n",
      "bloodpressure       1.00      1.00      1.00      3472\n",
      "  categorical       0.98      0.94      0.96      3472\n",
      "         city       0.99      0.99      0.99      3472\n",
      "      country       1.00      1.00      1.00      3472\n",
      "         date       0.91      0.97      0.94      3472\n",
      "     datetime       0.99      0.91      0.95      3472\n",
      "          day       1.00      1.00      1.00      3472\n",
      "    heartrate       1.00      1.00      1.00      3472\n",
      "         hour       1.00      1.00      1.00      3472\n",
      "     latitude       1.00      1.00      1.00      3472\n",
      "    longitude       1.00      1.00      1.00      3472\n",
      "    modelname       1.00      1.00      1.00      3472\n",
      "        money       1.00      0.99      0.99      3472\n",
      "        month       1.00      1.00      1.00      3472\n",
      "         name       1.00      0.97      0.98      3472\n",
      "          nan       0.79      0.05      0.10      3472\n",
      "    numerical       0.98      0.95      0.97      3472\n",
      " numerical>=0       0.95      0.95      0.95      3472\n",
      "   percentage       0.44      0.94      0.60      3472\n",
      "           ph       1.00      1.00      1.00      3472\n",
      "        phone       0.99      1.00      1.00      3472\n",
      "   postalcode       0.99      1.00      0.99      3472\n",
      "        state       1.00      1.00      1.00      3472\n",
      "       street       1.00      1.00      1.00      3472\n",
      "       string       0.99      0.93      0.96      3472\n",
      "         time       1.00      0.96      0.98      3472\n",
      "         week       1.00      1.00      1.00      3472\n",
      "      weekday       1.00      1.00      1.00      3472\n",
      "         year       1.00      1.00      1.00      3472\n",
      "\n",
      "     accuracy                           0.96    124992\n",
      "    macro avg       0.97      0.96      0.95    124992\n",
      " weighted avg       0.97      0.96      0.95    124992\n",
      "\n",
      "DataFrame for KNeighborsClassifier saved to AnalysedColumnsV_with_Predictions_KNeighborsClassifier.xlsx\n",
      "\n",
      "Training LinearSVC...\n",
      "Fitting 3 folds for each of 2 candidates, totalling 6 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\290099c\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for LinearSVC: {'C': 0.1, 'dual': 'auto', 'max_iter': 20000}\n",
      "Classification Report for LinearSVC:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      " E-mailformat       1.00      1.00      1.00      3472\n",
      "     IDcolumn       1.00      1.00      1.00      3472\n",
      "     IPformat       1.00      0.99      0.99      3472\n",
      "    URLformat       0.99      1.00      1.00      3472\n",
      "          age       1.00      1.00      1.00      3472\n",
      "        angle       1.00      1.00      1.00      3472\n",
      "       binary       1.00      0.97      0.98      3472\n",
      "bloodpressure       1.00      1.00      1.00      3472\n",
      "  categorical       0.98      0.94      0.96      3472\n",
      "         city       0.99      0.99      0.99      3472\n",
      "      country       1.00      1.00      1.00      3472\n",
      "         date       0.89      0.95      0.92      3472\n",
      "     datetime       0.99      0.89      0.94      3472\n",
      "          day       1.00      1.00      1.00      3472\n",
      "    heartrate       1.00      1.00      1.00      3472\n",
      "         hour       1.00      1.00      1.00      3472\n",
      "     latitude       1.00      1.00      1.00      3472\n",
      "    longitude       1.00      1.00      1.00      3472\n",
      "    modelname       0.99      1.00      1.00      3472\n",
      "        money       1.00      0.99      0.99      3472\n",
      "        month       1.00      1.00      1.00      3472\n",
      "         name       0.99      0.97      0.98      3472\n",
      "          nan       0.83      0.07      0.12      3472\n",
      "    numerical       0.93      0.95      0.94      3472\n",
      " numerical>=0       0.34      0.96      0.51      3472\n",
      "   percentage       0.91      0.29      0.44      3472\n",
      "           ph       1.00      1.00      1.00      3472\n",
      "        phone       1.00      0.99      0.99      3472\n",
      "   postalcode       0.98      1.00      0.99      3472\n",
      "        state       1.00      1.00      1.00      3472\n",
      "       street       1.00      1.00      1.00      3472\n",
      "       string       0.97      0.93      0.95      3472\n",
      "         time       1.00      0.96      0.98      3472\n",
      "         week       1.00      1.00      1.00      3472\n",
      "      weekday       1.00      1.00      1.00      3472\n",
      "         year       1.00      1.00      1.00      3472\n",
      "\n",
      "     accuracy                           0.94    124992\n",
      "    macro avg       0.97      0.94      0.94    124992\n",
      " weighted avg       0.97      0.94      0.94    124992\n",
      "\n",
      "DataFrame for LinearSVC saved to AnalysedColumnsV_with_Predictions_LinearSVC.xlsx\n",
      "Vectorizer saved for future use\n",
      "Last run on: 2025-06-10 14:45:11\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import scipy.sparse as sp\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier, NearestNeighbors\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "\n",
    "# Custom Tokenizer\n",
    "def custom_tokenizer(text):\n",
    "    if pd.isna(text):\n",
    "        return []\n",
    "    tokens = re.findall(r'\\w+', str(text).lower())\n",
    "    return [subtok for token in tokens for subtok in re.findall(r'[a-z]+|\\d+', token)]\n",
    "\n",
    "# Custom TfidfVectorizer\n",
    "class CustomTfidfVectorizer(TfidfVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        return lambda doc: custom_tokenizer(doc)\n",
    "\n",
    "def clean_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # 1. Extract the ID from the column name \n",
    "    df['ID'] = df['Original Column'].str.extract(r'(\\d+)\\.')\n",
    "\n",
    "    # 2. Extract the Column name up to the first colon, slash, or parenthesis\n",
    "    df['Column'] = df['Original Column'].str.extract(r'\\d+\\.\\s*([^/(:]+)')\n",
    "\n",
    "    # 3. Extract everything after the first colon, slash, or parenthesis as the full description\n",
    "    df['Description'] = df['Original Column'].str.extract(r'\\d+\\.\\s*[^/(:]+\\s*[:(/](.*)')[0]\n",
    "\n",
    "    # 4. For cases where Description is None,\n",
    "    #    extract the content within parentheses\n",
    "    mask = df['Description'].isnull()\n",
    "    df.loc[mask, 'Description'] = df.loc[mask, 'Original Column'].str.extract(r'\\(([^)]+)\\)')\n",
    "\n",
    "    # 5. Trim spaces from all string columns\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == \"object\" and col != 'dataset_index':\n",
    "            df[col] = df[col].str.strip()\n",
    "\n",
    "    # 6. Replace empty strings with pd.NA  ← changed from None to pd.NA to avoid downcasting warning\n",
    "    df = df.replace(r'^\\s*$', pd.NA, regex=True)\n",
    "    df = df.infer_objects(copy=False)\n",
    "    \n",
    "    # 7. Ensure Description is object dtype before applying .str.replace  ← added to avoid incompatible dtype warning\n",
    "    df['Description'] = df['Description'].astype(object)\n",
    "\n",
    "    # 8. Only apply .str.replace on non-null Description rows,\n",
    "    #    converting to string first to avoid errors\n",
    "    mask_desc = df['Description'].notnull()\n",
    "    df.loc[mask_desc, 'Description'] = (\n",
    "        df.loc[mask_desc, 'Description']\n",
    "          .astype(str)\n",
    "          .str.replace(r'\\(', ' ', regex=True)\n",
    "          .str.replace(r'\\)', '', regex=True)\n",
    "    )\n",
    "\n",
    "    # 9. Reorder columns to ensure ID comes before Column\n",
    "    columns_order = ['dataset_index', 'name', 'area', 'Original Column', 'ID', 'Column', 'Description']\n",
    "    columns_order = [col for col in columns_order if col in df.columns]\n",
    "    df = df[columns_order]\n",
    "\n",
    "    return df\n",
    "\n",
    "def replace_abbreviations(text, abbreviations_dict):\n",
    "    tokens = re.findall(\n",
    "        r'\\b\\w+(?:[-=]\\w+)*\\b'          # Words, with hyphens/equals (like 0-4, 1=NE)\n",
    "        r'|[=]'                         # Standalone equal signs\n",
    "        r'|[°º][CFK]'                   # °C, ºF, etc.\n",
    "        r'|[°º\\-]+'                     # Standalone degree/hyphens\n",
    "        r'|[a-zA-Z]+/[a-zA-Z]+(?:[²³μµ]?)'   # mg/dL, kg/m², µg/m³, cm², etc.\n",
    "        r'|\\d+[²³]?'                    # 10², 5³ (numbers with superscripts)\n",
    "        r'|%'                           # <- add this line to capture % as a token\n",
    "        , text\n",
    "    )\n",
    "    replaced_tokens = [abbreviations_dict.get(token.lower(), token) for token in tokens]\n",
    "    return \" \".join(replaced_tokens)\n",
    "\n",
    "def preprocess_columns(df, abbreviations_dict):\n",
    "    # Only convert \"Column\" to string type for non-null values and then to lowercase to create \"CleanedColumn\"\n",
    "    mask = df['Column'].notna()\n",
    "\n",
    "    df.loc[mask, 'Column'] = df.loc[mask, 'Column'].str.replace('#', 'number', regex=False)\n",
    "    \n",
    "    df.loc[mask, 'CleanedColumn'] = df.loc[mask, 'Column'].astype(str).apply(split_camel_case)\n",
    "    \n",
    "    # Remove '-' and '_' characters only for non-null values\n",
    "    mask = df['CleanedColumn'].notna()\n",
    "    df.loc[mask, 'CleanedColumn'] = df.loc[mask, 'CleanedColumn'].str.replace('[-_]', ' ', regex=True)\n",
    "\n",
    "    # Apply abbreviations dictionary to Description\n",
    "    mask = df['Description'].notna()\n",
    "    df.loc[mask, 'Description'] = df.loc[mask, 'Description'].apply(lambda x: replace_abbreviations(x, abbreviations_dict))\n",
    " \n",
    "    return df\n",
    "\n",
    "def split_camel_case(s):\n",
    "    # Special case for 'pH'\n",
    "    if s.lower() == 'ph':\n",
    "        return 'ph'\n",
    "    # Split on separators\n",
    "    tokens = re.split(r'[\\s._\\-]+', s)\n",
    "    processed_tokens = []\n",
    "\n",
    "    for token in tokens:\n",
    "        if not token:\n",
    "            continue\n",
    "        # If all uppercase or digits, keep as is\n",
    "        if token.isupper() or token.isdigit():\n",
    "            processed_tokens.append(token.lower())\n",
    "            continue\n",
    "        # If all-uppercase+digits, keep as is\n",
    "        if re.match(r'^[A-Z]{2,}\\d+$', token):\n",
    "            processed_tokens.append(token.lower())\n",
    "            continue\n",
    "\n",
    "        # Character-by-character scan\n",
    "        words = []\n",
    "        current = ''\n",
    "        i = 0\n",
    "        while i < len(token):\n",
    "            c = token[i]\n",
    "            if current == '':\n",
    "                current = c\n",
    "            elif (\n",
    "                # lower->upper transition\n",
    "                (current[-1].islower() and c.isupper()) or\n",
    "                # digit->letter or letter->digit\n",
    "                (current[-1].isdigit() and c.isalpha()) or\n",
    "                (current[-1].isalpha() and c.isdigit())\n",
    "            ):\n",
    "                words.append(current)\n",
    "                current = c\n",
    "            else:\n",
    "                current += c\n",
    "            i += 1\n",
    "        if current:\n",
    "            words.append(current)\n",
    "\n",
    "        # Now check if the last two or more are all caps: join them!\n",
    "        if len(words) >= 2 and all(w.isupper() for w in words[-2:]) and len(''.join(words[-2:])) >= 2:\n",
    "            # Join last all-cap runs\n",
    "            n = len(words) - 1\n",
    "            while n > 0 and words[n].isupper():\n",
    "                n -= 1\n",
    "            # All uppercase words from n+1 to end are the tail\n",
    "            head = words[:n+1]\n",
    "            tail = ''.join(words[n+1:])\n",
    "            processed_tokens.extend([w.lower() for w in head if w])\n",
    "            if tail:\n",
    "                processed_tokens.append(tail.lower())\n",
    "        else:\n",
    "            processed_tokens.extend([w.lower() for w in words if w])\n",
    "\n",
    "    return ' '.join(processed_tokens)\n",
    "\n",
    "def match_with_plural(token, dictionary):\n",
    "    # Attempt direct match first\n",
    "    if token in dictionary:\n",
    "        return dictionary[token]\n",
    "    # Check for 'ies' -> 'y' (families -> family)\n",
    "    if token.endswith('ies') and len(token) > 3:\n",
    "        singular = token[:-3] + 'y'\n",
    "        if singular in dictionary:\n",
    "            return dictionary[singular]\n",
    "    # Check for 'es' -> '' (e.g., classes -> class)\n",
    "    if token.endswith('es') and len(token) > 2:\n",
    "        singular = token[:-2]\n",
    "        if singular in dictionary:\n",
    "            return dictionary[singular]\n",
    "    # Check for 's' -> '' (e.g., chlorides -> chloride)\n",
    "    if token.endswith('s') and len(token) > 2:\n",
    "        singular = token[:-1]\n",
    "        if singular in dictionary:\n",
    "            return dictionary[singular]\n",
    "    return None\n",
    "\n",
    "def apply_analysis(df, target_words_dict, description_words_dict, abbreviations_dict, all_datasets_info, DB_or_dataset):\n",
    "\n",
    "    def get_special_name_format(table_name, column_name):\n",
    "        special_cases = {\n",
    "            'city': 'city',\n",
    "            'country': 'country',\n",
    "            'state': 'state',\n",
    "            'province': 'state'  # province is treated as state\n",
    "        }\n",
    "        \n",
    "        if DB_or_dataset.lower() == 'd':\n",
    "            # For datasets, table_name is a number, so we don't check it\n",
    "            if column_name.lower() == 'name':\n",
    "                return 'name', target_words_dict['name']\n",
    "        else:\n",
    "            # For database tables, check if any special case is in the table name\n",
    "            table_name_str = str(table_name).lower()\n",
    "            for case, format_type in special_cases.items():\n",
    "                if case in table_name_str and column_name.lower() == 'name':\n",
    "                    return f\"{format_type}_name\", target_words_dict[format_type]\n",
    "        \n",
    "        return 'name', target_words_dict['name']\n",
    "    \n",
    "    # main code \n",
    "    # Check if PK/FK information is available\n",
    "    has_pk_fk_info = 'primary_key' in all_datasets_info.columns and 'foreign_keys' in all_datasets_info.columns\n",
    "\n",
    "    # Get primary and foreign keys for each table\n",
    "    pk_fk_dict = {}\n",
    "    if has_pk_fk_info:\n",
    "        for _, row in all_datasets_info.iterrows():\n",
    "            table_name = row['index']\n",
    "            pk = row['primary_key'] if pd.notna(row['primary_key']) else None\n",
    "            fks = row['foreign_keys'].split(', ') if pd.notna(row['foreign_keys']) else []\n",
    "            fk_columns = [fk.split(' -> ')[0] for fk in fks]\n",
    "            pk_fk_dict[table_name] = {'pk': pk, 'fks': fk_columns} \n",
    "\n",
    "    # 'formats_ordered_list' is a list of dictionary keys in the order they appear in 'formats_dictionary'\n",
    "    formats_ordered_list = list(target_words_dict.keys())\n",
    "\n",
    "    # Initialize new columns\n",
    "    df['ColumnKeyword'] = None\n",
    "    df['ColumnFormat'] = None\n",
    "    df['DescriptionKeyword'] = None\n",
    "    df['DescriptionFormat'] = None\n",
    "    count = 0\n",
    "   \n",
    "    # Apply target words analysis\n",
    "    for i, row in df.iterrows():\n",
    "        count += 1\n",
    "        if count % 1000 == 0:\n",
    "            print(f\"✅ Processed {count} rows...\")\n",
    "\n",
    "        std_col_name = row['CleanedColumn']\n",
    "        col_name = row['Column']\n",
    "        table_name = row['dataset_index']\n",
    "\n",
    "        # Skip if 'CleanedColumn' is missing\n",
    "        if pd.isnull(std_col_name):\n",
    "            continue\n",
    "\n",
    "        # Split camel case\n",
    "        std_col_name = split_camel_case(std_col_name)\n",
    "\n",
    "        # Check if the column is a primary key or foreign key\n",
    "        if table_name in pk_fk_dict:\n",
    "            if col_name == pk_fk_dict[table_name]['pk'] or col_name in pk_fk_dict[table_name]['fks']:\n",
    "                df.at[i, 'ColumnKeyword'] = 'id'\n",
    "                df.at[i, 'ColumnFormat'] = 'IDcolumn'\n",
    "                continue\n",
    "\n",
    "        found = False\n",
    "\n",
    "        # Iterate through each word based on the order in 'formats_ordered_list'\n",
    "        for word in formats_ordered_list:\n",
    "            analysis = target_words_dict[word.lower()]\n",
    "            # Special handling for 'name'\n",
    "            if word == 'name':\n",
    "                keyword, format_type = get_special_name_format(table_name, col_name)\n",
    "                if keyword != 'name':\n",
    "                    df.at[i, 'ColumnKeyword'] = keyword\n",
    "                    df.at[i, 'ColumnFormat'] = format_type\n",
    "                    found = True\n",
    "                    break\n",
    "                pattern = rf'({word})(?![\\\\w-])'\n",
    "            # Special handling for uppercase 'ID' at the end of a column name\n",
    "            elif word == 'id' and col_name.endswith('ID'):\n",
    "                df.at[i, 'ColumnKeyword'] = 'id'\n",
    "                df.at[i, 'ColumnFormat'] = target_words_dict.get('id', 'ID column')\n",
    "                found = True\n",
    "                break\n",
    "            else:\n",
    "                # General matching for other terms\n",
    "                pattern = rf'\\b{word}\\b'\n",
    "\n",
    "            # Search for the pattern in the CleanedColumn\n",
    "            if re.search(pattern, std_col_name, re.IGNORECASE):\n",
    "                df.at[i, 'ColumnKeyword'] = word\n",
    "                df.at[i, 'ColumnFormat'] = analysis\n",
    "                found = True\n",
    "                break\n",
    "\n",
    "        # If no match found, replace abbreviations and try again\n",
    "        if not found:\n",
    "            replaced_text = replace_abbreviations(std_col_name, abbreviations_dict)\n",
    "            for word in formats_ordered_list:\n",
    "                pattern = rf'\\b{word}\\b'\n",
    "                if re.search(pattern, replaced_text, re.IGNORECASE):\n",
    "                    df.at[i, 'ColumnKeyword'] = word\n",
    "                    df.at[i, 'ColumnFormat'] = target_words_dict[word.lower()]\n",
    "                    found = True\n",
    "                    break\n",
    "\n",
    "\n",
    "        # If still no match, check tokens only\n",
    "        if not found:\n",
    "            tokens = re.findall(r'\\b\\w+\\b|%', row['CleanedColumn'].lower())\n",
    "            for token in tokens:\n",
    "                expanded_token = abbreviations_dict.get(token.lower(), None)\n",
    "                # First check plural support for expanded_token\n",
    "                if expanded_token:\n",
    "                    match = match_with_plural(expanded_token, target_words_dict)\n",
    "                    if match:\n",
    "                        df.at[i, 'ColumnKeyword'] = expanded_token\n",
    "                        df.at[i, 'ColumnFormat'] = match\n",
    "                        found = True\n",
    "                        break\n",
    "                # Then check plural support for the token itself\n",
    "                match = match_with_plural(token, target_words_dict)\n",
    "                if match:\n",
    "                    df.at[i, 'ColumnKeyword'] = token\n",
    "                    df.at[i, 'ColumnFormat'] = match\n",
    "                    found = True\n",
    "                    break\n",
    "\n",
    "        # -- PRIORITIZE percentage if % present in CleanedColumn tokens --\n",
    "        tokens = re.findall(r'\\b\\w+\\b|%', row['CleanedColumn'].lower())\n",
    "        if '%' in tokens and '%' in target_words_dict:\n",
    "            df.at[i, 'ColumnKeyword'] = '%'\n",
    "            df.at[i, 'ColumnFormat'] = target_words_dict['%']\n",
    "\n",
    "    count = 0\n",
    "    # Apply description words analysis\n",
    "    for i, row in df.iterrows():\n",
    "        count += 1\n",
    "        if count % 1000 == 0:\n",
    "            print(f\"✅ Processed {count} rows...\")\n",
    "        # Skip if 'Description' is missing\n",
    "        if pd.isnull(row['Description']):\n",
    "            continue\n",
    "        for word, analysis in description_words_dict.items():\n",
    "            if word in ['is', 'has']:\n",
    "                pattern = r'^\\s*' + re.escape(word) + r'\\b'\n",
    "                if re.search(pattern, row['Description'], re.IGNORECASE):\n",
    "                    df.at[i, 'DescriptionKeyword'] = word\n",
    "                    df.at[i, 'DescriptionFormat'] = analysis\n",
    "                    break\n",
    "            else:\n",
    "                if re.search(rf'\\b{re.escape(word)}\\b', row['Description'], re.IGNORECASE):\n",
    "                    df.at[i, 'DescriptionKeyword'] = word\n",
    "                    df.at[i, 'DescriptionFormat'] = analysis\n",
    "                    break\n",
    "                elif not word.isalnum():\n",
    "                    if word.lower() in row['Description'].lower():\n",
    "                        df.at[i, 'DescriptionKeyword'] = word\n",
    "                        df.at[i, 'DescriptionFormat'] = analysis\n",
    "                        break\n",
    "\n",
    "    return df\n",
    "\n",
    "def get_top_features(instance, feature_names, clf, top_n=3):\n",
    "    feature_importances = sorted(\n",
    "        [(importance, name) for importance, name in zip(instance.toarray()[0], feature_names) if importance > 0],\n",
    "        reverse=True\n",
    "    )[:top_n]\n",
    "    return [(name, importance) for importance, name in feature_importances]\n",
    "\n",
    "def custom_resample(X, y, min_samples=6):\n",
    "    # Convert to CSR format if it's not already\n",
    "    if not isinstance(X, sp.csr_matrix):\n",
    "        X = X.tocsr()\n",
    "\n",
    "    class_counts = Counter(y)\n",
    "    X_resampled = []\n",
    "    y_resampled = []\n",
    "    \n",
    "    for class_label, count in class_counts.items():\n",
    "        class_indices = np.where(y == class_label)[0]\n",
    "        if count < min_samples:\n",
    "            n_samples = min_samples\n",
    "            resampled_indices = np.random.choice(class_indices, size=n_samples, replace=True)\n",
    "        else:\n",
    "            n_samples = count\n",
    "            resampled_indices = class_indices\n",
    "        \n",
    "        X_resampled.append(X[resampled_indices])\n",
    "        y_resampled.extend([class_label] * n_samples)\n",
    "    \n",
    "    X_resampled = sp.vstack(X_resampled)\n",
    "    return X_resampled, np.array(y_resampled)\n",
    "\n",
    "def efficient_smote(X, y, sampling_strategy='auto', k_neighbors=5):\n",
    "    # Convert to CSR format if it's not already\n",
    "    if not isinstance(X, sp.csr_matrix):\n",
    "        X = X.tocsr()\n",
    "\n",
    "    X_resampled, y_resampled = custom_resample(X, y)\n",
    "\n",
    "    # Create a NearestNeighbors estimator\n",
    "    #nn = NearestNeighbors(n_neighbors=k_neighbors + 1, n_jobs=-1)\n",
    "    \n",
    "    #smote = SMOTE(sampling_strategy=sampling_strategy, k_neighbors=nn, n_jobs=None)\n",
    "    smote = SMOTE(sampling_strategy=sampling_strategy, k_neighbors=k_neighbors)\n",
    "    # Apply SMOTE\n",
    "    X_resampled, y_resampled = smote.fit_resample(X_resampled, y_resampled)\n",
    "    \n",
    "    return X_resampled, y_resampled\n",
    "\n",
    "def train_evaluate_save_model(model, model_name, X_train, X_test, y_train, y_test, df, vectorizer, weights, feature_names , output_file_path_text):\n",
    "    print(f\"\\nTraining {model_name}...\")\n",
    "    \n",
    "    param_grid = {\n",
    "        'RandomForestClassifier': {\n",
    "            'n_estimators': [100, 200],\n",
    "            'max_depth': [None, 30],\n",
    "            'min_samples_split': [2, 5],\n",
    "        },\n",
    "        'LogisticRegression': {\n",
    "            'C': [0.1, 1],\n",
    "            'solver': ['lbfgs'],\n",
    "            'max_iter': [20000],\n",
    "            'warm_start': [True]\n",
    "        },\n",
    "        'GradientBoostingClassifier': {\n",
    "            'n_estimators': [100],\n",
    "            'learning_rate': [0.1],\n",
    "            'max_depth': [3],\n",
    "        },\n",
    "        'KNeighborsClassifier': {\n",
    "            'n_neighbors': [3, 5],\n",
    "            'weights': ['uniform', 'distance'],\n",
    "        },\n",
    "        'LinearSVC': {\n",
    "            'C': [0.1, 1],\n",
    "            'max_iter': [20000],\n",
    "            'dual': ['auto']\n",
    "        }\n",
    "    }\n",
    "\n",
    "    grid_search = GridSearchCV(model, param_grid[model_name], cv=3, n_jobs=-1, verbose=1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    best_model = grid_search.best_estimator_\n",
    "    print(f\"Best parameters for {model_name}: {grid_search.best_params_}\")\n",
    "\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    report = classification_report(y_test, y_pred, zero_division=1)\n",
    "    print(f\"Classification Report for {model_name}:\")\n",
    "    print(report)\n",
    "\n",
    "    # Save the model\n",
    "    joblib.dump(best_model, f'{model_name}_model.joblib')\n",
    "\n",
    "    # Create predictions for all data\n",
    "    X_full = vectorizer.transform(df['combined'])\n",
    "    if sp.issparse(weights):\n",
    "        X_full = X_full.multiply(weights.diagonal())\n",
    "    else:\n",
    "        X_full = X_full.multiply(weights)\n",
    "    \n",
    "    df[f'predicted_format_{model_name}'] = best_model.predict(X_full)\n",
    "    \n",
    "    # Calculate top features\n",
    "    df[f'top_features_{model_name}'] = df.apply(lambda row: get_top_features(\n",
    "        vectorizer.transform([row['combined']]).multiply(weights), feature_names, best_model), axis=1)\n",
    "\n",
    "    # Add DIF column\n",
    "    def compare_formats(actual, predicted, output_file_path_text):\n",
    "        if actual in ['numerical', 'numerical>=0'] and predicted in ['numerical', 'numerical>=0']:\n",
    "            return 0\n",
    "        return int(actual != predicted)\n",
    "\n",
    "    # Add DIF column\n",
    "    df[f'DIF_{model_name}'] = df.apply(lambda row: compare_formats(row['FinalFormat'], row[f'predicted_format_{model_name}'], output_file_path_text), axis=1)\n",
    "\n",
    "    # Sort the DataFrame\n",
    "    df_sorted = df.sort_values(by=[f'DIF_{model_name}', f'predicted_format_{model_name}', 'FinalFormat'], ascending=[False, True, True])\n",
    "\n",
    "    # Save the DataFrame to an Excel file\n",
    "    #output_file_path = f'AnalysedColumnsDB_with_Predictions_{model_name}.xlsx'\n",
    "    output_file_path = f'{output_file_path_text}_{model_name}.xlsx'\n",
    "    df_sorted.to_excel(output_file_path, index=False)\n",
    "    print(f\"DataFrame for {model_name} saved to {output_file_path}\")\n",
    "\n",
    "    return best_model, report\n",
    "\n",
    "def main():\n",
    "    print(f\"It started on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    \n",
    "    DB_or_dataset = input(\"Insert if using Datasets (d), Database.tables (dt), the two Data Sources (ds), Kaggle Datasets (k), Kaggle Datasets to correct (kc), Sato Datasets (s), Sato Filtered (sf), or All Viznet Datasets (v): \")\n",
    "\n",
    "    print(DB_or_dataset)\n",
    "\n",
    "    if DB_or_dataset.lower() == 'd':\n",
    "        print(\"Using Datasets\")\n",
    "        datasets_xlsx = pd.read_excel(\"FiftyDatasets.xlsx\") \n",
    "        columns_xlsx = pd.read_excel(\"AllColumnsFromFiftyDatasets.xlsx\")\n",
    "        analysed_columns_file_path = 'AnalysedColumns.xlsx' \n",
    "        output_file_path_text = 'AnalysedColumns_with_Predictions'    \n",
    "    elif DB_or_dataset.lower() == 'dt':\n",
    "        print(\"Using Database tables\")\n",
    "        datasets_xlsx = pd.read_excel(\"AllDatasetsInfo.xlsx\")\n",
    "        columns_xlsx = pd.read_excel(\"AllColumnsInfo.xlsx\")\n",
    "        analysed_columns_file_path = 'AnalysedColumnsDB.xlsx'\n",
    "        output_file_path_text = 'AnalysedColumnsDB_with_Predictions'\n",
    "    elif DB_or_dataset.lower() == 'ds':\n",
    "        print(\"Using Datasets and Database tables\")\n",
    "        datasets_xlsx = pd.read_excel(\"AllDataSourcesInfo.xlsx\")\n",
    "        columns_xlsx = pd.read_excel(\"AllAttributes_andColumnsInfo.xlsx\")\n",
    "        analysed_columns_file_path = 'AnalysedColumnsDS.xlsx'\n",
    "        output_file_path_text = 'AnalysedColumnsDS_with_Predictions'\n",
    "    elif DB_or_dataset.lower() == 'k':\n",
    "        print(\"Using Kaggle Datasets\")\n",
    "        datasets_xlsx = pd.read_excel(\"kaggle_datasets_with_domain_and_match.xlsx\")\n",
    "        columns_xlsx = pd.read_excel(\"kaggle_headers.xlsx\")\n",
    "        analysed_columns_file_path = 'AnalysedColumnsK.xlsx'\n",
    "        output_file_path_text = 'AnalysedColumnsK_with_Predictions'\n",
    "    elif DB_or_dataset.lower() == 'kc':\n",
    "        print(\"Using Kaggle Datasets Corrections\")\n",
    "        datasets_xlsx = pd.read_excel(\"kaggle_datasets_with_domain_and_match.xlsx\")\n",
    "        columns_xlsx = pd.read_excel(\"kaggle_headers_10k.xlsx\")\n",
    "        analysed_columns_file_path = 'AnalysedColumnsKc.xlsx'\n",
    "        output_file_path_text = 'AnalysedColumnsKc_with_Predictions'\n",
    "    elif DB_or_dataset.lower() == 's':\n",
    "        print(\"Using Sato Datasets\")\n",
    "        datasets_xlsx = pd.read_excel(\"datasets_viznet.xlsx\")\n",
    "        columns_xlsx = pd.read_excel(\"columns_sato_only.xlsx\")\n",
    "        analysed_columns_file_path = 'AnalysedColumnsSato.xlsx'\n",
    "        output_file_path_text = 'AnalysedColumnsS_with_Predictions'\n",
    "    elif DB_or_dataset.lower() == 'sf':\n",
    "        print(\"Using Sato Filtered Datasets\")\n",
    "        datasets_xlsx = pd.read_excel(\"datasets_viznet.xlsx\")\n",
    "        columns_xlsx = pd.read_excel(\"filtered_columns_sato_only.xlsx\")\n",
    "        analysed_columns_file_path = 'AnalysedColumnsSatoFiltered.xlsx'\n",
    "        output_file_path_text = 'AnalysedColumnsSF_with_Predictions'\n",
    "    elif DB_or_dataset.lower() == 'v':\n",
    "        print(\"Using Viznet Filtered Datasets\")\n",
    "        datasets_xlsx = pd.read_excel(\"datasets_viznet.xlsx\")\n",
    "        columns_xlsx = pd.read_excel(\"filtered_columns_viznet_all.xlsx\")\n",
    "        analysed_columns_file_path = 'AnalysedColumnsViznetFiltered.xlsx'\n",
    "        output_file_path_text = 'AnalysedColumnsV_with_Predictions'\n",
    "    else:\n",
    "        raise ValueError(\"Invalid input. Please enter 'd', 'dt', 'ds', 'k', 'kc', 's', 'sf' or 'v'.\")\n",
    "\n",
    "    print(datasets_xlsx.head())\n",
    "\n",
    "    columns_xlsx = columns_xlsx.rename(columns={'index': 'dataset_index'})\n",
    "\n",
    "    # Load the dataset\n",
    "    df = columns_xlsx\n",
    "    print(f\"Original df shape: {df.shape}\")\n",
    "\n",
    "    # Load dictionaries\n",
    "    dictionary = {}\n",
    "\n",
    "    # Open the formats dictionary file and read line by line \n",
    "    with open(\"formats_dictionary.txt\", \"r\") as file:\n",
    "        for line in file:\n",
    "            # Remove the trailing newline and comma, then split the line into key and value at the colon\n",
    "            key, value = line.rstrip(\",\\n\").split(\":\")\n",
    "        \n",
    "            # Remove the quotes around the key and value\n",
    "            key = key.strip(\"'\")\n",
    "            value = value.strip(\"'\")\n",
    "\n",
    "            # Add the key-value pair to the dictionary\n",
    "            dictionary[key] = value\n",
    "\n",
    "    # Load the abbreviations dictionary\n",
    "    abbreviations_dict = {}\n",
    "    with open(\"abbreviations_dictionary.txt\", \"r\") as file:\n",
    "        for line in file:\n",
    "            abbr, full_form = line.strip().split(\":\")\n",
    "            abbreviations_dict[abbr.strip()] = full_form.strip()\n",
    "\n",
    "    target_words_dict = dictionary\n",
    "    description_words_dict = dictionary\n",
    "\n",
    "    # Clean and preprocess columns\n",
    "    df = clean_columns(df)\n",
    "\n",
    "    print('AFTER CLEAN', df.head())\n",
    "    df = preprocess_columns(df, abbreviations_dict)\n",
    "\n",
    "    print(df)\n",
    "\n",
    "    # Apply analysis\n",
    "    df = apply_analysis(df, target_words_dict, description_words_dict, abbreviations_dict, datasets_xlsx, DB_or_dataset)\n",
    "\n",
    "    # Load the FinalFormat from AnalysedColumns\n",
    "    analysed_df = pd.read_excel(analysed_columns_file_path)\n",
    "    \n",
    "    # Merge the FinalFormat into our main dataframe\n",
    "    df['FinalFormat'] = analysed_df['FinalFormat']\n",
    "\n",
    "    # Prepare features for machine learning\n",
    "    df['combined'] = df['CleanedColumn'].fillna('') + ' [SEP] ' + df['Description'].fillna('') \n",
    "    df['combined'] += ' [SEP] ' + df['ColumnKeyword'].fillna('') + ' [SEP] ' + df['DescriptionKeyword'].fillna('')\n",
    "\n",
    "    #df['combined'] += df['ColumnFormat'].fillna('') + ' [SEP] ' + df['DescriptionFormat'].fillna('')\n",
    "    \n",
    "    # Load DB headers distribution file\n",
    "    db_headers = {}\n",
    "    with open('DBheaders+formats_dict.txt', 'r') as file:\n",
    "        next(file)  # skip the header line\n",
    "        for line in file:\n",
    "            header, count = line.strip().split('\\t')\n",
    "            db_headers[header.lower()] = int(count)\n",
    "\n",
    "    # Extract features using Custom TF-IDF with vocabulary restricted to DB headers\n",
    "    vectorizer = CustomTfidfVectorizer(vocabulary=list(db_headers.keys()))\n",
    "    X = vectorizer.fit_transform(df['combined']).tocsr()\n",
    "\n",
    "    # Modify TF-IDF scores explicitly based on DB headers\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    weights = np.array([db_headers.get(feat, 1) for feat in feature_names])\n",
    "  \n",
    "    # Ensure weights is a 1D array with the same number of elements as X has columns\n",
    "    if weights.shape[0] != X.shape[1]:\n",
    "        raise ValueError(f\"Number of weights ({weights.shape[0]}) does not match number of features in X ({X.shape[1]})\")\n",
    "    \n",
    "    # Multiply each column of X by its corresponding weight\n",
    "    X = X.multiply(weights)\n",
    "\n",
    "    # Encode the FinalFormat column \n",
    "    y = df['FinalFormat'].astype(str)\n",
    "\n",
    "    # Apply efficient SMOTE\n",
    "    X_resampled, y_resampled = efficient_smote(X, y)\n",
    "\n",
    "    print(\"Class distribution after SMOTE:\")\n",
    "    print(pd.Series(y_resampled).value_counts())\n",
    "\n",
    "    # Split the resampled data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42, stratify=y_resampled)\n",
    "\n",
    "    # List of models to train\n",
    "    models = [\n",
    "        (RandomForestClassifier(class_weight='balanced', random_state=42), 'RandomForestClassifier'),\n",
    "        (LogisticRegression(class_weight='balanced', random_state=42, warm_start=True), 'LogisticRegression'),\n",
    "        (GradientBoostingClassifier(random_state=42), 'GradientBoostingClassifier'),\n",
    "        (KNeighborsClassifier(), 'KNeighborsClassifier'),\n",
    "        (LinearSVC(class_weight='balanced', random_state=42, dual='auto'), 'LinearSVC')\n",
    "    ]\n",
    "\n",
    "    # Train, evaluate, and save each model\n",
    "    for model, model_name in models:\n",
    "        best_model, report = train_evaluate_save_model(model, model_name, X_train, X_test, y_train, y_test, df, vectorizer, weights, feature_names, output_file_path_text)\n",
    "        \n",
    "        # Save classification report\n",
    "        with open(f'{model_name}_classification_report.txt', 'w') as f:\n",
    "            f.write(report)\n",
    "\n",
    "    # Save the vectorizer for future use\n",
    "    joblib.dump(vectorizer, 'vectorizer.joblib')\n",
    "    print(\"Vectorizer saved for future use\")\n",
    "\n",
    "    print(f\"Last run on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
