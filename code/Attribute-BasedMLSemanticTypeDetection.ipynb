{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It started on: 2024-10-03 21:17:36\n",
      "ds\n",
      "Using Datasets and Database tables\n",
      "  index           name database      area  instances  attributes Column  \\\n",
      "0    52           Iris      NaN      Life        150           4    NaN   \n",
      "1   107           Wine      NaN  Physical        178          13    NaN   \n",
      "2    92       Spambase      NaN  Computer       4601          57    NaN   \n",
      "3    45  Heart Disease      NaN      Life        303          75    NaN   \n",
      "4     2          Adult      NaN    Social      48842          14    NaN   \n",
      "\n",
      "  Description primary_key foreign_keys  \n",
      "0         NaN         NaN          NaN  \n",
      "1         NaN         NaN          NaN  \n",
      "2         NaN         NaN          NaN  \n",
      "3         NaN         NaN          NaN  \n",
      "4         NaN         NaN          NaN  \n",
      "Original df shape: (1442, 5)\n",
      "              index    name       area                 Original Column ID  \\\n",
      "0               NaN    Iris       Life     1.       sepal length in cm  1   \n",
      "1               NaN    Iris       Life      2.       sepal width in cm  2   \n",
      "2               NaN    Iris       Life     3.       petal length in cm  3   \n",
      "3               NaN    Iris       Life      4.       petal width in cm  4   \n",
      "4               NaN    Iris       Life                  5.       class  5   \n",
      "...             ...     ...        ...                             ... ..   \n",
      "1437  UW_std.person  person  Education      2. professor (varchar(11))  2   \n",
      "1438  UW_std.person  person  Education        3. student (varchar(11))  3   \n",
      "1439  UW_std.person  person  Education    4. hasPosition (varchar(11))  4   \n",
      "1440  UW_std.person  person  Education        5. inPhase (varchar(40))  5   \n",
      "1441  UW_std.person  person  Education  6. yearsInProgram (varchar(40)  6   \n",
      "\n",
      "                  Column Description       CleanedColumn  \n",
      "0     sepal length in cm         NaN  sepal length in cm  \n",
      "1      sepal width in cm         NaN   sepal width in cm  \n",
      "2     petal length in cm         NaN  petal length in cm  \n",
      "3      petal width in cm         NaN   petal width in cm  \n",
      "4                  class         NaN               class  \n",
      "...                  ...         ...                 ...  \n",
      "1437           professor  varchar 11           professor  \n",
      "1438             student  varchar 11             student  \n",
      "1439         hasPosition  varchar 11        has position  \n",
      "1440             inPhase  varchar 40            in phase  \n",
      "1441      yearsInProgram  varchar 40    years in program  \n",
      "\n",
      "[1442 rows x 8 columns]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 516\u001b[0m\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLast run on: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdatetime\u001b[38;5;241m.\u001b[39mnow()\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 516\u001b[0m     main()\n",
      "Cell \u001b[1;32mIn[1], line 443\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    440\u001b[0m \u001b[38;5;28mprint\u001b[39m(df)\n\u001b[0;32m    442\u001b[0m \u001b[38;5;66;03m# Apply analysis\u001b[39;00m\n\u001b[1;32m--> 443\u001b[0m df \u001b[38;5;241m=\u001b[39m apply_analysis(df, target_words_dict, description_words_dict, abbreviations_dict, datasets_xlsx, DB_or_dataset)\n\u001b[0;32m    445\u001b[0m \u001b[38;5;66;03m# Load the FinalFormat from AnalysedColumns\u001b[39;00m\n\u001b[0;32m    446\u001b[0m analysed_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_excel(analysed_columns_file_path)\n",
      "Cell \u001b[1;32mIn[1], line 241\u001b[0m, in \u001b[0;36mapply_analysis\u001b[1;34m(df, target_words_dict, description_words_dict, abbreviations_dict, all_datasets_info, DB_or_dataset)\u001b[0m\n\u001b[0;32m    238\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m    240\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m word, analysis \u001b[38;5;129;01min\u001b[39;00m description_words_dict\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m--> 241\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;129;01mnot\u001b[39;00m pd\u001b[38;5;241m.\u001b[39misnull(row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDescription\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;129;01mand\u001b[39;00m re\u001b[38;5;241m.\u001b[39msearch(\u001b[38;5;124mrf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mword\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m, row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDescription\u001b[39m\u001b[38;5;124m'\u001b[39m], re\u001b[38;5;241m.\u001b[39mIGNORECASE))):\n\u001b[0;32m    242\u001b[0m         df\u001b[38;5;241m.\u001b[39mat[i, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDescriptionKeyword\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m word\n\u001b[0;32m    243\u001b[0m         df\u001b[38;5;241m.\u001b[39mat[i, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDescriptionFormat\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m analysis\n",
      "File \u001b[1;32mc:\\Users\\290099c\\AppData\\Local\\anaconda3\\Lib\\re\\__init__.py:176\u001b[0m, in \u001b[0;36msearch\u001b[1;34m(pattern, string, flags)\u001b[0m\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msearch\u001b[39m(pattern, string, flags\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m    174\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Scan through string looking for a match to the pattern, returning\u001b[39;00m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;124;03m    a Match object, or None if no match was found.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 176\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _compile(pattern, flags)\u001b[38;5;241m.\u001b[39msearch(string)\n",
      "File \u001b[1;32mc:\\Users\\290099c\\AppData\\Local\\anaconda3\\Lib\\re\\__init__.py:294\u001b[0m, in \u001b[0;36m_compile\u001b[1;34m(pattern, flags)\u001b[0m\n\u001b[0;32m    288\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[0;32m    289\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe re.TEMPLATE/re.T flag is deprecated \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    290\u001b[0m               \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is an undocumented flag \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    291\u001b[0m               \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwithout an obvious purpose. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    292\u001b[0m               \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDon\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt use it.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    293\u001b[0m               \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m)\n\u001b[1;32m--> 294\u001b[0m p \u001b[38;5;241m=\u001b[39m _compiler\u001b[38;5;241m.\u001b[39mcompile(pattern, flags)\n\u001b[0;32m    295\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (flags \u001b[38;5;241m&\u001b[39m DEBUG):\n\u001b[0;32m    296\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(_cache) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m _MAXCACHE:\n\u001b[0;32m    297\u001b[0m         \u001b[38;5;66;03m# Drop the oldest item\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\290099c\\AppData\\Local\\anaconda3\\Lib\\re\\_compiler.py:747\u001b[0m, in \u001b[0;36mcompile\u001b[1;34m(p, flags)\u001b[0m\n\u001b[0;32m    744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    745\u001b[0m     pattern \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 747\u001b[0m code \u001b[38;5;241m=\u001b[39m _code(p, flags)\n\u001b[0;32m    749\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m&\u001b[39m SRE_FLAG_DEBUG:\n\u001b[0;32m    750\u001b[0m     \u001b[38;5;28mprint\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\290099c\\AppData\\Local\\anaconda3\\Lib\\re\\_compiler.py:580\u001b[0m, in \u001b[0;36m_code\u001b[1;34m(p, flags)\u001b[0m\n\u001b[0;32m    577\u001b[0m _compile_info(code, p, flags)\n\u001b[0;32m    579\u001b[0m \u001b[38;5;66;03m# compile the pattern\u001b[39;00m\n\u001b[1;32m--> 580\u001b[0m _compile(code, p\u001b[38;5;241m.\u001b[39mdata, flags)\n\u001b[0;32m    582\u001b[0m code\u001b[38;5;241m.\u001b[39mappend(SUCCESS)\n\u001b[0;32m    584\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m code\n",
      "File \u001b[1;32mc:\\Users\\290099c\\AppData\\Local\\anaconda3\\Lib\\re\\_compiler.py:159\u001b[0m, in \u001b[0;36m_compile\u001b[1;34m(code, pattern, flags)\u001b[0m\n\u001b[0;32m    157\u001b[0m     code[skip] \u001b[38;5;241m=\u001b[39m _len(code) \u001b[38;5;241m-\u001b[39m skip\n\u001b[0;32m    158\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m op \u001b[38;5;129;01mis\u001b[39;00m AT:\n\u001b[1;32m--> 159\u001b[0m     emit(op)\n\u001b[0;32m    160\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m&\u001b[39m SRE_FLAG_MULTILINE:\n\u001b[0;32m    161\u001b[0m         av \u001b[38;5;241m=\u001b[39m AT_MULTILINE\u001b[38;5;241m.\u001b[39mget(av, av)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import scipy.sparse as sp\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier, NearestNeighbors\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "\n",
    "# Custom Tokenizer\n",
    "def custom_tokenizer(text):\n",
    "    if pd.isna(text):\n",
    "        return []\n",
    "    tokens = re.findall(r'\\w+', str(text).lower())\n",
    "    return [subtok for token in tokens for subtok in re.findall(r'[a-z]+|\\d+', token)]\n",
    "\n",
    "# Custom TfidfVectorizer\n",
    "class CustomTfidfVectorizer(TfidfVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        return lambda doc: custom_tokenizer(doc)\n",
    "\n",
    "def clean_columns(df):\n",
    "\n",
    "    df['index'] = df['index'].astype(str)\n",
    "    # Extract the ID from the column name \n",
    "    df['ID'] = df['Original Column'].str.extract(r'(\\d+)\\.')\n",
    "    \n",
    "    # Extract the Column name up to the first colon, slash, or parenthesis\n",
    "    df['Column'] = df['Original Column'].str.extract(r'\\d+\\.\\s*([^/(:]+)')\n",
    "    \n",
    "    # Extract everything after the first colon, slash, or parenthesis as the full description\n",
    "    df['Description'] = df['Original Column'].str.extract(r'\\d+\\.\\s*[^/(:]+\\s*[:(/](.*)')[0]\n",
    "    \n",
    "    # For cases where Description is None (like the new case with parentheses),\n",
    "    # extract the content within parentheses\n",
    "    mask = df['Description'].isnull()\n",
    "    df.loc[mask, 'Description'] = df.loc[mask, 'Original Column'].str.extract(r'\\(([^)]+)\\)')\n",
    "    \n",
    "    # Clean up: Trim spaces from all string columns\n",
    "    df = df.apply(lambda x: x.str.strip() if x.dtype == \"object\" else x)\n",
    "    \n",
    "    # Replace empty strings with None\n",
    "    df = df.replace(r'^\\s*$', None, regex=True)\n",
    "    \n",
    "    # Remove any remaining parentheses from Description and add space between type and size\n",
    "    df['Description'] = df['Description'].str.replace(r'\\(', ' ', regex=True).str.replace(r'\\)', '', regex=True)\n",
    "    \n",
    "    # Reorder columns to ensure ID comes before Column\n",
    "    columns_order = ['index', 'name', 'area', 'Original Column', 'ID', 'Column', 'Description']\n",
    "    df = df.reindex(columns=columns_order)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def replace_abbreviations(text, abbreviations_dict):\n",
    "    words = text.split()\n",
    "    replaced_words = [abbreviations_dict.get(word, word) for word in words]\n",
    "    return \" \".join(replaced_words)\n",
    "\n",
    "def preprocess_columns(df, abbreviations_dict):\n",
    "    # Only convert \"Column\" to string type for non-null values and then to lowercase to create \"CleanedColumn\"\n",
    "    mask = df['Column'].notna()\n",
    "    df.loc[mask, 'CleanedColumn'] = df.loc[mask, 'Column'].astype(str).apply(split_camel_case)\n",
    "    \n",
    "    # Remove '-' and '_' characters only for non-null values\n",
    "    mask = df['CleanedColumn'].notna()\n",
    "    df.loc[mask, 'CleanedColumn'] = df.loc[mask, 'CleanedColumn'].str.replace('[-_]', ' ', regex=True)\n",
    "\n",
    "    # Apply abbreviations dictionary to Description\n",
    "    mask = df['Description'].notna()\n",
    "    df.loc[mask, 'Description'] = df.loc[mask, 'Description'].apply(lambda x: replace_abbreviations(x, abbreviations_dict))\n",
    " \n",
    "    return df\n",
    "\n",
    "def split_camel_case(s):\n",
    "    # Special case for 'pH'\n",
    "    if s.lower() == 'ph':\n",
    "        return 'ph'\n",
    "    \n",
    "    # Split the string into words\n",
    "    words = re.findall(r'[A-Z]{2,}(?=[A-Z][a-z]+[0-9]*|\\d|\\W|$)|\\w+|[^\\w\\s]', s)\n",
    "    \n",
    "    # Process each word\n",
    "    processed_words = []\n",
    "    for word in words:\n",
    "        # Keep acronyms and all-uppercase words as is\n",
    "        if word.isupper() and len(word) > 1:\n",
    "            processed_words.append(word)\n",
    "        else:\n",
    "            # Split camel case for other words\n",
    "            split_word = re.findall(r'[A-Z]?[a-z]+|[A-Z]{2,}(?=[A-Z][a-z]|\\d|\\W|$)|\\d+', word)\n",
    "            processed_words.extend(split_word)\n",
    "    \n",
    "    # Join the words and convert to lowercase\n",
    "    return ' '.join(processed_words).lower()\n",
    "\n",
    "def apply_analysis(df, target_words_dict, description_words_dict, abbreviations_dict, all_datasets_info, DB_or_dataset):\n",
    "\n",
    "    def get_special_name_format(table_name, column_name):\n",
    "        special_cases = {\n",
    "            'city': 'city',\n",
    "            'country': 'country',\n",
    "            'state': 'state',\n",
    "            'province': 'state'  # province is treated as state\n",
    "        }\n",
    "\n",
    "        if DB_or_dataset.lower() == 'd':\n",
    "            # For datasets, table_name is a number, so we don't check it\n",
    "            if column_name.lower() == 'name':\n",
    "                return 'name', target_words_dict['name']\n",
    "        else:\n",
    "            # For database tables, check if any special case is in the table name\n",
    "            table_name_str = str(table_name).lower()\n",
    "            for case, format_type in special_cases.items():\n",
    "                if case in table_name_str and column_name.lower() == 'name':\n",
    "                    return f\"{format_type}_name\", target_words_dict[format_type]\n",
    "        \n",
    "        return 'name', target_words_dict['name']\n",
    "    \n",
    "    # main code \n",
    "    # Check if PK/FK information is available\n",
    "    has_pk_fk_info = 'primary_key' in all_datasets_info.columns and 'foreign_keys' in all_datasets_info.columns\n",
    "\n",
    "    # Get primary and foreign keys for each table\n",
    "    pk_fk_dict = {}\n",
    "    if has_pk_fk_info:\n",
    "        for _, row in all_datasets_info.iterrows():\n",
    "            table_name = row['index']\n",
    "            pk = row['primary_key'] if pd.notna(row['primary_key']) else None\n",
    "            fks = row['foreign_keys'].split(', ') if pd.notna(row['foreign_keys']) else []\n",
    "            fk_columns = [fk.split(' -> ')[0] for fk in fks]\n",
    "            pk_fk_dict[table_name] = {'pk': pk, 'fks': fk_columns} \n",
    "\n",
    "    # 'formats_ordered_list' is a list of dictionary keys in the order they appear in 'formats_dictionary'\n",
    "    formats_ordered_list = list(target_words_dict.keys())\n",
    "\n",
    "    # Initialize new columns\n",
    "    df['ColumnKeyword'] = None\n",
    "    df['ColumnFormat'] = None\n",
    "    df['DescriptionKeyword'] = None\n",
    "    df['DescriptionFormat'] = None\n",
    "   \n",
    "    # Apply target words analysis\n",
    "    for i, row in df.iterrows():\n",
    "        std_col_name = row['CleanedColumn']\n",
    "        col_name = row['Column']\n",
    "        table_name = row['index']\n",
    "\n",
    "        # Skip if 'CleanedColumn' is missing\n",
    "        if pd.isnull(std_col_name):\n",
    "            continue\n",
    "\n",
    "        # Split camel case\n",
    "        std_col_name = split_camel_case(std_col_name)\n",
    "\n",
    "        # Check if the column is a primary key or foreign key\n",
    "        if table_name in pk_fk_dict:\n",
    "            if col_name == pk_fk_dict[table_name]['pk'] or col_name in pk_fk_dict[table_name]['fks']:\n",
    "                df.at[i, 'ColumnKeyword'] = 'id'\n",
    "                df.at[i, 'ColumnFormat'] = 'IDcolumn'\n",
    "                continue\n",
    "\n",
    "        found = False\n",
    "\n",
    "        # Iterate through each word based on the order in 'formats_ordered_list'\n",
    "        for word in formats_ordered_list:\n",
    "            analysis = target_words_dict[word]\n",
    "            # Special handling for 'name'\n",
    "            if word == 'name':\n",
    "                keyword, format_type = get_special_name_format(table_name, col_name)\n",
    "                if keyword != 'name':\n",
    "                    df.at[i, 'ColumnKeyword'] = keyword\n",
    "                    df.at[i, 'ColumnFormat'] = format_type\n",
    "                    found = True\n",
    "                    break\n",
    "                pattern = rf'({word})(?![\\\\w-])'\n",
    "            # Special handling for uppercase 'ID' at the end of a column name\n",
    "            elif word == 'id' and col_name.endswith('ID'):\n",
    "                df.at[i, 'ColumnKeyword'] = 'id'\n",
    "                df.at[i, 'ColumnFormat'] = target_words_dict.get('id', 'ID column')\n",
    "                found = True\n",
    "                break\n",
    "            else:\n",
    "                # General matching for other terms\n",
    "                pattern = rf'\\b{word}\\b'\n",
    "\n",
    "            # Search for the pattern in the CleanedColumn\n",
    "            if re.search(pattern, std_col_name, re.IGNORECASE):\n",
    "                df.at[i, 'ColumnKeyword'] = word\n",
    "                df.at[i, 'ColumnFormat'] = analysis\n",
    "                found = True\n",
    "                break\n",
    "\n",
    "        # If no match found, replace abbreviations and try again\n",
    "        if not found:\n",
    "            replaced_text = replace_abbreviations(std_col_name, abbreviations_dict)\n",
    "            for word in formats_ordered_list:\n",
    "                pattern = rf'\\b{word}\\b'\n",
    "                if re.search(pattern, replaced_text, re.IGNORECASE):\n",
    "                    df.at[i, 'ColumnKeyword'] = word\n",
    "                    df.at[i, 'ColumnFormat'] = target_words_dict[word]\n",
    "                    found = True\n",
    "                    break\n",
    "\n",
    "        # If still no match, break down the word into substrings\n",
    "        if not found:\n",
    "            for j in range(len(row['CleanedColumn']), 2, -1):\n",
    "                for k in range(len(row['CleanedColumn']) - j + 1):\n",
    "                    subword = row['CleanedColumn'][k:k+j]\n",
    "                    \n",
    "                    # Check if the subword exists in the abbreviation dictionary\n",
    "                    expanded_subword = abbreviations_dict.get(subword, None)\n",
    "                    if expanded_subword:\n",
    "                        # If the expanded subword exists in the target words dictionary, use it\n",
    "                        if expanded_subword in target_words_dict:\n",
    "                            df.at[i, 'ColumnKeyword'] = expanded_subword\n",
    "                            df.at[i, 'ColumnFormat'] = target_words_dict[expanded_subword]\n",
    "                            found = True\n",
    "                            break\n",
    "                        \n",
    "                    # Else, continue with the original subword\n",
    "                    elif subword in target_words_dict:\n",
    "                        df.at[i, 'ColumnKeyword'] = subword\n",
    "                        df.at[i, 'ColumnFormat'] = target_words_dict[subword]\n",
    "                        break\n",
    "\n",
    "                if found:\n",
    "                    break\n",
    "\n",
    "    # Apply description words analysis\n",
    "    for i, row in df.iterrows():\n",
    "        # Skip if 'Description' is missing\n",
    "        if pd.isnull(row['Description']):\n",
    "            continue\n",
    "\n",
    "        for word, analysis in description_words_dict.items():\n",
    "            if ((not pd.isnull(row['Description']) and re.search(rf'\\b{word}\\b', row['Description'], re.IGNORECASE))):\n",
    "                df.at[i, 'DescriptionKeyword'] = word\n",
    "                df.at[i, 'DescriptionFormat'] = analysis\n",
    "                break\n",
    "\n",
    "    return df\n",
    "\n",
    "def get_top_features(instance, feature_names, clf, top_n=3):\n",
    "    feature_importances = sorted(\n",
    "        [(importance, name) for importance, name in zip(instance.toarray()[0], feature_names) if importance > 0],\n",
    "        reverse=True\n",
    "    )[:top_n]\n",
    "    return [(name, importance) for importance, name in feature_importances]\n",
    "\n",
    "def custom_resample(X, y, min_samples=6):\n",
    "    # Convert to CSR format if it's not already\n",
    "    if not isinstance(X, sp.csr_matrix):\n",
    "        X = X.tocsr()\n",
    "\n",
    "    class_counts = Counter(y)\n",
    "    X_resampled = []\n",
    "    y_resampled = []\n",
    "    \n",
    "    for class_label, count in class_counts.items():\n",
    "        class_indices = np.where(y == class_label)[0]\n",
    "        if count < min_samples:\n",
    "            n_samples = min_samples\n",
    "            resampled_indices = np.random.choice(class_indices, size=n_samples, replace=True)\n",
    "        else:\n",
    "            n_samples = count\n",
    "            resampled_indices = class_indices\n",
    "        \n",
    "        X_resampled.append(X[resampled_indices])\n",
    "        y_resampled.extend([class_label] * n_samples)\n",
    "    \n",
    "    X_resampled = sp.vstack(X_resampled)\n",
    "    return X_resampled, np.array(y_resampled)\n",
    "\n",
    "def efficient_smote(X, y, sampling_strategy='auto', k_neighbors=5):\n",
    "    # Convert to CSR format if it's not already\n",
    "    if not isinstance(X, sp.csr_matrix):\n",
    "        X = X.tocsr()\n",
    "\n",
    "    X_resampled, y_resampled = custom_resample(X, y)\n",
    "\n",
    "    # Create a NearestNeighbors estimator\n",
    "    nn = NearestNeighbors(n_neighbors=k_neighbors + 1, n_jobs=-1)\n",
    "    \n",
    "    # Initialize SMOTE with the NearestNeighbors estimator\n",
    "    smote = SMOTE(sampling_strategy=sampling_strategy, k_neighbors=nn, n_jobs=None)\n",
    "    \n",
    "    # Apply SMOTE\n",
    "    X_resampled, y_resampled = smote.fit_resample(X_resampled, y_resampled)\n",
    "    \n",
    "    return X_resampled, y_resampled\n",
    "\n",
    "def train_evaluate_save_model(model, model_name, X_train, X_test, y_train, y_test, df, vectorizer, weights, feature_names , output_file_path_text):\n",
    "    print(f\"\\nTraining {model_name}...\")\n",
    "    \n",
    "    param_grid = {\n",
    "        'RandomForestClassifier': {\n",
    "            'n_estimators': [100, 200],\n",
    "            'max_depth': [None, 30],\n",
    "            'min_samples_split': [2, 5],\n",
    "        },\n",
    "        'LogisticRegression': {\n",
    "            'C': [0.1, 1],\n",
    "            'solver': ['lbfgs'],\n",
    "            'max_iter': [20000],\n",
    "            'warm_start': [True]\n",
    "        },\n",
    "        'GradientBoostingClassifier': {\n",
    "            'n_estimators': [100],\n",
    "            'learning_rate': [0.1],\n",
    "            'max_depth': [3],\n",
    "        },\n",
    "        'KNeighborsClassifier': {\n",
    "            'n_neighbors': [3, 5],\n",
    "            'weights': ['uniform', 'distance'],\n",
    "        },\n",
    "        'LinearSVC': {\n",
    "            'C': [0.1, 1],\n",
    "            'max_iter': [20000],\n",
    "            'dual': ['auto']\n",
    "        }\n",
    "    }\n",
    "\n",
    "    grid_search = GridSearchCV(model, param_grid[model_name], cv=3, n_jobs=-1, verbose=1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    best_model = grid_search.best_estimator_\n",
    "    print(f\"Best parameters for {model_name}: {grid_search.best_params_}\")\n",
    "\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    report = classification_report(y_test, y_pred, zero_division=1)\n",
    "    print(f\"Classification Report for {model_name}:\")\n",
    "    print(report)\n",
    "\n",
    "    # Save the model\n",
    "    joblib.dump(best_model, f'{model_name}_model.joblib')\n",
    "\n",
    "    # Create predictions for all data\n",
    "    X_full = vectorizer.transform(df['combined'])\n",
    "    if sp.issparse(weights):\n",
    "        X_full = X_full.multiply(weights.diagonal())\n",
    "    else:\n",
    "        X_full = X_full.multiply(weights)\n",
    "    \n",
    "    df[f'predicted_format_{model_name}'] = best_model.predict(X_full)\n",
    "    \n",
    "    # Calculate top features\n",
    "    df[f'top_features_{model_name}'] = df.apply(lambda row: get_top_features(\n",
    "        vectorizer.transform([row['combined']]).multiply(weights), feature_names, best_model), axis=1)\n",
    "\n",
    "    # Add DIF column\n",
    "    def compare_formats(actual, predicted, output_file_path_text):\n",
    "        if actual in ['numerical', 'numerical>=0'] and predicted in ['numerical', 'numerical>=0']:\n",
    "            return 0\n",
    "        return int(actual != predicted)\n",
    "\n",
    "    # Add DIF column\n",
    "    df[f'DIF_{model_name}'] = df.apply(lambda row: compare_formats(row['FinalFormat'], row[f'predicted_format_{model_name}'], output_file_path_text), axis=1)\n",
    "\n",
    "    # Sort the DataFrame\n",
    "    df_sorted = df.sort_values(by=[f'DIF_{model_name}', f'predicted_format_{model_name}', 'FinalFormat'], ascending=[False, True, True])\n",
    "\n",
    "    # Save the DataFrame to an Excel file\n",
    "    #output_file_path = f'AnalysedColumnsDB_with_Predictions_{model_name}.xlsx'\n",
    "    output_file_path = f'{output_file_path_text}_{model_name}.xlsx'\n",
    "    df_sorted.to_excel(output_file_path, index=False)\n",
    "    print(f\"DataFrame for {model_name} saved to {output_file_path}\")\n",
    "\n",
    "    return best_model, report\n",
    "\n",
    "def main():\n",
    "    print(f\"It started on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    \n",
    "    DB_or_dataset = input(\"Insert if using Datasets (d) or Database.tables (dt) or All Data Sources (ds): \")\n",
    "\n",
    "    print(DB_or_dataset)\n",
    "\n",
    "    if DB_or_dataset.lower() == 'd':\n",
    "        print(\"Using Datasets\")\n",
    "        datasets_xlsx = pd.read_excel(\"FiftyDatasets.xlsx\") \n",
    "        columns_xlsx = \"AllColumnsFromFiftyDatasets.xlsx\"\n",
    "        analysed_columns_file_path = 'AnalysedColumns.xlsx'     \n",
    "        output_file_path_text = 'AnalysedColumns_with_Predictions'\n",
    "    elif DB_or_dataset.lower() == 'dt':\n",
    "        print(\"Using Database tables\")\n",
    "        datasets_xlsx = pd.read_excel(\"AllDatasetsInfo.xlsx\")\n",
    "        columns_xlsx = \"AllColumnsInfo.xlsx\"\n",
    "        analysed_columns_file_path = 'AnalysedColumnsDB.xlsx'\n",
    "        output_file_path_text = 'AnalysedColumnsDB_with_Predictions'\n",
    "    elif DB_or_dataset.lower() == 'ds':\n",
    "        print(\"Using Datasets and Database tables\")\n",
    "        datasets_xlsx = pd.read_excel(\"AllDatasourcessInfo.xlsx\")\n",
    "        columns_xlsx = \"AllAttributes_andColumnsInfo.xlsx\"\n",
    "        analysed_columns_file_path = 'AnalysedColumnsDS.xlsx'\n",
    "        output_file_path_text = 'AnalysedColumnsDS_with_Predictions'\n",
    "    else:\n",
    "        raise ValueError(\"Invalid input. Please enter 'd' for Datasets or 'dt' for Database tables or or 'ds' for All Data Sources.\")\n",
    "\n",
    "    print(datasets_xlsx.head())\n",
    "\n",
    "    # Load the dataset\n",
    "    df = pd.read_excel(columns_xlsx)\n",
    "    print(f\"Original df shape: {df.shape}\")\n",
    "\n",
    "    # Load dictionaries\n",
    "    dictionary = {}\n",
    "\n",
    "    # Open the formats dictionary file and read line by line \n",
    "    with open(\"formats_dictionary.txt\", \"r\") as file:\n",
    "        for line in file:\n",
    "            # Remove the trailing newline and comma, then split the line into key and value at the colon\n",
    "            key, value = line.rstrip(\",\\n\").split(\":\")\n",
    "        \n",
    "            # Remove the quotes around the key and value\n",
    "            key = key.strip(\"'\")\n",
    "            value = value.strip(\"'\")\n",
    "\n",
    "            # Add the key-value pair to the dictionary\n",
    "            dictionary[key] = value\n",
    "\n",
    "    # Load the abbreviations dictionary\n",
    "    abbreviations_dict = {}\n",
    "    with open(\"abbreviations_dictionary.txt\", \"r\") as file:\n",
    "        for line in file:\n",
    "            abbr, full_form = line.strip().split(\":\")\n",
    "            abbreviations_dict[abbr.strip()] = full_form.strip()\n",
    "\n",
    "    target_words_dict = dictionary\n",
    "    description_words_dict = dictionary\n",
    "\n",
    "    # Clean and preprocess columns\n",
    "    df = clean_columns(df)\n",
    "\n",
    "    print('AFTER CLEAN', df.head())\n",
    "    df = preprocess_columns(df, abbreviations_dict)\n",
    "\n",
    "    print(df)\n",
    "\n",
    "    # Apply analysis\n",
    "    df = apply_analysis(df, target_words_dict, description_words_dict, abbreviations_dict, datasets_xlsx, DB_or_dataset)\n",
    "\n",
    "    # Load the FinalFormat from AnalysedColumns\n",
    "    analysed_df = pd.read_excel(analysed_columns_file_path)\n",
    "    \n",
    "    # Merge the FinalFormat into our main dataframe\n",
    "    df['FinalFormat'] = analysed_df['FinalFormat']\n",
    "\n",
    "    # Prepare features for machine learning\n",
    "    df['combined'] = df['CleanedColumn'].fillna('') + ' [SEP] ' + df['Description'].fillna('') \n",
    "    df['combined'] += ' [SEP] ' + df['ColumnKeyword'].fillna('') + ' [SEP] ' + df['DescriptionKeyword'].fillna('')\n",
    "\n",
    "    #df['combined'] += df['ColumnFormat'].fillna('') + ' [SEP] ' + df['DescriptionFormat'].fillna('')\n",
    "    \n",
    "    # Load DB headers distribution file\n",
    "    db_headers = {}\n",
    "    with open('DBheaders+formats_dict.txt', 'r') as file:\n",
    "        next(file)  # skip the header line\n",
    "        for line in file:\n",
    "            header, count = line.strip().split('\\t')\n",
    "            db_headers[header.lower()] = int(count)\n",
    "\n",
    "    # Extract features using Custom TF-IDF with vocabulary restricted to DB headers\n",
    "    vectorizer = CustomTfidfVectorizer(vocabulary=list(db_headers.keys()))\n",
    "    X = vectorizer.fit_transform(df['combined']).tocsr()\n",
    "\n",
    "    # Modify TF-IDF scores explicitly based on DB headers\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    weights = np.array([db_headers.get(feat, 1) for feat in feature_names])\n",
    "  \n",
    "    # Ensure weights is a 1D array with the same number of elements as X has columns\n",
    "    if weights.shape[0] != X.shape[1]:\n",
    "        raise ValueError(f\"Number of weights ({weights.shape[0]}) does not match number of features in X ({X.shape[1]})\")\n",
    "    \n",
    "    # Multiply each column of X by its corresponding weight\n",
    "    X = X.multiply(weights)\n",
    "\n",
    "    # Encode the FinalFormat column \n",
    "    y = df['FinalFormat'].astype(str)\n",
    "\n",
    "    # Apply efficient SMOTE\n",
    "    X_resampled, y_resampled = efficient_smote(X, y)\n",
    "\n",
    "    print(\"Class distribution after SMOTE:\")\n",
    "    print(pd.Series(y_resampled).value_counts())\n",
    "\n",
    "    # Split the resampled data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42, stratify=y_resampled)\n",
    "\n",
    "    # List of models to train\n",
    "    models = [\n",
    "        (RandomForestClassifier(class_weight='balanced', random_state=42), 'RandomForestClassifier'),\n",
    "        (LogisticRegression(class_weight='balanced', random_state=42, warm_start=True), 'LogisticRegression'),\n",
    "        (GradientBoostingClassifier(random_state=42), 'GradientBoostingClassifier'),\n",
    "        (KNeighborsClassifier(), 'KNeighborsClassifier'),\n",
    "        (LinearSVC(class_weight='balanced', random_state=42, dual='auto'), 'LinearSVC')\n",
    "    ]\n",
    "\n",
    "    # Train, evaluate, and save each model\n",
    "    for model, model_name in models:\n",
    "        best_model, report = train_evaluate_save_model(model, model_name, X_train, X_test, y_train, y_test, df, vectorizer, weights, feature_names, output_file_path_text)\n",
    "        \n",
    "        # Save classification report\n",
    "        with open(f'{model_name}_classification_report.txt', 'w') as f:\n",
    "            f.write(report)\n",
    "\n",
    "    # Save the vectorizer for future use\n",
    "    joblib.dump(vectorizer, 'vectorizer.joblib')\n",
    "    print(\"Vectorizer saved for future use\")\n",
    "\n",
    "    print(f\"Last run on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
